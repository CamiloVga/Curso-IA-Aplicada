{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPikAZZofVxHLPhZdxScf0k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CamiloVga/Curso-IA-Aplicada/blob/main/Script_Clase_7_Supervisado_y_No_Supervisado.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ñ Inteligencia Artificial Aplicada para la Econom√≠a\n",
        "## Universidad de los Andes\n",
        "\n",
        "### üë®‚Äçüè´ Profesores\n",
        "- **Profesor Magistral:** [Camilo Vega Barbosa](https://www.linkedin.com/in/camilo-vega-169084b1/)\n",
        "- **Asistente de Docencia:** [Sergio Julian Zona Moreno](https://www.linkedin.com/in/sergio-julian-zona-moreno/)\n",
        "\n",
        "## üìä Script Algoritmos de Machine Learning Supervisados y No Supervisados\n",
        "\n",
        "Este script implementa los principales algoritmos de ML supervisados y no supervisados:\n",
        "- Regresi√≥n Log√≠stica (con y sin regularizaci√≥n)\n",
        "- KNN y SVM\n",
        "- √Årboles de Decisi√≥n, Random Forest y XGBoost\n",
        "- K-means y PCA\n",
        "\n",
        "### Requisitos de Software:\n",
        "- Conocimientos b√°sicos de Python\n",
        "- Familiaridad con NumPy y Pandas\n",
        "- Comprensi√≥n b√°sica de conceptos estad√≠sticos\n",
        "\n",
        "### Requisitos T√©cnicos:\n",
        "- **Token de Hugging Face**: Necesario para acceder al dataset. Puedes obtener tu token en [Hugging Face](https://huggingface.co/settings/tokens)\n",
        "- **Entorno de Ejecuci√≥n**:\n",
        " - Recomendado: GPU T4 (Cambiar en: Runtime -> Change runtime type -> GPU)\n",
        " - Alternativa: CPU (el c√≥digo funcionar√°, pero ser√° m√°s lento)\n",
        "- **Memoria RAM**: M√≠nimo 4GB recomendados\n",
        "- **Espacio en Disco**: ~200 GB para datasets y modelos\n",
        "\n",
        "üí° **Nota**: Aunque recomendamos usar GPU para mayor velocidad, todo el c√≥digo es compatible con CPU y funcionar√° correctamente, solo que tomar√° m√°s tiempo en ejecutarse."
      ],
      "metadata": {
        "id": "Yh8OpSTaxfOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algoritmos Supervisados"
      ],
      "metadata": {
        "id": "1RguaHPvx704"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAV0q0rVw6oo"
      },
      "outputs": [],
      "source": [
        "# 0. Instalaciones\n",
        "!pip install -q pandas numpy scikit-learn xgboost matplotlib seaborn datasets\n",
        "\n",
        "# 1. Importaciones necesarias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import (confusion_matrix, roc_curve, roc_auc_score,\n",
        "                          silhouette_score, classification_report)\n",
        "\n",
        "# Modelos\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "import xgboost as xgb\n",
        "\n",
        "# 2. Carga de datos\n",
        "\"\"\"\n",
        "DATASET: Default of Credit Card Clients\n",
        "Link: https://huggingface.co/datasets/scikit-learn/credit-card-clients\n",
        "\n",
        "Descripci√≥n: Contiene informaci√≥n sobre incumplimientos de pago de clientes de tarjetas\n",
        "de cr√©dito en Taiw√°n (abril-septiembre 2005). Incluye 23 variables explicativas y\n",
        "1 variable objetivo binaria (default.payment.next.month).\n",
        "\n",
        "Variables principales:\n",
        "- LIMIT_BAL: Monto de cr√©dito otorgado (NT dollars)\n",
        "- SEX: G√©nero (1=masculino, 2=femenino)\n",
        "- EDUCATION: Nivel educativo (1=posgrado, 2=universidad, 3=secundaria, 4=otros)\n",
        "- MARRIAGE: Estado civil (1=casado, 2=soltero, 3=otros)\n",
        "- AGE: Edad en a√±os\n",
        "- PAY_0 a PAY_6: Estado de pago mensual (-1=pago a tiempo, 1=retraso 1 mes, etc.)\n",
        "- BILL_AMT1-6: Monto facturado mensual\n",
        "- PAY_AMT1-6: Monto pagado mensual\n",
        "- default.payment.next.month: Variable objetivo (1=default, 0=no default)\n",
        "\n",
        "Total de registros disponibles: 30,000\n",
        "Registros utilizados en este script: 10,000 (para eficiencia computacional)\n",
        "\"\"\"\n",
        "\n",
        "# Dataset principal para algoritmos supervisados\n",
        "dataset = load_dataset(\"scikit-learn/credit-card-clients\", streaming=True)\n",
        "df_credit = pd.DataFrame(list(dataset['train'].shuffle(seed=42).take(10000)))\n",
        "\n",
        "# Para usar tu propio CSV, descomenta las siguientes l√≠neas:\n",
        "# df_credit = pd.read_csv('tu_archivo.csv')\n",
        "# Aseg√∫rate de que tenga una columna objetivo llamada 'target' o ajusta el c√≥digo\n",
        "\n",
        "print(\"Informaci√≥n del dataset cargado:\")\n",
        "print(f\"Dimensiones: {df_credit.shape}\")\n",
        "print(f\"Columnas: {list(df_credit.columns)}\")\n",
        "print(f\"\\nDistribuci√≥n de la variable objetivo:\")\n",
        "print(df_credit['default.payment.next.month'].value_counts(normalize=True))\n",
        "\n",
        "# 3. Preprocesamiento de datos\n",
        "def preprocess_credit_data(df):\n",
        "    \"\"\"\n",
        "    Preprocesa los datos de cr√©dito para que sean aptos para machine learning.\n",
        "    Los algoritmos de ML requieren que todos los datos sean num√©ricos y est√©n en escalas similares.\n",
        "    \"\"\"\n",
        "\n",
        "    # PASO 1: Identificar qu√© variables son num√©ricas vs categ√≥ricas\n",
        "    # Num√©ricas: son cantidades medibles (dinero, edad) que tienen sentido matem√°tico\n",
        "    # - LIMIT_BAL: l√≠mite de cr√©dito en d√≥lares\n",
        "    # - AGE: edad en a√±os\n",
        "    # - BILL_AMT1-6: montos facturados cada mes\n",
        "    # - PAY_AMT1-6: montos pagados cada mes\n",
        "    numeric_features = ['LIMIT_BAL', 'AGE'] + [f'BILL_AMT{i}' for i in range(1,7)] + [f'PAY_AMT{i}' for i in range(1,7)]\n",
        "\n",
        "    # Categ√≥ricas: son etiquetas o categor√≠as sin sentido matem√°tico directo\n",
        "    # - SEX: 1=hombre, 2=mujer (no tiene sentido sumar o restar estos n√∫meros)\n",
        "    # - EDUCATION: 1=posgrado, 2=universidad, etc. (no es que posgrado sea \"menor\" que universidad)\n",
        "    # - MARRIAGE: estado civil\n",
        "    # - PAY_0 a PAY_6: estado de pago (-1=a tiempo, 1=1 mes tarde, etc.)\n",
        "    categorical_features = ['SEX', 'EDUCATION', 'MARRIAGE'] + ['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\n",
        "\n",
        "    # PASO 2: Limpiar datos con valores inconsistentes\n",
        "    df = df.copy()  # Evitar modificar el DataFrame original\n",
        "    # En EDUCATION: 0, 5, 6 no est√°n definidos en la documentaci√≥n, los agrupamos como \"otros\" (4)\n",
        "    df['EDUCATION'] = df['EDUCATION'].replace([0, 5, 6], 4)\n",
        "    # En MARRIAGE: 0 no est√° definido, lo agrupamos como \"otros\" (3)\n",
        "    df['MARRIAGE'] = df['MARRIAGE'].replace([0], 3)\n",
        "\n",
        "    # PASO 3: Estandarizar variables num√©ricas (media=0, desviaci√≥n=1)\n",
        "    # ¬øPor qu√©? Los algoritmos son sensibles a la escala. Sin esto:\n",
        "    # - LIMIT_BAL (miles/millones) dominar√≠a sobre AGE (decenas)\n",
        "    # - El modelo dar√≠a m√°s importancia a variables con valores grandes\n",
        "    scaler = StandardScaler()\n",
        "    df[numeric_features] = scaler.fit_transform(df[numeric_features])\n",
        "\n",
        "    # PASO 4: Codificar variables categ√≥ricas\n",
        "    # Los algoritmos solo entienden n√∫meros, no etiquetas\n",
        "    # LabelEncoder convierte categor√≠as a n√∫meros (ej: masculino=0, femenino=1)\n",
        "    # Nota: esto asume que no hay orden en las categor√≠as\n",
        "    for feature in categorical_features:\n",
        "        le = LabelEncoder()\n",
        "        df[feature] = le.fit_transform(df[feature].astype(str))\n",
        "\n",
        "    # PASO 5: Separar predictores (X) de variable objetivo (y)\n",
        "    # X: todas las caracter√≠sticas que usaremos para predecir\n",
        "    # y: lo que queremos predecir (si habr√° default o no)\n",
        "    X = df.drop('default.payment.next.month', axis=1)\n",
        "    y = df['default.payment.next.month']\n",
        "\n",
        "    # Retornamos tambi√©n el scaler por si necesitamos procesar nuevos datos\n",
        "    return X, y, scaler\n",
        "\n",
        "# Procesar datos\n",
        "X, y, scaler = preprocess_credit_data(df_credit)\n",
        "\n",
        "# Verificar el resultado\n",
        "print(\"\\nDatos despu√©s del preprocesamiento:\")\n",
        "print(f\"Forma de X: {X.shape}\")\n",
        "print(f\"Primeras 3 columnas de X:\\n{X.iloc[:5, :3]}\")  # Muestra que ahora todo es num√©rico\n",
        "print(f\"\\nDistribuci√≥n de y: {y.value_counts()}\")\n",
        "\n",
        "# 4. Divisi√≥n de datos\n",
        "# test_size: proporci√≥n de datos para prueba (0.2 = 20%)\n",
        "# random_state: semilla para reproducibilidad\n",
        "# stratify: mantiene la proporci√≥n de clases en train y test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "   X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nTama√±o conjunto entrenamiento: {X_train.shape}\")\n",
        "print(f\"Tama√±o conjunto prueba: {X_test.shape}\")\n",
        "print(f\"Distribuci√≥n de clases en entrenamiento: {pd.Series(y_train).value_counts(normalize=True).round(3).to_dict()}\")\n",
        "\n",
        "# 5. Definici√≥n de modelos supervisados\n",
        "models = {\n",
        "   # Regresi√≥n Log√≠stica sin regularizaci√≥n\n",
        "   'Logit_Basico': LogisticRegression(\n",
        "       random_state=42,\n",
        "       max_iter=1000  # N√∫mero m√°ximo de iteraciones\n",
        "   ),\n",
        "\n",
        "   # Regresi√≥n Log√≠stica con regularizaci√≥n L2 (Ridge)\n",
        "   'Logit_Ridge': LogisticRegression(\n",
        "       penalty='l2',      # Tipo de regularizaci√≥n\n",
        "       C=1.0,            # Inverso de la fuerza de regularizaci√≥n (menor C = m√°s regularizaci√≥n)\n",
        "       random_state=42,\n",
        "       max_iter=1000\n",
        "   ),\n",
        "\n",
        "   # Regresi√≥n Log√≠stica con regularizaci√≥n L1 (Lasso)\n",
        "   'Logit_Lasso': LogisticRegression(\n",
        "       penalty='l1',\n",
        "       C=1.0,            # Inverso de la fuerza de regularizaci√≥n\n",
        "       solver='liblinear',  # Necesario para L1\n",
        "       random_state=42,\n",
        "       max_iter=1000\n",
        "   ),\n",
        "\n",
        "   # K-Nearest Neighbors\n",
        "   'KNN': KNeighborsClassifier(\n",
        "       n_neighbors=5,     # N√∫mero de vecinos a considerar (m√°s vecinos = decisi√≥n m√°s suave)\n",
        "       weights='uniform', # 'uniform' o 'distance' (distance da m√°s peso a vecinos cercanos)\n",
        "       metric='euclidean' # M√©trica de distancia\n",
        "   ),\n",
        "\n",
        "   # Support Vector Machine\n",
        "   'SVM': SVC(\n",
        "       kernel='rbf',      # Tipo de kernel: 'linear', 'poly', 'rbf', 'sigmoid'\n",
        "       C=1.0,            # Par√°metro de regularizaci√≥n (mayor C = menos regularizaci√≥n)\n",
        "       gamma='scale',     # Coeficiente del kernel (afecta la influencia de cada punto)\n",
        "       probability=True,  # Necesario para obtener probabilidades\n",
        "       random_state=42\n",
        "   ),\n",
        "\n",
        "   # √Årbol de Decisi√≥n\n",
        "   'DecisionTree': DecisionTreeClassifier(\n",
        "       criterion='entropy',    # Criterio de divisi√≥n: 'gini' o 'entropy'\n",
        "       max_depth=6,           # Profundidad m√°xima (menor = menos overfitting)\n",
        "       min_samples_split=20,  # M√≠nimo de muestras para dividir un nodo\n",
        "       min_samples_leaf=10,   # M√≠nimo de muestras en una hoja\n",
        "       random_state=42\n",
        "   ),\n",
        "\n",
        "   # Random Forest\n",
        "   'RandomForest': RandomForestClassifier(\n",
        "       n_estimators=100,      # N√∫mero de √°rboles (m√°s √°rboles = mejor pero m√°s lento)\n",
        "       max_depth=None,        # Profundidad m√°xima de cada √°rbol\n",
        "       min_samples_leaf=5,    # M√≠nimo de muestras en hojas\n",
        "       max_features='sqrt',   # N√∫mero de features a considerar en cada split\n",
        "       n_jobs=-1,            # Usar todos los cores disponibles\n",
        "       random_state=42\n",
        "   ),\n",
        "\n",
        "   # XGBoost\n",
        "   'XGBoost': xgb.XGBClassifier(\n",
        "       learning_rate=0.1,     # Tasa de aprendizaje (menor = m√°s conservador)\n",
        "       n_estimators=100,      # N√∫mero de √°rboles\n",
        "       max_depth=6,          # Profundidad m√°xima de cada √°rbol\n",
        "       subsample=0.8,        # Fracci√≥n de muestras para entrenar cada √°rbol\n",
        "       colsample_bytree=0.8, # Fracci√≥n de columnas para cada √°rbol\n",
        "       random_state=42\n",
        "   )\n",
        "}\n",
        "\n",
        "# 6. Entrenamiento y predicciones\n",
        "predictions = {}\n",
        "probabilities = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "   print(f\"Entrenando {name}...\")\n",
        "   model.fit(X_train, y_train)\n",
        "   predictions[name] = model.predict(X_test)\n",
        "   if hasattr(model, 'predict_proba'):\n",
        "       probabilities[name] = model.predict_proba(X_test)[:, 1]\n",
        "   else:\n",
        "       probabilities[name] = model.decision_function(X_test)\n",
        "\n",
        "# 7. Evaluaci√≥n - M√©tricas tradicionales\n",
        "def plot_confusion_matrices(y_true, predictions_dict):\n",
        "   n_models = len(predictions_dict)\n",
        "   fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "   axes = axes.ravel()\n",
        "\n",
        "   for idx, (name, y_pred) in enumerate(predictions_dict.items()):\n",
        "       cm = confusion_matrix(y_true, y_pred)\n",
        "       sns.heatmap(cm, annot=True, fmt='d', ax=axes[idx], cmap='Blues')\n",
        "       axes[idx].set_title(f'{name}')\n",
        "       axes[idx].set_xlabel('Predicci√≥n')\n",
        "       axes[idx].set_ylabel('Real')\n",
        "\n",
        "   # Ocultar subplots vac√≠os si hay menos de 8 modelos\n",
        "   for idx in range(n_models, 8):\n",
        "       axes[idx].axis('off')\n",
        "\n",
        "   plt.tight_layout()\n",
        "   plt.show()\n",
        "\n",
        "def plot_roc_curves(y_true, probabilities_dict):\n",
        "   plt.figure(figsize=(10, 8))\n",
        "\n",
        "   for name, probs in probabilities_dict.items():\n",
        "       fpr, tpr, _ = roc_curve(y_true, probs)\n",
        "       auc = roc_auc_score(y_true, probs)\n",
        "       plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})')\n",
        "\n",
        "   plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "   plt.xlabel('Tasa de Falsos Positivos')\n",
        "   plt.ylabel('Tasa de Verdaderos Positivos')\n",
        "   plt.title('Curvas ROC - Comparaci√≥n de Modelos')\n",
        "   plt.legend()\n",
        "   plt.grid(True, alpha=0.3)\n",
        "   plt.show()\n",
        "\n",
        "# Visualizar resultados\n",
        "plot_confusion_matrices(y_test, predictions)\n",
        "plot_roc_curves(y_test, probabilities)\n",
        "\n",
        "# Tabla comparativa de m√©tricas\n",
        "print(\"\\nM√©tricas de evaluaci√≥n:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"{'Modelo':<15} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'AUC-ROC':<10}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for name in predictions.keys():\n",
        "   report = classification_report(y_test, predictions[name], output_dict=True)\n",
        "   auc = roc_auc_score(y_test, probabilities[name])\n",
        "\n",
        "   print(f\"{name:<15} \"\n",
        "         f\"{report['accuracy']:<10.3f} \"\n",
        "         f\"{report['1']['precision']:<10.3f} \"\n",
        "         f\"{report['1']['recall']:<10.3f} \"\n",
        "         f\"{report['1']['f1-score']:<10.3f} \"\n",
        "         f\"{auc:<10.3f}\")\n",
        "\n",
        "# 8. Validaci√≥n cruzada\n",
        "# Usar F1-score como m√©trica principal\n",
        "cv_scores = {}\n",
        "cv_scores_stratified = {}\n",
        "\n",
        "# K-fold normal\n",
        "kfold = 5\n",
        "for name, model in models.items():\n",
        "   scores = cross_val_score(model, X_train, y_train, cv=kfold, scoring='f1')\n",
        "   cv_scores[name] = scores\n",
        "\n",
        "# K-fold estratificado\n",
        "skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "for name, model in models.items():\n",
        "   scores = cross_val_score(model, X_train, y_train, cv=skfold, scoring='f1')\n",
        "   cv_scores_stratified[name] = scores\n",
        "\n",
        "# Visualizar resultados de CV\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# CV normal\n",
        "data_cv = [cv_scores[name] for name in models.keys()]\n",
        "ax1.boxplot(data_cv, labels=models.keys())\n",
        "ax1.set_title('Validaci√≥n Cruzada K-Fold (F1-Score)')\n",
        "ax1.set_xlabel('Modelo')\n",
        "ax1.set_ylabel('F1-Score')\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# CV estratificado\n",
        "data_cv_strat = [cv_scores_stratified[name] for name in models.keys()]\n",
        "ax2.boxplot(data_cv_strat, labels=models.keys())\n",
        "ax2.set_title('Validaci√≥n Cruzada Estratificada (F1-Score)')\n",
        "ax2.set_xlabel('Modelo')\n",
        "ax2.set_ylabel('F1-Score')\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Imprimir medias y desviaciones\n",
        "print(\"\\nResultados de Validaci√≥n Cruzada (F1-Score):\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'Modelo':<15} {'CV Normal (media¬±std)':<25} {'CV Estratificado (media¬±std)':<25}\")\n",
        "print(\"-\" * 60)\n",
        "for name in models.keys():\n",
        "   print(f\"{name:<15} \"\n",
        "         f\"{np.mean(cv_scores[name]):.3f}¬±{np.std(cv_scores[name]):.3f}\"\n",
        "         f\"{'':>10}\"\n",
        "         f\"{np.mean(cv_scores_stratified[name]):.3f}¬±{np.std(cv_scores_stratified[name]):.3f}\")\n",
        "\n",
        "# Observa: ¬øQu√© modelos tienen menor variabilidad? ¬øCu√°les mantienen mejor rendimiento?\n",
        "# La CV estratificada suele dar resultados m√°s estables en datasets desbalanceados\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algoritmos Supervisados"
      ],
      "metadata": {
        "id": "kBXq6s7yx8Ti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Algoritmos no supervisados\n",
        "\"\"\"\n",
        "Para algoritmos no supervisados, utilizamos el mismo dataset pero con un enfoque diferente.\n",
        "Aqu√≠ no usamos la variable objetivo, solo exploramos patrones naturales en los datos.\n",
        "\n",
        "El clustering puede revelar:\n",
        "- Grupos naturales de clientes con comportamientos similares\n",
        "- Segmentos de riesgo no evidentes en el an√°lisis supervisado\n",
        "- Variables que dominan la estructura de los datos\n",
        "\"\"\"\n",
        "\n",
        "# Cargar dataset para clustering\n",
        "dataset_clustering = load_dataset(\"scikit-learn/credit-card-clients\", streaming=True)\n",
        "df_clustering = pd.DataFrame(list(dataset_clustering['train'].shuffle(seed=42).take(10000)))\n",
        "\n",
        "# Seleccionar solo caracter√≠sticas num√©ricas y estandarizar\n",
        "numeric_cols = df_clustering.select_dtypes(include=['float64', 'int64']).columns\n",
        "X_clustering = df_clustering[numeric_cols]\n",
        "X_clustering_scaled = StandardScaler().fit_transform(X_clustering)\n",
        "\n",
        "print(f\"\\nDatos para clustering:\")\n",
        "print(f\"Dimensiones: {X_clustering_scaled.shape}\")\n",
        "print(f\"Variables utilizadas: {list(numeric_cols)}\")\n",
        "\n",
        "# K-means con diferentes n√∫meros de clusters\n",
        "inertias = []\n",
        "silhouette_scores = []\n",
        "k_range = range(2, 11)\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(\n",
        "        n_clusters=k,           # N√∫mero de clusters\n",
        "        init='k-means++',       # M√©todo de inicializaci√≥n ('k-means++' o 'random')\n",
        "        n_init=10,             # N√∫mero de veces que se ejecuta con diferentes centroides\n",
        "        max_iter=300,          # M√°ximo de iteraciones\n",
        "        random_state=42\n",
        "    )\n",
        "    kmeans.fit(X_clustering_scaled)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "    silhouette_scores.append(silhouette_score(X_clustering_scaled, kmeans.labels_))\n",
        "\n",
        "# Visualizar m√©todo del codo y silhouette score\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "ax1.plot(k_range, inertias, marker='o')\n",
        "ax1.set_title('M√©todo del Codo')\n",
        "ax1.set_xlabel('N√∫mero de clusters (k)')\n",
        "ax1.set_ylabel('Inercia')\n",
        "ax1.grid(True)\n",
        "\n",
        "ax2.plot(k_range, silhouette_scores, marker='o', color='orange')\n",
        "ax2.set_title('Silhouette Score')\n",
        "ax2.set_xlabel('N√∫mero de clusters (k)')\n",
        "ax2.set_ylabel('Silhouette Score')\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Observa: El codo indica donde agregar m√°s clusters no mejora mucho\n",
        "# Silhouette score m√°s alto indica mejor separaci√≥n de clusters\n",
        "\n",
        "# Determinar el n√∫mero √≥ptimo de clusters\n",
        "# Basado en el silhouette score m√°s alto\n",
        "optimal_k_silhouette = k_range[np.argmax(silhouette_scores)]\n",
        "print(f\"\\nN√∫mero √≥ptimo de clusters seg√∫n Silhouette Score: {optimal_k_silhouette}\")\n",
        "print(f\"Silhouette Score m√°ximo: {max(silhouette_scores):.3f}\")\n",
        "\n",
        "# Para el an√°lisis posterior, usar k=3 o el √≥ptimo encontrado\n",
        "k_final = 3  # Puedes cambiar esto a optimal_k_silhouette si prefieres\n",
        "print(f\"\\nN√∫mero de clusters elegido para el an√°lisis: {k_final}\")\n",
        "\n",
        "# PCA para reducci√≥n de dimensionalidad\n",
        "# Aplicar PCA\n",
        "pca_full = PCA(random_state=42)\n",
        "pca_full.fit(X_clustering_scaled)\n",
        "\n",
        "# Visualizar varianza explicada\n",
        "cumsum_var = np.cumsum(pca_full.explained_variance_ratio_)\n",
        "n_components_95 = np.argmax(cumsum_var >= 0.95) + 1\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(cumsum_var) + 1), cumsum_var, marker='o')\n",
        "plt.axhline(y=0.95, color='r', linestyle='--', label='95% varianza')\n",
        "plt.axvline(x=n_components_95, color='g', linestyle='--',\n",
        "            label=f'{n_components_95} componentes')\n",
        "plt.xlabel('N√∫mero de componentes')\n",
        "plt.ylabel('Varianza explicada acumulada')\n",
        "plt.title('PCA - Varianza Explicada')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nComponentes necesarios para 95% de varianza: {n_components_95}\")\n",
        "print(f\"Reducci√≥n de dimensionalidad: de {X_clustering_scaled.shape[1]} a {n_components_95} variables\")\n",
        "\n",
        "# Aplicar PCA con 2 componentes para visualizaci√≥n\n",
        "pca_2d = PCA(n_components=2, random_state=42)\n",
        "X_pca_2d = pca_2d.fit_transform(X_clustering_scaled)\n",
        "\n",
        "# K-means en datos con PCA usando el n√∫mero de clusters elegido\n",
        "kmeans_pca = KMeans(n_clusters=k_final, random_state=42)\n",
        "labels_pca = kmeans_pca.fit_predict(X_pca_2d)\n",
        "\n",
        "# Visualizar clusters\n",
        "plt.figure(figsize=(10, 6))\n",
        "scatter = plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=labels_pca, cmap='viridis')\n",
        "plt.xlabel('Primera componente principal')\n",
        "plt.ylabel('Segunda componente principal')\n",
        "plt.title(f'Clusters K-means en espacio PCA (k={k_final})')\n",
        "plt.colorbar(scatter, label='Cluster')\n",
        "plt.show()\n",
        "\n",
        "# Comparar K-means con y sin PCA\n",
        "kmeans_original = KMeans(n_clusters=k_final, random_state=42)\n",
        "labels_original = kmeans_original.fit_predict(X_clustering_scaled)\n",
        "\n",
        "print(f\"\\nComparaci√≥n K-means con k={k_final}:\")\n",
        "print(f\"Silhouette Score (datos originales): {silhouette_score(X_clustering_scaled, labels_original):.3f}\")\n",
        "print(f\"Silhouette Score (con PCA 2D): {silhouette_score(X_pca_2d, labels_pca):.3f}\")\n",
        "\n",
        "# Observa: PCA puede mejorar o empeorar el clustering dependiendo de los datos\n",
        "# Un silhouette score m√°s alto indica clusters mejor definidos"
      ],
      "metadata": {
        "id": "eUPuAP2Sx9F9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}