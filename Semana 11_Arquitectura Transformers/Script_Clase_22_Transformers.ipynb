{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN79Tf5uKdnLzQSY2FiMD9z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CamiloVga/Curso-IA-Aplicada/blob/main/Semana%2011_Arquitectura%20Transformers/Script_Clase_22_Transformers.ipynb)"
  ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ñ Inteligencia Artificial Aplicada para la Econom√≠a\n",
        "## Universidad de los Andes\n",
        "\n",
        "### üë®‚Äçüè´ Profesores\n",
        "- **Profesor Magistral:** [Camilo Vega Barbosa](https://www.linkedin.com/in/camilovegabarbosa/)\n",
        "- **Asistente de Docencia:** [Sergio Julian Zona Moreno](https://www.linkedin.com/in/sergiozonamoreno/)\n",
        "\n",
        "### üìö Implementaci√≥n de Transformers con Llama-2\n",
        "Este script implementa un chatbot basado en la arquitectura Transformer utilizando el modelo Llama-2-7b de Meta:\n",
        "\n",
        "1. **Configuraci√≥n y Optimizaci√≥n del Modelo Transformer üöÄ**\n",
        "   * Carga optimizada de Llama-2-7b con quantizaci√≥n de 8-bit\n",
        "   * Configuraci√≥n de precisi√≥n de 16 bits para eficiencia en GPU\n",
        "   * Distribuci√≥n autom√°tica del modelo para optimizar recursos\n",
        "   * Preparaci√≥n del modelo para ejecuci√≥n en GPUs de Colab\n",
        "\n",
        "2. **Generaci√≥n de Texto con Arquitectura Transformer üß†**\n",
        "   * Tokenizaci√≥n y procesamiento de entradas de texto\n",
        "   * Configuraci√≥n de par√°metros de sampling probabil√≠stico\n",
        "   * Generaci√≥n controlada mediante temperature y top_p\n",
        "   * Decodificaci√≥n eficiente de tokens a texto natural\n",
        "\n",
        "3. **Interfaz Conversacional con Gradio üí¨**\n",
        "   * Implementaci√≥n de interfaz de usuario intuitiva\n",
        "   * Manejo de contexto y formato espec√≠fico para Llama-2\n",
        "   * Ejemplos predefinidos sobre conceptos de Transformers\n",
        "   * Dise√±o responsivo para facilitar la experimentaci√≥n\n"
      ],
      "metadata": {
        "id": "eBXTlQg_POyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar bibliotecas necesarias\n",
        "!pip install transformers torch accelerate bitsandbytes gradio -q\n",
        "\n",
        "# Importar bibliotecas\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Cargar tokenizador\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "\n",
        "# Cargar modelo con optimizaciones para T4\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-2-7b-hf\",\n",
        "    device_map=\"auto\",           # Distribuye el modelo autom√°ticamente\n",
        "    torch_dtype=torch.float16,   # Usa precisi√≥n de 16 bits\n",
        "    load_in_8bit=True            # Carga el modelo con cuantizaci√≥n de 8 bits\n",
        ")\n",
        "\n",
        "# Generar texto\n",
        "prompt = \"Explica de manera simple qu√© es un transformer en inteligencia artificial:\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")  # Mover a GPU\n",
        "outputs = model.generate(\n",
        "    inputs.input_ids,\n",
        "    max_new_tokens=200,\n",
        "    do_sample=True,\n",
        "    temperature=0.7\n",
        ")\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "o5ZBq7-oKEil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Importar bibliotecas\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "# Historial del chat para mantener la conversaci√≥n\n",
        "conversation_history = []\n",
        "\n",
        "# Funci√≥n para generar respuestas\n",
        "def generate_response(message, history):\n",
        "    # Construir el contexto del chat con formato para Llama-2\n",
        "    prompt = \"\"\n",
        "    for user_msg, bot_msg in history:\n",
        "        prompt += f\"<s>[INST] {user_msg} [/INST] {bot_msg}</s>\"\n",
        "\n",
        "    # A√±adir el mensaje actual\n",
        "    prompt += f\"<s>[INST] {message} [/INST]\"\n",
        "\n",
        "    # Generar respuesta\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        inputs.input_ids,\n",
        "        max_new_tokens=512,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "    # Decodificar la respuesta\n",
        "    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "    return response\n",
        "\n",
        "# Crear la interfaz de Gradio\n",
        "demo = gr.ChatInterface(\n",
        "    fn=generate_response,\n",
        "    title=\"Chat con Llama-2-7b\",\n",
        "    description=\"Un chatbot simple usando el modelo Llama-2-7b. Haz una pregunta para empezar.\",\n",
        "    examples=[\"¬øQu√© es un transformer en IA?\", \"Explica el concepto de atenci√≥n en NLP\", \"¬øC√≥mo funciona GPT?\"],\n",
        "    theme=\"soft\"\n",
        ")\n",
        "\n",
        "# Lanzar la app\n",
        "demo.launch(share=True)  # share=True crea un enlace p√∫blico"
      ],
      "metadata": {
        "id": "QIVeQk-hMXR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ogztraLoeP2J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
