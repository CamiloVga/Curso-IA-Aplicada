{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CamiloVga/Curso-IA-Aplicada/blob/main/Script_Clase_12_Fundamentos_de_redes_neuronales.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qIH_dNAWvIe"
      },
      "source": [
        "# ü§ñ Inteligencia Artificial Aplicada para la Econom√≠a\n",
        "## Universidad de los Andes\n",
        "\n",
        "### üë®‚Äçüè´ Profesores\n",
        "- **Profesor Magistral:** [Camilo Vega Barbosa](https://www.linkedin.com/in/camilo-vega-169084b1/)\n",
        "- **Asistente de Docencia:** [Sergio Julian Zona Moreno](https://www.linkedin.com/in/sergio-julian-zona-moreno/)\n",
        "\n",
        "### üìö Ejercicio Pr√°ctico: Predicci√≥n de Admisiones en Escuelas de Derecho\n",
        "Este notebook demuestra la implementaci√≥n pr√°ctica de conceptos fundamentales de Deep Learning:\n",
        "\n",
        "1. **Fundamentos de Redes Neuronales**\n",
        "   - Arquitectura b√°sica\n",
        "   - Proceso de forward propagation\n",
        "   - Funci√≥n de activaci√≥n\n",
        "\n",
        "2. **Perceptr√≥n Simple**\n",
        "   - Implementaci√≥n desde cero\n",
        "   - Entrenamiento con datos reales\n",
        "   - Visualizaci√≥n de la frontera de decisi√≥n\n",
        "\n",
        "3. **Perceptr√≥n Multicapa (MLP)**\n",
        "   - Arquitectura con capas ocultas\n",
        "   - Funciones de activaci√≥n ReLU y Sigmoid\n",
        "   - Proceso de entrenamiento\n",
        "\n",
        "4. **Deep Learning**\n",
        "   - Implementaci√≥n de red con 4 capas\n",
        "   - T√©cnicas de backpropagation\n",
        "   - Optimizaci√≥n de hiperpar√°metros\n",
        "   - Grid Search para calibraci√≥n\n",
        "\n",
        "### üéØ Objetivo\n",
        "Predecir la admisi√≥n de estudiantes a escuelas de derecho utilizando variables clave:\n",
        "- LSAT score\n",
        "- GPA\n",
        "- Estado de residencia\n",
        "\n",
        "\n",
        "### Requisitos T√©cnicos:\n",
        "- **Token de Hugging Face**: Necesario para acceder al dataset. Puedes obtener tu token en [Hugging Face](https://huggingface.co/settings/tokens)\n",
        "- **Entorno de Ejecuci√≥n**:\n",
        "  - Recomendado: GPU T4 (Cambiar en: Runtime -> Change runtime type -> GPU)\n",
        "- **Memoria RAM**: M√≠nimo 4GB recomendados\n",
        "- **Espacio en Disco**: ~200 GB para datasets y modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK_1_tEjYN7E"
      },
      "source": [
        "# üõ†Ô∏è Instalaciones Necesarias\n",
        "\n",
        "Antes de comenzar, necesitamos instalar algunas librer√≠as espec√≠ficas para nuestro an√°lisis de Deep Learning. A continuaci√≥n se listan las librer√≠as y su prop√≥sito:\n",
        "\n",
        "### Manejo de Datos y C√°lculos\n",
        "- **pandas**: Manipulaci√≥n y an√°lisis de datos tabulares\n",
        "- **numpy**: Operaciones num√©ricas y matrices multidimensionales\n",
        "- **scipy**: Funciones cient√≠ficas y estad√≠sticas avanzadas\n",
        "- **ydata-profiling**: Generaci√≥n automatizada de reportes de an√°lisis exploratorio de datos\n",
        "\n",
        "### Deep Learning y Machine Learning\n",
        "- **tensorflow**: Framework principal para implementaci√≥n de redes neuronales\n",
        "- **keras**: API de alto nivel para construcci√≥n de modelos de deep learning\n",
        "- **scikit-learn**: Herramientas complementarias para preprocesamiento y evaluaci√≥n\n",
        "- **pytorch**: Framework alternativo para deep learning, popular en investigaci√≥n y desarrollo\n",
        "\n",
        "### Visualizaci√≥n\n",
        "- **matplotlib**: Biblioteca base para crear gr√°ficos est√°ticos\n",
        "- **seaborn**: Visualizaciones estad√≠sticas de alto nivel\n",
        "- **plotly**: Gr√°ficos interactivos para an√°lisis detallado\n",
        "\n",
        "### Optimizaci√≥n y Evaluaci√≥n\n",
        "- **optuna**: Optimizaci√≥n autom√°tica de hiperpar√°metros\n",
        "- **tensorboard**: Visualizaci√≥n y monitoreo del entrenamiento\n",
        "- **scikit-optimize**: Optimizaci√≥n de hiperpar√°metros y b√∫squeda en grid\n",
        "\n",
        "### Utilidades\n",
        "- **tqdm**: Barras de progreso para monitorear procesos largos\n",
        "- **joblib**: Paralelizaci√≥n y caching de operaciones computacionales\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nKU_Df7V9Gk",
        "outputId": "0f3e8dff-9cfb-4e57-eeba-cd3b9e9c389a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Instalaci√≥n silenciosa de librer√≠as\n",
        "# Frameworks de Deep Learning\n",
        "!pip install -q tensorflow\n",
        "!pip install -q torch\n",
        "!pip install -q keras\n",
        "!pip install -q keras-tuner\n",
        "\n",
        "# An√°lisis y manipulaci√≥n de datos\n",
        "!pip install -q pandas\n",
        "!pip install -q numpy\n",
        "!pip install -q scikit-learn\n",
        "\n",
        "# Visualizaci√≥n\n",
        "!pip install -q matplotlib\n",
        "!pip install -q seaborn\n",
        "!pip install -q plotly\n",
        "\n",
        "# Optimizaci√≥n y evaluaci√≥n\n",
        "!pip install -q optuna\n",
        "!pip install -q tensorboard\n",
        "!pip install -q scikit-optimize\n",
        "\n",
        "# Utilidades\n",
        "!pip install -q tqdm\n",
        "!pip install -q joblib\n",
        "\n",
        "# An√°lisis exploratorio y acceso a datos\n",
        "!pip install -q ydata-profiling\n",
        "!pip install -q datasets\n",
        "!pip install -q huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lk9-p-tsaj2X"
      },
      "source": [
        "# üìä Base de Datos: Bank Marketing\n",
        "\n",
        "Para este ejercicio utilizaremos la base de datos \"Bank Marketing\" disponible en [HuggingFace](https://huggingface.co/datasets/cestwc/bank-marketing), que contiene informaci√≥n sobre campa√±as de marketing de una instituci√≥n bancaria portuguesa.\n",
        "\n",
        "## üìù Descripci√≥n\n",
        "La base contiene informaci√≥n detallada sobre campa√±as de marketing directo (llamadas telef√≥nicas) de una instituci√≥n bancaria portuguesa. El objetivo es predecir si el cliente suscribir√° un dep√≥sito a plazo (variable 'y').\n",
        "\n",
        "## üîç Variables Disponibles\n",
        "\n",
        "### Informaci√≥n del Cliente\n",
        "- **age**: Edad del cliente\n",
        "- **job**: Tipo de trabajo\n",
        "- **marital**: Estado civil\n",
        "- **education**: Nivel educativo\n",
        "- **default**: ¬øTiene cr√©dito en default?\n",
        "- **balance**: Saldo promedio anual\n",
        "- **housing**: ¬øTiene pr√©stamo de vivienda?\n",
        "- **loan**: ¬øTiene pr√©stamo personal?\n",
        "\n",
        "### Informaci√≥n de la Campa√±a Actual\n",
        "- **contact**: Tipo de contacto\n",
        "- **day**: √öltimo d√≠a de contacto del mes\n",
        "- **month**: √öltimo mes de contacto\n",
        "- **duration**: Duraci√≥n del √∫ltimo contacto en segundos\n",
        "- **campaign**: N√∫mero de contactos realizados durante esta campa√±a\n",
        "- **pdays**: D√≠as transcurridos desde el √∫ltimo contacto\n",
        "- **previous**: N√∫mero de contactos realizados antes de esta campa√±a\n",
        "- **poutcome**: Resultado de la campa√±a de marketing anterior\n",
        "\n",
        "### Variable Objetivo\n",
        "- **y**: ¬øEl cliente suscribi√≥ un dep√≥sito a plazo? (0=no, 1=s√≠)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ra9PAYzYMRu"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Importamos las librer√≠as necesarias\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from ydata_profiling import ProfileReport\n",
        "\n",
        "# Cargamos el dataset y lo mezclamos\n",
        "print(\"‚è≥ Cargando y mezclando 10,000 registros aleatorios de Bank Marketing desde HuggingFace...\")\n",
        "dataset = load_dataset(\"cestwc/bank-marketing\", streaming=True)\n",
        "df = pd.DataFrame(list(dataset['train'].shuffle(seed=42).take(10000)))\n",
        "\n",
        "print(\"\\nüìä Generando reporte de an√°lisis exploratorio...\")\n",
        "profile = ProfileReport(df,\n",
        "                       title=\"An√°lisis Exploratorio - Dataset Bank Marketing (10k registros)\",\n",
        "                       correlations={\n",
        "                           \"pearson\": {\"calculate\": True},\n",
        "                           \"spearman\": {\"calculate\": True},\n",
        "                           \"kendall\": {\"calculate\": True},\n",
        "                           \"phi_k\": {\"calculate\": True},\n",
        "                       },\n",
        "                       missing_diagrams={\n",
        "                           \"matrix\": True,\n",
        "                           \"bar\": True,\n",
        "                           \"heatmap\": True,\n",
        "                       },\n",
        "                       samples={\"head\": 10})\n",
        "\n",
        "# Guardamos el reporte en HTML\n",
        "print(\"\\nüíæ Guardando reporte...\")\n",
        "profile.to_file(\"analisis_bank_marketing_10k.html\")\n",
        "\n",
        "# Mostramos informaci√≥n b√°sica del dataset\n",
        "print(\"\\nüìà Resumen del Dataset:\")\n",
        "print(f\"N√∫mero de registros: {df.shape[0]}\")\n",
        "print(f\"N√∫mero de variables: {df.shape[1]}\")\n",
        "print(\"\\nInformaci√≥n de las variables:\")\n",
        "print(df.info())\n",
        "print(\"\\nEstad√≠sticas descriptivas:\")\n",
        "print(df.describe())\n",
        "\n",
        "# Mostramos distribuci√≥n de la variable objetivo\n",
        "print(\"\\nDistribuci√≥n de la variable objetivo (y):\")\n",
        "print(\"0 = No suscribi√≥ dep√≥sito a plazo\")\n",
        "print(\"1 = S√≠ suscribi√≥ dep√≥sito a plazo\")\n",
        "print(df['y'].value_counts(normalize=True).mul(100).round(2).astype(str) + '%')\n",
        "\n",
        "print(\"\\n‚úÖ Proceso completado. El reporte detallado se ha guardado en 'analisis_bank_marketing_10k.html'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhnM48OAZplS"
      },
      "source": [
        "# ü§ñ Implementaci√≥n de Perceptr√≥n Simple\n",
        "Para nuestra primera aproximaci√≥n al deep learning, implementaremos un perceptr√≥n simple\n",
        "utilizando solo tres caracter√≠sticas del dataset:\n",
        "- age: Edad del cliente\n",
        "- balance: Saldo promedio anual\n",
        "- duration: Duraci√≥n de la √∫ltima llamada\n",
        "\n",
        "El perceptr√≥n intentar√° predecir si el cliente suscribir√° un dep√≥sito a plazo (y).\n",
        "Esta implementaci√≥n b√°sica nos ayudar√° a entender:\n",
        "1. C√≥mo funciona la propagaci√≥n hacia adelante (forward propagation)\n",
        "2. C√≥mo se realiza el entrenamiento con datos reales\n",
        "3. C√≥mo visualizar la frontera de decisi√≥n del perceptr√≥n\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmcaa76iZvSx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. PREPARACI√ìN DE DATOS\n",
        "# ----------------------\n",
        "# Seleccionamos solo 3 caracter√≠sticas para mantenerlo simple\n",
        "X = df[['age', 'balance', 'duration']]\n",
        "y = df['y']\n",
        "\n",
        "# Dividimos los datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Escalamos los datos\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 2. CREACI√ìN Y ENTRENAMIENTO DEL PERCEPTR√ìN\n",
        "# -----------------------------------------\n",
        "# Creamos un perceptr√≥n simple (una capa oculta con 3 neuronas)\n",
        "perceptron = MLPClassifier(\n",
        "    hidden_layer_sizes=(3,),  # Una capa oculta con 3 neuronas\n",
        "    activation='logistic',    # Funci√≥n de activaci√≥n sigmoide\n",
        "    solver='sgd',            # Descenso de gradiente estoc√°stico\n",
        "    max_iter=1000,           # M√°ximo n√∫mero de iteraciones\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Entrenamos el modelo\n",
        "perceptron.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 3. EVALUACI√ìN DEL MODELO\n",
        "# -----------------------\n",
        "# Hacemos predicciones\n",
        "y_pred = perceptron.predict(X_test_scaled)\n",
        "y_pred_prob = perceptron.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Calculamos ROC AUC\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# 4. VISUALIZACIONES\n",
        "# -----------------\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# 4.1 Matriz de Confusi√≥n\n",
        "plt.subplot(1, 3, 1)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Matriz de Confusi√≥n')\n",
        "plt.xlabel('Predicho')\n",
        "plt.ylabel('Real')\n",
        "\n",
        "# 4.2 Curva ROC\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2,\n",
        "         label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Tasa de Falsos Positivos')\n",
        "plt.ylabel('Tasa de Verdaderos Positivos')\n",
        "plt.title('Curva ROC')\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "# 4.3 Importancia de caracter√≠sticas\n",
        "plt.subplot(1, 3, 3)\n",
        "feature_importance = np.abs(perceptron.coefs_[0]).mean(axis=1)\n",
        "features = ['Edad', 'Balance', 'Duraci√≥n']\n",
        "plt.bar(features, feature_importance)\n",
        "plt.title('Importancia de Caracter√≠sticas')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 5. C√ÅLCULO DE M√âTRICAS DE DESEMPE√ëO\n",
        "# -------------------------------------\n",
        "# Extraer valores de la matriz de confusi√≥n - ajustado para clase 0 como positiva\n",
        "vp = cm[0,0]  # Verdaderos Positivos (NO predicho como NO)\n",
        "fn = cm[0,1]  # Falsos Negativos (NO predicho como S√ç)\n",
        "fp = cm[1,0]  # Falsos Positivos (S√ç predicho como NO)\n",
        "vn = cm[1,1]  # Verdaderos Negativos (S√ç predicho como S√ç)\n",
        "\n",
        "# Calcular m√©tricas globales\n",
        "accuracy = (vp + vn) / (vp + vn + fp + fn)\n",
        "specificity = vn / (vn + fp) if (vn + fp) > 0 else 0  # VN/(VN+FP)\n",
        "precision = vp / (vp + fp)                             # VP/(VP+FP)\n",
        "recall = vp / (vp + fn)                               # VP/(VP+FN)\n",
        "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "# Crear DataFrame para m√©tricas globales\n",
        "metrics_df = pd.DataFrame({\n",
        "    'M√©trica': ['Accuracy', 'Recall (Sensibilidad)', 'Especificidad', 'Precisi√≥n', 'F1-Score', 'ROC-AUC'],\n",
        "    'Valor': [\n",
        "        f\"{accuracy:.2%}\",\n",
        "        f\"{recall:.2%}\",\n",
        "        f\"{specificity:.2%}\",\n",
        "        f\"{precision:.2%}\",\n",
        "        f\"{f1_score:.2%}\",\n",
        "        f\"{roc_auc:.2%}\"\n",
        "    ],\n",
        "    'F√≥rmula': [\n",
        "        f\"(VP+VN)/(VP+VN+FP+FN) = ({vp}+{vn})/({vp}+{vn}+{fp}+{fn})\",\n",
        "        f\"VP/(VP+FN) = {vp}/({vp}+{fn})\",\n",
        "        f\"VN/(VN+FP) = {vn}/({vn}+{fp})\",\n",
        "        f\"VP/(VP+FP) = {vp}/({vp}+{fp})\",\n",
        "        f\"2*(Precision*Recall)/(Precision+Recall)\",\n",
        "        \"√Årea bajo la curva ROC\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "# 6. MOSTRAR RESULTADOS\n",
        "# -------------------\n",
        "print(\"\\n=== Matriz de Confusi√≥n ===\")\n",
        "print(\"                     Predicho NO     Predicho S√ç\")\n",
        "print(f\"Real NO (0) [+]     {vp:4d} (VP)     {fn:4d} (FN)\")\n",
        "print(f\"Real S√ç (1) [-]     {fp:4d} (FP)     {vn:4d} (VN)\")\n",
        "\n",
        "print(\"\\n=== M√©tricas de Desempe√±o ===\")\n",
        "print(metrics_df.to_string(index=False))\n",
        "\n",
        "# 7. EJEMPLO DE PREDICCI√ìN\n",
        "# ----------------------\n",
        "# Tomamos un ejemplo real para mostrar c√≥mo funciona el modelo\n",
        "ejemplo = X_test.iloc[0]\n",
        "ejemplo_scaled = scaler.transform([ejemplo])\n",
        "prediccion = perceptron.predict_proba(ejemplo_scaled)[0]\n",
        "\n",
        "print(\"\\n=== Ejemplo de Predicci√≥n ===\")\n",
        "print(f\"Cliente con:\")\n",
        "print(f\"- Edad: {ejemplo['age']} a√±os\")\n",
        "print(f\"- Balance: ${ejemplo['balance']}\")\n",
        "print(f\"- Duraci√≥n de llamada: {ejemplo['duration']} segundos\")\n",
        "print(f\"Probabilidad de suscripci√≥n: {prediccion[1]:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3ZemA14hWF6"
      },
      "source": [
        "Entiendo, vamos a mejorar la explicaci√≥n introductoria para que sea m√°s completa y educativa. Aqu√≠ est√° la versi√≥n mejorada:\n",
        "\n",
        "# üß† Red Neuronal Multicapa (MLP) con Grid Search\n",
        "'''\n",
        "En esta secci√≥n implementaremos una red neuronal multicapa m√°s avanzada, utilizando t√©cnicas de optimizaci√≥n de hiperpar√°metros para encontrar la mejor configuraci√≥n posible del modelo.\n",
        "\n",
        "### üìä Arquitectura Base:\n",
        "- Capa de entrada: 3 neuronas (age, balance, duration)\n",
        "- Capas ocultas: Configuraciones variables (2,2), (4,4), (8,8)\n",
        "- Capa de salida: 1 neurona (predicci√≥n binaria)\n",
        "\n",
        "### üîç Grid Search y Su Importancia:\n",
        "El Grid Search es una t√©cnica sistem√°tica que nos permite:\n",
        "1. Probar m√∫ltiples combinaciones de hiperpar√°metros\n",
        "2. Identificar la configuraci√≥n √≥ptima del modelo\n",
        "3. Evitar el ajuste manual y subjetivo\n",
        "4. Validar el desempe√±o de forma cruzada (cross-validation)\n",
        "\n",
        "### ‚öôÔ∏è Hiperpar√°metros a Optimizar:\n",
        "\n",
        "1. **Arquitectura de la Red** (hidden_layer_sizes):\n",
        "   - Diferentes combinaciones de neuronas por capa\n",
        "   - Impacta en la capacidad de aprendizaje del modelo\n",
        "   - M√°s neuronas = mayor capacidad pero riesgo de overfitting\n",
        "\n",
        "2. **Funciones de Activaci√≥n** (activation):\n",
        "   - ReLU: Mejor para capas profundas, evita el problema del desvanecimiento del gradiente\n",
        "   - Tanh: √ötil para normalizar las salidas entre -1 y 1\n",
        "\n",
        "3. **Tasa de Aprendizaje** (learning_rate_init):\n",
        "   - Controla el tama√±o de los pasos en el descenso del gradiente\n",
        "   - Valores peque√±os = aprendizaje m√°s estable pero m√°s lento\n",
        "   - Valores grandes = aprendizaje m√°s r√°pido pero riesgo de no converger\n",
        "\n",
        "4. **Tama√±o del Batch** (batch_size):\n",
        "   - Afecta la velocidad y estabilidad del entrenamiento\n",
        "   - Batches peque√±os = actualizaciones m√°s frecuentes, m√°s ruido\n",
        "   - Batches grandes = actualizaciones m√°s estables, mayor uso de memoria\n",
        "\n",
        "### üìà M√©tricas y Visualizaciones:\n",
        "Se implementan diversas formas de evaluar el modelo:\n",
        "- Matriz de confusi√≥n\n",
        "- Curva ROC y AUC\n",
        "- Curva de p√©rdida durante el entrenamiento\n",
        "- Comparaci√≥n de arquitecturas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSOHU5W_hYYK"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# 1. CONFIGURACI√ìN DE LA RED\n",
        "# -------------------------\n",
        "# Definimos diferentes configuraciones para experimentar\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(2, 2), (4, 4), (8, 8)],  # Diferentes arquitecturas\n",
        "    'activation': ['relu', 'tanh'],                   # Funciones de activaci√≥n\n",
        "    'learning_rate_init': [0.001, 0.01, 0.1],        # Tasas de aprendizaje\n",
        "    'max_iter': [1000],                              # N√∫mero de √©pocas\n",
        "    'batch_size': [32, 64, 128]                      # Tama√±o del batch\n",
        "}\n",
        "\n",
        "# 2. PREPARACI√ìN DE DATOS\n",
        "# ----------------------\n",
        "X = df[['age', 'balance', 'duration']]\n",
        "y = df['y']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 3. CREACI√ìN Y ENTRENAMIENTO DEL MODELO\n",
        "# -------------------------------------\n",
        "# Creamos el modelo base\n",
        "mlp = MLPClassifier(random_state=42)\n",
        "\n",
        "# Implementamos Grid Search con validaci√≥n cruzada\n",
        "grid_search = GridSearchCV(\n",
        "    mlp, param_grid,\n",
        "    cv=5,                  # 5-fold cross validation\n",
        "    scoring='roc_auc',     # M√©trica para optimizar\n",
        "    n_jobs=-1,            # Usar todos los cores disponibles\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Entrenamos el modelo probando todas las configuraciones\n",
        "print(\"üîÑ Iniciando entrenamiento con Grid Search...\")\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 4. EVALUACI√ìN DEL MEJOR MODELO\n",
        "# ----------------------------\n",
        "print(\"\\nüèÜ Mejores hiperpar√°metros encontrados:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Obtenemos predicciones con el mejor modelo\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "y_pred_prob = best_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Calculamos m√©tricas\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# 5. VISUALIZACIONES\n",
        "# ----------------\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# 5.1 Matriz de Confusi√≥n\n",
        "plt.subplot(2, 2, 1)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Matriz de Confusi√≥n')\n",
        "plt.xlabel('Predicho')\n",
        "plt.ylabel('Real')\n",
        "\n",
        "# 5.2 Curva ROC\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2,\n",
        "         label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Tasa de Falsos Positivos')\n",
        "plt.ylabel('Tasa de Verdaderos Positivos')\n",
        "plt.title('Curva ROC')\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "# 5.3 Curva de P√©rdida\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(best_model.loss_curve_)\n",
        "plt.title('Curva de P√©rdida Durante el Entrenamiento')\n",
        "plt.xlabel('√âpocas')\n",
        "plt.ylabel('P√©rdida')\n",
        "\n",
        "# 5.4 Comparaci√≥n de Arquitecturas\n",
        "plt.subplot(2, 2, 4)\n",
        "results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "architectures = results_df.groupby('param_hidden_layer_sizes')['mean_test_score'].mean()\n",
        "architectures.plot(kind='bar')\n",
        "plt.title('Desempe√±o por Arquitectura')\n",
        "plt.xlabel('Arquitectura')\n",
        "plt.ylabel('ROC AUC Promedio')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 6. M√âTRICAS DE DESEMPE√ëO\n",
        "# -----------------------\n",
        "# Extraemos valores de la matriz de confusi√≥n\n",
        "vp = cm[0,0]  # Verdaderos Positivos\n",
        "fn = cm[0,1]  # Falsos Negativos\n",
        "fp = cm[1,0]  # Falsos Positivos\n",
        "vn = cm[1,1]  # Verdaderos Negativos\n",
        "\n",
        "# Calculamos m√©tricas\n",
        "accuracy = (vp + vn) / (vp + vn + fp + fn)\n",
        "specificity = vn / (vn + fp)\n",
        "precision = vp / (vp + fp)\n",
        "recall = vp / (vp + fn)\n",
        "f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "# Creamos DataFrame de m√©tricas\n",
        "metrics_df = pd.DataFrame({\n",
        "    'M√©trica': ['Accuracy', 'Recall', 'Especificidad', 'Precisi√≥n', 'F1-Score', 'ROC-AUC'],\n",
        "    'Valor': [\n",
        "        f\"{accuracy:.2%}\",\n",
        "        f\"{recall:.2%}\",\n",
        "        f\"{specificity:.2%}\",\n",
        "        f\"{precision:.2%}\",\n",
        "        f\"{f1_score:.2%}\",\n",
        "        f\"{roc_auc:.2%}\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\nüìä M√©tricas de Desempe√±o del Mejor Modelo:\")\n",
        "print(metrics_df.to_string(index=False))\n",
        "\n",
        "# 7. GUARDADO DE RESULTADOS\n",
        "# ------------------------\n",
        "# Guardamos los resultados del Grid Search para an√°lisis posterior\n",
        "results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "results_df.to_csv('grid_search_results.csv', index=False)\n",
        "print(\"\\nüíæ Resultados del Grid Search guardados en 'grid_search_results.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0bbikPyk0CX"
      },
      "source": [
        "## üìä An√°lisis de Resultados del Modelo MLP\n",
        "\n",
        "üéØ **Rendimiento y Matriz de Confusi√≥n**: Los resultados son excelentes, con una precisi√≥n global del 95.94%. En la matriz de confusi√≥n podemos ver que el modelo identific√≥ correctamente 1,876 casos de no suscripci√≥n (verdaderos positivos, y=0) y 42 casos de suscripci√≥n (verdaderos negativos, y=1). Solo hubo 54 falsos negativos (casos predichos como suscripci√≥n cuando no suscribieron) y 28 falsos positivos (casos predichos como no suscripci√≥n cuando s√≠ suscribieron).\n",
        "\n",
        "üìà **M√©tricas de Desempe√±o**: El modelo muestra un recall (sensibilidad) del 97.20%, lo que significa que identifica correctamente el 97.20% de los casos de no suscripci√≥n. La especificidad del 60% indica que identifica correctamente el 60% de los casos de suscripci√≥n. La precisi√≥n del 98.53% muestra que cuando el modelo predice que alguien no suscribir√°, acierta el 98.53% de las veces.\n",
        "\n",
        "üåü **Conclusi√≥n Final**: La curva ROC con un AUC de 0.96 confirma el excelente poder discriminativo del modelo. La arquitectura simple (2,2) con activaci√≥n tanh y batch size de 128 result√≥ ser la configuraci√≥n √≥ptima, demostrando que para este problema, una red m√°s simple es m√°s efectiva que arquitecturas m√°s complejas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slMw1Ix1unm9"
      },
      "outputs": [],
      "source": [
        "# Visualizaci√≥n de hiperpar√°metros y pesos optimizados\n",
        "print(\"\\nüèÜ MEJORES HIPERPAR√ÅMETROS ENCONTRADOS:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Arquitectura: {best_model.hidden_layer_sizes}\")\n",
        "print(f\"Funci√≥n de activaci√≥n: {best_model.activation}\")\n",
        "print(f\"Tasa de aprendizaje: {best_model.learning_rate_init}\")\n",
        "print(f\"Tama√±o del batch: {best_model.batch_size}\")\n",
        "\n",
        "print(\"\\nüßÆ PESOS OPTIMIZADOS POR CAPA:\")\n",
        "print(\"-\" * 50)\n",
        "for i, (pesos, sesgos) in enumerate(zip(best_model.coefs_, best_model.intercepts_)):\n",
        "    print(f\"\\nCapa {i+1}:\")\n",
        "    print(f\"Matriz de pesos:\\n{pesos}\")\n",
        "    print(f\"Vector de sesgos:\\n{sesgos}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "449CSXGrxGRl"
      },
      "source": [
        "## üìä An√°lisis de la Red Neuronal Optimizada\n",
        "\n",
        "üß† La red neuronal encontr√≥ su **configuraci√≥n √≥ptima** con una arquitectura simple pero efectiva de (2, 2) neuronas en sus capas ocultas, utilizando la funci√≥n de activaci√≥n **tanh** y una tasa de aprendizaje relativamente alta de 0.1.\n",
        "\n",
        "Los **pesos optimizados** muestran un patr√≥n interesante: en la primera capa, los valores oscilan aproximadamente entre -7 y 0.8, sugiriendo que el modelo aprendi√≥ a dar diferentes niveles de importancia a las caracter√≠sticas de entrada. La segunda capa muestra pesos m√°s uniformes alrededor de 0.9 a 3.7, indicando una consolidaci√≥n de los patrones aprendidos, mientras que la capa final refina estos patrones para la predicci√≥n final.\n",
        "\n",
        "Esta estructura de pesos, junto con un tama√±o de batch de 128, sugiere que el modelo encontr√≥ un buen **balance entre generalizaci√≥n y precisi√≥n.** üéØ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNCU5WTbydsu"
      },
      "source": [
        "# üß† Deep Learning: Optimizaci√≥n y T√©cnicas de Regularizaci√≥n\n",
        "\n",
        "### üìù ¬øQu√© vamos a hacer?\n",
        "Construiremos una red neuronal con m√∫ltiples capas para nuestro problema de marketing bancario, implementando t√©cnicas modernas de optimizaci√≥n y regularizaci√≥n. El c√≥digo est√° dise√±ado para permitir la experimentaci√≥n con diferentes configuraciones de hiperpar√°metros, facilitando el aprendizaje sobre su impacto en el rendimiento del modelo.\n",
        "\n",
        "### ‚ùå Desaf√≠os en el Entrenamiento\n",
        "\n",
        "1. **Vanishing Gradient (Gradiente Desvaneciente)**\n",
        "   - Los gradientes se vuelven muy peque√±os al propagarse hacia atr√°s\n",
        "   - Las primeras capas aprenden muy lentamente\n",
        "   - Para combatirlo usamos:\n",
        "     * ReLU como funci√≥n de activaci√≥n\n",
        "     * Batch Normalization en cada capa\n",
        "     * Inicializaci√≥n apropiada de pesos\n",
        "\n",
        "2. **Overfitting (Sobreajuste)**\n",
        "   - El modelo memoriza en lugar de generalizar\n",
        "   - Se√±ales de overfitting:\n",
        "     * Buena precisi√≥n en entrenamiento pero mala en validaci√≥n\n",
        "     * La p√©rdida de validaci√≥n aumenta mientras la de entrenamiento baja\n",
        "   - Lo controlamos con:\n",
        "     * Dropout configurable\n",
        "     * Regularizaci√≥n L2 ajustable\n",
        "     * Early Stopping con paciencia personalizable\n",
        "\n",
        "3. **Selecci√≥n de Hiperpar√°metros**\n",
        "   - M√∫ltiples hiperpar√°metros para experimentar:\n",
        "     * N√∫mero de capas\n",
        "     * Neuronas por capa\n",
        "     * Tasa de aprendizaje\n",
        "     * Tama√±o de batch\n",
        "     * Todos f√°cilmente ajustables en el c√≥digo\n",
        "\n",
        "### ‚úÖ Soluciones Implementadas\n",
        "\n",
        "1. **Batch Normalization**\n",
        "   - Aplicada despu√©s de cada capa densa\n",
        "   - Normaliza las activaciones\n",
        "   - Acelera el entrenamiento\n",
        "   - Reduce dependencia de la inicializaci√≥n\n",
        "\n",
        "2. **Regularizaci√≥n L2**\n",
        "   - Implementada en cada capa densa\n",
        "   - Valores configurables para experimentaci√≥n\n",
        "   - Penaliza pesos grandes\n",
        "   - Promueve soluciones m√°s simples\n",
        "\n",
        "3. **Dropout**\n",
        "   - Aplicado despu√©s de cada capa\n",
        "   - Porcentaje de dropout ajustable\n",
        "   - Previene la co-adaptaci√≥n\n",
        "   - Mejora la generalizaci√≥n\n",
        "\n",
        "4. **Optimizaci√≥n Autom√°tica**\n",
        "   - B√∫squeda aleatoria de hiperpar√°metros\n",
        "   - N√∫mero de pruebas configurable\n",
        "   - Validaci√≥n cruzada con split ajustable\n",
        "   - Early stopping personalizable\n",
        "\n",
        "5. **Arquitectura Flexible**\n",
        "   - ReLU como activaci√≥n en capas ocultas\n",
        "   - Sigmoid en la capa de salida\n",
        "   - Batch Normalization + Dropout en cada capa\n",
        "   - Adam como optimizador\n",
        "   - Estructura personalizable para experimentaci√≥n\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dn_LwcM15X7w"
      },
      "outputs": [],
      "source": [
        "# Se debe reiniciar el entorno de ejecuci√≥n para correr este bloque de c√≥digo.\n",
        "\n",
        "# Instalamos las dependencias\n",
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install keras-tuner\n",
        "!pip install datasets\n",
        "!pip install pandas numpy scikit-learn matplotlib seaborn\n",
        "\n",
        "# ======================\n",
        "# 1. IMPORTACI√ìN DE LIBRER√çAS\n",
        "# ======================\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import keras_tuner as kt  # Para b√∫squeda de hiperpar√°metros\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Fijamos una semilla para reproducibilidad\n",
        "RANDOM_SEED = 42\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# ======================\n",
        "# 2. CONFIGURACI√ìN DE HIPERPAR√ÅMETROS\n",
        "# ======================\n",
        "# Definimos el rango de valores que puede tomar cada hiperpar√°metro\n",
        "HP_RANGES = {\n",
        "    # N√∫mero de capas ocultas (entre 3 y 4)\n",
        "    'n_layers': {\n",
        "        'min_value': 3,\n",
        "        'max_value': 4,\n",
        "        'step': 1\n",
        "    },\n",
        "\n",
        "    # N√∫mero de neuronas por capa (32 o 64)\n",
        "    'units': {\n",
        "        'min_value': 32,\n",
        "        'max_value': 64,\n",
        "        'step': 32\n",
        "    },\n",
        "\n",
        "    # Tasa de dropout para regularizaci√≥n (entre 0.2 y 0.4)\n",
        "    'dropout': {\n",
        "        'min_value': 0.2,\n",
        "        'max_value': 0.4,\n",
        "        'step': 0.1\n",
        "    },\n",
        "\n",
        "    # Tasa de aprendizaje del optimizador\n",
        "    'learning_rate': [0.001, 0.01],  # Probamos dos valores\n",
        "\n",
        "    # Factor de regularizaci√≥n L2\n",
        "    'l2_lambda': [0.0001, 0.001]  # Probamos dos valores\n",
        "}\n",
        "\n",
        "# Configuraci√≥n del entrenamiento\n",
        "TRAINING_CONFIG = {\n",
        "    'epochs': 15,          # N√∫mero de pasadas por todo el dataset\n",
        "    'batch_size': 64,      # Tama√±o de los mini-lotes\n",
        "    'max_trials': 5        # N√∫mero de combinaciones de hiperpar√°metros a probar\n",
        "}\n",
        "\n",
        "# ======================\n",
        "# 3. DEFINICI√ìN DEL MODELO\n",
        "# ======================\n",
        "def create_model(hp):\n",
        "    \"\"\"\n",
        "    Funci√≥n que crea el modelo con los hiperpar√°metros que se est√°n probando.\n",
        "    hp: objeto que permite acceder a los hiperpar√°metros que se est√°n evaluando\n",
        "    \"\"\"\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # Decidimos cu√°ntas capas tendr√° el modelo\n",
        "    n_layers = hp.Int('n_layers', **HP_RANGES['n_layers'])\n",
        "\n",
        "    # Primera capa (capa de entrada)\n",
        "    model.add(keras.layers.Dense(\n",
        "        units=hp.Int('units_0', **HP_RANGES['units']),  # N√∫mero de neuronas\n",
        "        input_shape=(3,),  # 3 caracter√≠sticas de entrada\n",
        "        kernel_regularizer=keras.regularizers.l2(  # Regularizaci√≥n L2\n",
        "            hp.Choice('l2_0', HP_RANGES['l2_lambda'])\n",
        "        )\n",
        "    ))\n",
        "    # A√±adimos capas auxiliares para mejorar el entrenamiento\n",
        "    model.add(keras.layers.BatchNormalization())  # Normaliza las activaciones\n",
        "    model.add(keras.layers.ReLU())  # Funci√≥n de activaci√≥n\n",
        "    model.add(keras.layers.Dropout(  # Previene el sobreajuste\n",
        "        hp.Float('dropout_0', **HP_RANGES['dropout'])\n",
        "    ))\n",
        "\n",
        "    # Agregamos las capas ocultas adicionales\n",
        "    for i in range(1, n_layers):\n",
        "        model.add(keras.layers.Dense(\n",
        "            units=hp.Int(f'units_{i}', **HP_RANGES['units']),\n",
        "            kernel_regularizer=keras.regularizers.l2(\n",
        "                hp.Choice(f'l2_{i}', HP_RANGES['l2_lambda'])\n",
        "            )\n",
        "        ))\n",
        "        model.add(keras.layers.BatchNormalization())\n",
        "        model.add(keras.layers.ReLU())\n",
        "        model.add(keras.layers.Dropout(\n",
        "            hp.Float(f'dropout_{i}', **HP_RANGES['dropout'])\n",
        "        ))\n",
        "\n",
        "    # Capa de salida (una neurona con sigmoid para clasificaci√≥n binaria)\n",
        "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Compilamos el modelo\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(\n",
        "            hp.Choice('learning_rate', HP_RANGES['learning_rate'])\n",
        "        ),\n",
        "        loss='binary_crossentropy',  # Funci√≥n de p√©rdida para clasificaci√≥n binaria\n",
        "        metrics=['accuracy']  # M√©trica que queremos monitorear\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# ======================\n",
        "# 4. PREPARACI√ìN DE DATOS\n",
        "# ======================\n",
        "# Cargamos y preparamos los datos\n",
        "dataset = load_dataset(\"cestwc/bank-marketing\", streaming=True)\n",
        "df = pd.DataFrame(list(dataset['train'].shuffle(seed=42).take(10000)))\n",
        "\n",
        "# Seleccionamos solo 3 caracter√≠sticas\n",
        "X = df[['age', 'balance', 'duration']]\n",
        "y = df['y'].astype(int)\n",
        "\n",
        "# Dividimos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Escalamos los datos\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ======================\n",
        "# 5. B√öSQUEDA DE MEJORES HIPERPAR√ÅMETROS\n",
        "# ======================\n",
        "# Iniciamos la b√∫squeda aleatoria de hiperpar√°metros\n",
        "tuner = kt.RandomSearch(\n",
        "    create_model,\n",
        "    objective='val_accuracy',  # Optimizamos bas√°ndonos en la precisi√≥n\n",
        "    max_trials=TRAINING_CONFIG['max_trials'],\n",
        "    directory='keras_tuner',\n",
        "    project_name='bank_marketing',\n",
        "    overwrite=True\n",
        ")\n",
        "\n",
        "# Configuramos early stopping para evitar sobreajuste\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=2,  # Esperamos 2 √©pocas antes de parar\n",
        "    restore_best_weights=True  # Guardamos los mejores pesos\n",
        ")\n",
        "\n",
        "# Realizamos la b√∫squeda\n",
        "tuner.search(\n",
        "    X_train_scaled,\n",
        "    y_train,\n",
        "    epochs=TRAINING_CONFIG['epochs'],\n",
        "    batch_size=TRAINING_CONFIG['batch_size'],\n",
        "    validation_split=0.2,  # 20% de los datos para validaci√≥n\n",
        "    callbacks=[stop_early],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ======================\n",
        "# 6. ENTRENAMIENTO DEL MEJOR MODELO\n",
        "# ======================\n",
        "# Obtenemos los mejores hiperpar√°metros encontrados\n",
        "best_hps = tuner.get_best_hyperparameters()[0]\n",
        "print(\"\\nüèÜ Mejores hiperpar√°metros encontrados:\")\n",
        "print(\"N√∫mero de capas:\", best_hps.get('n_layers'))\n",
        "print(\"Tasa de aprendizaje:\", best_hps.get('learning_rate'))\n",
        "\n",
        "# Entrenamos el modelo final con los mejores hiperpar√°metros\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "history = best_model.fit(\n",
        "    X_train_scaled,\n",
        "    y_train,\n",
        "    epochs=TRAINING_CONFIG['epochs'],\n",
        "    batch_size=TRAINING_CONFIG['batch_size'],\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluamos el modelo final\n",
        "test_loss, test_acc = best_model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "print(f\"\\nüìä Precisi√≥n final en test: {test_acc:.2%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "demk2kkv80bH"
      },
      "outputs": [],
      "source": [
        "# ======================\n",
        "# 7. EVALUACI√ìN COMPLETA (CON Y=0 COMO CLASE POSITIVA)\n",
        "# ======================\n",
        "from sklearn.metrics import classification_report, roc_curve, auc, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Predicciones\n",
        "y_probs = best_model.predict(X_test_scaled)\n",
        "y_pred = (y_probs > 0.5).astype(int)\n",
        "\n",
        "# Calculamos la matriz de confusi√≥n primero - esto es necesario para los c√°lculos siguientes\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# 1. Visualizaci√≥n de la funci√≥n de p√©rdida\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='P√©rdida de entrenamiento')\n",
        "plt.plot(history.history['val_loss'], label='P√©rdida de validaci√≥n')\n",
        "plt.title('Funci√≥n de P√©rdida Durante el Entrenamiento')\n",
        "plt.xlabel('√âpoca')\n",
        "plt.ylabel('P√©rdida')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Precisi√≥n de entrenamiento')\n",
        "plt.plot(history.history['val_accuracy'], label='Precisi√≥n de validaci√≥n')\n",
        "plt.title('Precisi√≥n Durante el Entrenamiento')\n",
        "plt.xlabel('√âpoca')\n",
        "plt.ylabel('Precisi√≥n')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Matriz de Confusi√≥n\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['No (0)', 'S√≠ (1)'], yticklabels=['No (0)', 'S√≠ (1)'])\n",
        "plt.title('Matriz de Confusi√≥n')\n",
        "plt.xlabel('Predicho')\n",
        "plt.ylabel('Real')\n",
        "plt.show()\n",
        "\n",
        "# 3. Extraer valores de la matriz de confusi√≥n\n",
        "# Para Y=0 como clase positiva:\n",
        "# [[TP, FN],\n",
        "#  [FP, TN]]\n",
        "TP = cm[0,0]  # Verdaderos Positivos (predicho correctamente como 0)\n",
        "FN = cm[0,1]  # Falsos Negativos (predicho como 1 cuando era 0)\n",
        "FP = cm[1,0]  # Falsos Positivos (predicho como 0 cuando era 1)\n",
        "TN = cm[1,1]  # Verdaderos Negativos (predicho correctamente como 1)\n",
        "\n",
        "# 4. Calcular m√©tricas manualmente\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "precision = TP / (TP + FP) if (TP + FP) > 0 else 0  # Evitar divisi√≥n por cero\n",
        "recall = TP / (TP + FN) if (TP + FN) > 0 else 0  # Evitar divisi√≥n por cero\n",
        "specificity = TN / (TN + FP) if (TN + FP) > 0 else 0  # Evitar divisi√≥n por cero\n",
        "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0  # Evitar divisi√≥n por cero\n",
        "\n",
        "# 5. Curva ROC y AUC\n",
        "# Invertir las probabilidades para Y=0 como clase positiva\n",
        "y_probs_inverted = 1 - y_probs\n",
        "\n",
        "# Calcular la curva ROC con Y=0 como clase positiva\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_probs_inverted, pos_label=0)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Graficar la curva ROC\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlabel('Tasa de Falsos Positivos (FPR)')\n",
        "plt.ylabel('Tasa de Verdaderos Positivos (TPR)')\n",
        "plt.title('Curva ROC (Y=0 como Clase Positiva)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "# 6. Reporte de Clasificaci√≥n\n",
        "print(\"\\nüìù Reporte de Clasificaci√≥n (Y=0 como Clase Positiva):\")\n",
        "print(classification_report(y_test, y_pred, target_names=['No (0)', 'S√≠ (1)'], zero_division=0))\n",
        "\n",
        "# 7. Resumen de M√©tricas\n",
        "metrics_df = pd.DataFrame({\n",
        "    'M√©trica': ['P√©rdida final', 'P√©rdida de validaci√≥n final', 'Precisi√≥n', 'Recall', 'F1-Score',\n",
        "                'AUC', 'Exactitud (Accuracy)', 'Especificidad'],\n",
        "    'Valor': [\n",
        "        f\"{history.history['loss'][-1]:.3f}\",\n",
        "        f\"{history.history['val_loss'][-1]:.3f}\",\n",
        "        f\"{precision:.3f}\",\n",
        "        f\"{recall:.3f}\",\n",
        "        f\"{f1:.3f}\",\n",
        "        f\"{roc_auc:.3f}\",\n",
        "        f\"{accuracy:.3f}\",\n",
        "        f\"{specificity:.3f}\"\n",
        "    ],\n",
        "    'F√≥rmula': [\n",
        "        'Binary Crossentropy',\n",
        "        'Binary Crossentropy en validaci√≥n',\n",
        "        f'TP/(TP+FP) = {TP}/{TP+FP}',\n",
        "        f'TP/(TP+FN) = {TP}/{TP+FN}',\n",
        "        'Media arm√≥nica de Precisi√≥n y Recall',\n",
        "        '√Årea bajo la curva ROC',\n",
        "        f'(TP+TN)/(TP+TN+FP+FN) = {TP+TN}/{TP+TN+FP+FN}',\n",
        "        f'TN/(TN+FP) = {TN}/{TN+FP}'\n",
        "    ],\n",
        "    'Descripci√≥n': [\n",
        "        'P√©rdida en conjunto de entrenamiento',\n",
        "        'P√©rdida en conjunto de validaci√≥n',\n",
        "        'Porcentaje de predicciones positivas correctas',\n",
        "        'Porcentaje de casos positivos identificados',\n",
        "        'Balance entre precisi√≥n y recall',\n",
        "        'Capacidad discriminativa del modelo',\n",
        "        'Porcentaje total de predicciones correctas',\n",
        "        'Porcentaje de casos negativos identificados correctamente'\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\nüìä Resumen de M√©tricas (Y=0 como Clase Positiva):\")\n",
        "# Intentamos usar to_markdown si est√° disponible, si no usamos to_string\n",
        "try:\n",
        "    print(metrics_df.to_markdown(index=False))\n",
        "except AttributeError:\n",
        "    print(metrics_df.to_string(index=False))\n",
        "\n",
        "# 8. Imprimir valores de la matriz de confusi√≥n para verificaci√≥n\n",
        "print(\"\\nüîç Valores de la Matriz de Confusi√≥n:\")\n",
        "print(f\"Verdaderos Positivos (TP): {TP}\")\n",
        "print(f\"Falsos Negativos (FN): {FN}\")\n",
        "print(f\"Falsos Positivos (FP): {FP}\")\n",
        "print(f\"Verdaderos Negativos (TN): {TN}\")\n",
        "\n",
        "# 9. Imprimir valores finales de p√©rdida\n",
        "print(\"\\nüìâ Valores Finales de P√©rdida:\")\n",
        "print(f\"P√©rdida de entrenamiento final: {history.history['loss'][-1]:.4f}\")\n",
        "print(f\"P√©rdida de validaci√≥n final: {history.history['val_loss'][-1]:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOGKEcWVtF4Duy46lUb/Vnh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}