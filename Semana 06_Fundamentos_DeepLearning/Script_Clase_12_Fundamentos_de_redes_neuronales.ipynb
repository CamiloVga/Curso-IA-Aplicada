{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CamiloVga/Curso-IA-Aplicada/blob/main/Script_Clase_12_Fundamentos_de_redes_neuronales.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qIH_dNAWvIe"
      },
      "source": [
        "# 🤖 Inteligencia Artificial Aplicada para la Economía\n",
        "## Universidad de los Andes\n",
        "\n",
        "### 👨‍🏫 Profesores\n",
        "- **Profesor Magistral:** [Camilo Vega Barbosa](https://www.linkedin.com/in/camilo-vega-169084b1/)\n",
        "- **Asistente de Docencia:** [Sergio Julian Zona Moreno](https://www.linkedin.com/in/sergio-julian-zona-moreno/)\n",
        "\n",
        "### 📚 Ejercicio Práctico: Predicción de Admisiones en Escuelas de Derecho\n",
        "Este notebook demuestra la implementación práctica de conceptos fundamentales de Deep Learning:\n",
        "\n",
        "1. **Fundamentos de Redes Neuronales**\n",
        "   - Arquitectura básica\n",
        "   - Proceso de forward propagation\n",
        "   - Función de activación\n",
        "\n",
        "2. **Perceptrón Simple**\n",
        "   - Implementación desde cero\n",
        "   - Entrenamiento con datos reales\n",
        "   - Visualización de la frontera de decisión\n",
        "\n",
        "3. **Perceptrón Multicapa (MLP)**\n",
        "   - Arquitectura con capas ocultas\n",
        "   - Funciones de activación ReLU y Sigmoid\n",
        "   - Proceso de entrenamiento\n",
        "\n",
        "4. **Deep Learning**\n",
        "   - Implementación de red con 4 capas\n",
        "   - Técnicas de backpropagation\n",
        "   - Optimización de hiperparámetros\n",
        "   - Grid Search para calibración\n",
        "\n",
        "### 🎯 Objetivo\n",
        "Predecir la admisión de estudiantes a escuelas de derecho utilizando variables clave:\n",
        "- LSAT score\n",
        "- GPA\n",
        "- Estado de residencia\n",
        "\n",
        "\n",
        "### Requisitos Técnicos:\n",
        "- **Token de Hugging Face**: Necesario para acceder al dataset. Puedes obtener tu token en [Hugging Face](https://huggingface.co/settings/tokens)\n",
        "- **Entorno de Ejecución**:\n",
        "  - Recomendado: GPU T4 (Cambiar en: Runtime -> Change runtime type -> GPU)\n",
        "- **Memoria RAM**: Mínimo 4GB recomendados\n",
        "- **Espacio en Disco**: ~200 GB para datasets y modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK_1_tEjYN7E"
      },
      "source": [
        "# 🛠️ Instalaciones Necesarias\n",
        "\n",
        "Antes de comenzar, necesitamos instalar algunas librerías específicas para nuestro análisis de Deep Learning. A continuación se listan las librerías y su propósito:\n",
        "\n",
        "### Manejo de Datos y Cálculos\n",
        "- **pandas**: Manipulación y análisis de datos tabulares\n",
        "- **numpy**: Operaciones numéricas y matrices multidimensionales\n",
        "- **scipy**: Funciones científicas y estadísticas avanzadas\n",
        "- **ydata-profiling**: Generación automatizada de reportes de análisis exploratorio de datos\n",
        "\n",
        "### Deep Learning y Machine Learning\n",
        "- **tensorflow**: Framework principal para implementación de redes neuronales\n",
        "- **keras**: API de alto nivel para construcción de modelos de deep learning\n",
        "- **scikit-learn**: Herramientas complementarias para preprocesamiento y evaluación\n",
        "- **pytorch**: Framework alternativo para deep learning, popular en investigación y desarrollo\n",
        "\n",
        "### Visualización\n",
        "- **matplotlib**: Biblioteca base para crear gráficos estáticos\n",
        "- **seaborn**: Visualizaciones estadísticas de alto nivel\n",
        "- **plotly**: Gráficos interactivos para análisis detallado\n",
        "\n",
        "### Optimización y Evaluación\n",
        "- **optuna**: Optimización automática de hiperparámetros\n",
        "- **tensorboard**: Visualización y monitoreo del entrenamiento\n",
        "- **scikit-optimize**: Optimización de hiperparámetros y búsqueda en grid\n",
        "\n",
        "### Utilidades\n",
        "- **tqdm**: Barras de progreso para monitorear procesos largos\n",
        "- **joblib**: Paralelización y caching de operaciones computacionales\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nKU_Df7V9Gk",
        "outputId": "0f3e8dff-9cfb-4e57-eeba-cd3b9e9c389a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Instalación silenciosa de librerías\n",
        "# Frameworks de Deep Learning\n",
        "!pip install -q tensorflow\n",
        "!pip install -q torch\n",
        "!pip install -q keras\n",
        "!pip install -q keras-tuner\n",
        "\n",
        "# Análisis y manipulación de datos\n",
        "!pip install -q pandas\n",
        "!pip install -q numpy\n",
        "!pip install -q scikit-learn\n",
        "\n",
        "# Visualización\n",
        "!pip install -q matplotlib\n",
        "!pip install -q seaborn\n",
        "!pip install -q plotly\n",
        "\n",
        "# Optimización y evaluación\n",
        "!pip install -q optuna\n",
        "!pip install -q tensorboard\n",
        "!pip install -q scikit-optimize\n",
        "\n",
        "# Utilidades\n",
        "!pip install -q tqdm\n",
        "!pip install -q joblib\n",
        "\n",
        "# Análisis exploratorio y acceso a datos\n",
        "!pip install -q ydata-profiling\n",
        "!pip install -q datasets\n",
        "!pip install -q huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lk9-p-tsaj2X"
      },
      "source": [
        "# 📊 Base de Datos: Bank Marketing\n",
        "\n",
        "Para este ejercicio utilizaremos la base de datos \"Bank Marketing\" disponible en [HuggingFace](https://huggingface.co/datasets/cestwc/bank-marketing), que contiene información sobre campañas de marketing de una institución bancaria portuguesa.\n",
        "\n",
        "## 📝 Descripción\n",
        "La base contiene información detallada sobre campañas de marketing directo (llamadas telefónicas) de una institución bancaria portuguesa. El objetivo es predecir si el cliente suscribirá un depósito a plazo (variable 'y').\n",
        "\n",
        "## 🔍 Variables Disponibles\n",
        "\n",
        "### Información del Cliente\n",
        "- **age**: Edad del cliente\n",
        "- **job**: Tipo de trabajo\n",
        "- **marital**: Estado civil\n",
        "- **education**: Nivel educativo\n",
        "- **default**: ¿Tiene crédito en default?\n",
        "- **balance**: Saldo promedio anual\n",
        "- **housing**: ¿Tiene préstamo de vivienda?\n",
        "- **loan**: ¿Tiene préstamo personal?\n",
        "\n",
        "### Información de la Campaña Actual\n",
        "- **contact**: Tipo de contacto\n",
        "- **day**: Último día de contacto del mes\n",
        "- **month**: Último mes de contacto\n",
        "- **duration**: Duración del último contacto en segundos\n",
        "- **campaign**: Número de contactos realizados durante esta campaña\n",
        "- **pdays**: Días transcurridos desde el último contacto\n",
        "- **previous**: Número de contactos realizados antes de esta campaña\n",
        "- **poutcome**: Resultado de la campaña de marketing anterior\n",
        "\n",
        "### Variable Objetivo\n",
        "- **y**: ¿El cliente suscribió un depósito a plazo? (0=no, 1=sí)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ra9PAYzYMRu"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Importamos las librerías necesarias\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from ydata_profiling import ProfileReport\n",
        "\n",
        "# Cargamos el dataset y lo mezclamos\n",
        "print(\"⏳ Cargando y mezclando 10,000 registros aleatorios de Bank Marketing desde HuggingFace...\")\n",
        "dataset = load_dataset(\"cestwc/bank-marketing\", streaming=True)\n",
        "df = pd.DataFrame(list(dataset['train'].shuffle(seed=42).take(10000)))\n",
        "\n",
        "print(\"\\n📊 Generando reporte de análisis exploratorio...\")\n",
        "profile = ProfileReport(df,\n",
        "                       title=\"Análisis Exploratorio - Dataset Bank Marketing (10k registros)\",\n",
        "                       correlations={\n",
        "                           \"pearson\": {\"calculate\": True},\n",
        "                           \"spearman\": {\"calculate\": True},\n",
        "                           \"kendall\": {\"calculate\": True},\n",
        "                           \"phi_k\": {\"calculate\": True},\n",
        "                       },\n",
        "                       missing_diagrams={\n",
        "                           \"matrix\": True,\n",
        "                           \"bar\": True,\n",
        "                           \"heatmap\": True,\n",
        "                       },\n",
        "                       samples={\"head\": 10})\n",
        "\n",
        "# Guardamos el reporte en HTML\n",
        "print(\"\\n💾 Guardando reporte...\")\n",
        "profile.to_file(\"analisis_bank_marketing_10k.html\")\n",
        "\n",
        "# Mostramos información básica del dataset\n",
        "print(\"\\n📈 Resumen del Dataset:\")\n",
        "print(f\"Número de registros: {df.shape[0]}\")\n",
        "print(f\"Número de variables: {df.shape[1]}\")\n",
        "print(\"\\nInformación de las variables:\")\n",
        "print(df.info())\n",
        "print(\"\\nEstadísticas descriptivas:\")\n",
        "print(df.describe())\n",
        "\n",
        "# Mostramos distribución de la variable objetivo\n",
        "print(\"\\nDistribución de la variable objetivo (y):\")\n",
        "print(\"0 = No suscribió depósito a plazo\")\n",
        "print(\"1 = Sí suscribió depósito a plazo\")\n",
        "print(df['y'].value_counts(normalize=True).mul(100).round(2).astype(str) + '%')\n",
        "\n",
        "print(\"\\n✅ Proceso completado. El reporte detallado se ha guardado en 'analisis_bank_marketing_10k.html'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhnM48OAZplS"
      },
      "source": [
        "# 🤖 Implementación de Perceptrón Simple\n",
        "Para nuestra primera aproximación al deep learning, implementaremos un perceptrón simple\n",
        "utilizando solo tres características del dataset:\n",
        "- age: Edad del cliente\n",
        "- balance: Saldo promedio anual\n",
        "- duration: Duración de la última llamada\n",
        "\n",
        "El perceptrón intentará predecir si el cliente suscribirá un depósito a plazo (y).\n",
        "Esta implementación básica nos ayudará a entender:\n",
        "1. Cómo funciona la propagación hacia adelante (forward propagation)\n",
        "2. Cómo se realiza el entrenamiento con datos reales\n",
        "3. Cómo visualizar la frontera de decisión del perceptrón\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmcaa76iZvSx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. PREPARACIÓN DE DATOS\n",
        "# ----------------------\n",
        "# Seleccionamos solo 3 características para mantenerlo simple\n",
        "X = df[['age', 'balance', 'duration']]\n",
        "y = df['y']\n",
        "\n",
        "# Dividimos los datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Escalamos los datos\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 2. CREACIÓN Y ENTRENAMIENTO DEL PERCEPTRÓN\n",
        "# -----------------------------------------\n",
        "# Creamos un perceptrón simple (una capa oculta con 3 neuronas)\n",
        "perceptron = MLPClassifier(\n",
        "    hidden_layer_sizes=(3,),  # Una capa oculta con 3 neuronas\n",
        "    activation='logistic',    # Función de activación sigmoide\n",
        "    solver='sgd',            # Descenso de gradiente estocástico\n",
        "    max_iter=1000,           # Máximo número de iteraciones\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Entrenamos el modelo\n",
        "perceptron.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 3. EVALUACIÓN DEL MODELO\n",
        "# -----------------------\n",
        "# Hacemos predicciones\n",
        "y_pred = perceptron.predict(X_test_scaled)\n",
        "y_pred_prob = perceptron.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Calculamos ROC AUC\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# 4. VISUALIZACIONES\n",
        "# -----------------\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# 4.1 Matriz de Confusión\n",
        "plt.subplot(1, 3, 1)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Matriz de Confusión')\n",
        "plt.xlabel('Predicho')\n",
        "plt.ylabel('Real')\n",
        "\n",
        "# 4.2 Curva ROC\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2,\n",
        "         label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Tasa de Falsos Positivos')\n",
        "plt.ylabel('Tasa de Verdaderos Positivos')\n",
        "plt.title('Curva ROC')\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "# 4.3 Importancia de características\n",
        "plt.subplot(1, 3, 3)\n",
        "feature_importance = np.abs(perceptron.coefs_[0]).mean(axis=1)\n",
        "features = ['Edad', 'Balance', 'Duración']\n",
        "plt.bar(features, feature_importance)\n",
        "plt.title('Importancia de Características')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 5. CÁLCULO DE MÉTRICAS DE DESEMPEÑO\n",
        "# -------------------------------------\n",
        "# Extraer valores de la matriz de confusión - ajustado para clase 0 como positiva\n",
        "vp = cm[0,0]  # Verdaderos Positivos (NO predicho como NO)\n",
        "fn = cm[0,1]  # Falsos Negativos (NO predicho como SÍ)\n",
        "fp = cm[1,0]  # Falsos Positivos (SÍ predicho como NO)\n",
        "vn = cm[1,1]  # Verdaderos Negativos (SÍ predicho como SÍ)\n",
        "\n",
        "# Calcular métricas globales\n",
        "accuracy = (vp + vn) / (vp + vn + fp + fn)\n",
        "specificity = vn / (vn + fp) if (vn + fp) > 0 else 0  # VN/(VN+FP)\n",
        "precision = vp / (vp + fp)                             # VP/(VP+FP)\n",
        "recall = vp / (vp + fn)                               # VP/(VP+FN)\n",
        "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "# Crear DataFrame para métricas globales\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Métrica': ['Accuracy', 'Recall (Sensibilidad)', 'Especificidad', 'Precisión', 'F1-Score', 'ROC-AUC'],\n",
        "    'Valor': [\n",
        "        f\"{accuracy:.2%}\",\n",
        "        f\"{recall:.2%}\",\n",
        "        f\"{specificity:.2%}\",\n",
        "        f\"{precision:.2%}\",\n",
        "        f\"{f1_score:.2%}\",\n",
        "        f\"{roc_auc:.2%}\"\n",
        "    ],\n",
        "    'Fórmula': [\n",
        "        f\"(VP+VN)/(VP+VN+FP+FN) = ({vp}+{vn})/({vp}+{vn}+{fp}+{fn})\",\n",
        "        f\"VP/(VP+FN) = {vp}/({vp}+{fn})\",\n",
        "        f\"VN/(VN+FP) = {vn}/({vn}+{fp})\",\n",
        "        f\"VP/(VP+FP) = {vp}/({vp}+{fp})\",\n",
        "        f\"2*(Precision*Recall)/(Precision+Recall)\",\n",
        "        \"Área bajo la curva ROC\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "# 6. MOSTRAR RESULTADOS\n",
        "# -------------------\n",
        "print(\"\\n=== Matriz de Confusión ===\")\n",
        "print(\"                     Predicho NO     Predicho SÍ\")\n",
        "print(f\"Real NO (0) [+]     {vp:4d} (VP)     {fn:4d} (FN)\")\n",
        "print(f\"Real SÍ (1) [-]     {fp:4d} (FP)     {vn:4d} (VN)\")\n",
        "\n",
        "print(\"\\n=== Métricas de Desempeño ===\")\n",
        "print(metrics_df.to_string(index=False))\n",
        "\n",
        "# 7. EJEMPLO DE PREDICCIÓN\n",
        "# ----------------------\n",
        "# Tomamos un ejemplo real para mostrar cómo funciona el modelo\n",
        "ejemplo = X_test.iloc[0]\n",
        "ejemplo_scaled = scaler.transform([ejemplo])\n",
        "prediccion = perceptron.predict_proba(ejemplo_scaled)[0]\n",
        "\n",
        "print(\"\\n=== Ejemplo de Predicción ===\")\n",
        "print(f\"Cliente con:\")\n",
        "print(f\"- Edad: {ejemplo['age']} años\")\n",
        "print(f\"- Balance: ${ejemplo['balance']}\")\n",
        "print(f\"- Duración de llamada: {ejemplo['duration']} segundos\")\n",
        "print(f\"Probabilidad de suscripción: {prediccion[1]:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3ZemA14hWF6"
      },
      "source": [
        "Entiendo, vamos a mejorar la explicación introductoria para que sea más completa y educativa. Aquí está la versión mejorada:\n",
        "\n",
        "# 🧠 Red Neuronal Multicapa (MLP) con Grid Search\n",
        "'''\n",
        "En esta sección implementaremos una red neuronal multicapa más avanzada, utilizando técnicas de optimización de hiperparámetros para encontrar la mejor configuración posible del modelo.\n",
        "\n",
        "### 📊 Arquitectura Base:\n",
        "- Capa de entrada: 3 neuronas (age, balance, duration)\n",
        "- Capas ocultas: Configuraciones variables (2,2), (4,4), (8,8)\n",
        "- Capa de salida: 1 neurona (predicción binaria)\n",
        "\n",
        "### 🔍 Grid Search y Su Importancia:\n",
        "El Grid Search es una técnica sistemática que nos permite:\n",
        "1. Probar múltiples combinaciones de hiperparámetros\n",
        "2. Identificar la configuración óptima del modelo\n",
        "3. Evitar el ajuste manual y subjetivo\n",
        "4. Validar el desempeño de forma cruzada (cross-validation)\n",
        "\n",
        "### ⚙️ Hiperparámetros a Optimizar:\n",
        "\n",
        "1. **Arquitectura de la Red** (hidden_layer_sizes):\n",
        "   - Diferentes combinaciones de neuronas por capa\n",
        "   - Impacta en la capacidad de aprendizaje del modelo\n",
        "   - Más neuronas = mayor capacidad pero riesgo de overfitting\n",
        "\n",
        "2. **Funciones de Activación** (activation):\n",
        "   - ReLU: Mejor para capas profundas, evita el problema del desvanecimiento del gradiente\n",
        "   - Tanh: Útil para normalizar las salidas entre -1 y 1\n",
        "\n",
        "3. **Tasa de Aprendizaje** (learning_rate_init):\n",
        "   - Controla el tamaño de los pasos en el descenso del gradiente\n",
        "   - Valores pequeños = aprendizaje más estable pero más lento\n",
        "   - Valores grandes = aprendizaje más rápido pero riesgo de no converger\n",
        "\n",
        "4. **Tamaño del Batch** (batch_size):\n",
        "   - Afecta la velocidad y estabilidad del entrenamiento\n",
        "   - Batches pequeños = actualizaciones más frecuentes, más ruido\n",
        "   - Batches grandes = actualizaciones más estables, mayor uso de memoria\n",
        "\n",
        "### 📈 Métricas y Visualizaciones:\n",
        "Se implementan diversas formas de evaluar el modelo:\n",
        "- Matriz de confusión\n",
        "- Curva ROC y AUC\n",
        "- Curva de pérdida durante el entrenamiento\n",
        "- Comparación de arquitecturas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSOHU5W_hYYK"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# 1. CONFIGURACIÓN DE LA RED\n",
        "# -------------------------\n",
        "# Definimos diferentes configuraciones para experimentar\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(2, 2), (4, 4), (8, 8)],  # Diferentes arquitecturas\n",
        "    'activation': ['relu', 'tanh'],                   # Funciones de activación\n",
        "    'learning_rate_init': [0.001, 0.01, 0.1],        # Tasas de aprendizaje\n",
        "    'max_iter': [1000],                              # Número de épocas\n",
        "    'batch_size': [32, 64, 128]                      # Tamaño del batch\n",
        "}\n",
        "\n",
        "# 2. PREPARACIÓN DE DATOS\n",
        "# ----------------------\n",
        "X = df[['age', 'balance', 'duration']]\n",
        "y = df['y']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 3. CREACIÓN Y ENTRENAMIENTO DEL MODELO\n",
        "# -------------------------------------\n",
        "# Creamos el modelo base\n",
        "mlp = MLPClassifier(random_state=42)\n",
        "\n",
        "# Implementamos Grid Search con validación cruzada\n",
        "grid_search = GridSearchCV(\n",
        "    mlp, param_grid,\n",
        "    cv=5,                  # 5-fold cross validation\n",
        "    scoring='roc_auc',     # Métrica para optimizar\n",
        "    n_jobs=-1,            # Usar todos los cores disponibles\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Entrenamos el modelo probando todas las configuraciones\n",
        "print(\"🔄 Iniciando entrenamiento con Grid Search...\")\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 4. EVALUACIÓN DEL MEJOR MODELO\n",
        "# ----------------------------\n",
        "print(\"\\n🏆 Mejores hiperparámetros encontrados:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Obtenemos predicciones con el mejor modelo\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "y_pred_prob = best_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Calculamos métricas\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# 5. VISUALIZACIONES\n",
        "# ----------------\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# 5.1 Matriz de Confusión\n",
        "plt.subplot(2, 2, 1)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Matriz de Confusión')\n",
        "plt.xlabel('Predicho')\n",
        "plt.ylabel('Real')\n",
        "\n",
        "# 5.2 Curva ROC\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2,\n",
        "         label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Tasa de Falsos Positivos')\n",
        "plt.ylabel('Tasa de Verdaderos Positivos')\n",
        "plt.title('Curva ROC')\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "# 5.3 Curva de Pérdida\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(best_model.loss_curve_)\n",
        "plt.title('Curva de Pérdida Durante el Entrenamiento')\n",
        "plt.xlabel('Épocas')\n",
        "plt.ylabel('Pérdida')\n",
        "\n",
        "# 5.4 Comparación de Arquitecturas\n",
        "plt.subplot(2, 2, 4)\n",
        "results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "architectures = results_df.groupby('param_hidden_layer_sizes')['mean_test_score'].mean()\n",
        "architectures.plot(kind='bar')\n",
        "plt.title('Desempeño por Arquitectura')\n",
        "plt.xlabel('Arquitectura')\n",
        "plt.ylabel('ROC AUC Promedio')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 6. MÉTRICAS DE DESEMPEÑO\n",
        "# -----------------------\n",
        "# Extraemos valores de la matriz de confusión\n",
        "vp = cm[0,0]  # Verdaderos Positivos\n",
        "fn = cm[0,1]  # Falsos Negativos\n",
        "fp = cm[1,0]  # Falsos Positivos\n",
        "vn = cm[1,1]  # Verdaderos Negativos\n",
        "\n",
        "# Calculamos métricas\n",
        "accuracy = (vp + vn) / (vp + vn + fp + fn)\n",
        "specificity = vn / (vn + fp)\n",
        "precision = vp / (vp + fp)\n",
        "recall = vp / (vp + fn)\n",
        "f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "# Creamos DataFrame de métricas\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Métrica': ['Accuracy', 'Recall', 'Especificidad', 'Precisión', 'F1-Score', 'ROC-AUC'],\n",
        "    'Valor': [\n",
        "        f\"{accuracy:.2%}\",\n",
        "        f\"{recall:.2%}\",\n",
        "        f\"{specificity:.2%}\",\n",
        "        f\"{precision:.2%}\",\n",
        "        f\"{f1_score:.2%}\",\n",
        "        f\"{roc_auc:.2%}\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\n📊 Métricas de Desempeño del Mejor Modelo:\")\n",
        "print(metrics_df.to_string(index=False))\n",
        "\n",
        "# 7. GUARDADO DE RESULTADOS\n",
        "# ------------------------\n",
        "# Guardamos los resultados del Grid Search para análisis posterior\n",
        "results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "results_df.to_csv('grid_search_results.csv', index=False)\n",
        "print(\"\\n💾 Resultados del Grid Search guardados en 'grid_search_results.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0bbikPyk0CX"
      },
      "source": [
        "## 📊 Análisis de Resultados del Modelo MLP\n",
        "\n",
        "🎯 **Rendimiento y Matriz de Confusión**: Los resultados son excelentes, con una precisión global del 95.94%. En la matriz de confusión podemos ver que el modelo identificó correctamente 1,876 casos de no suscripción (verdaderos positivos, y=0) y 42 casos de suscripción (verdaderos negativos, y=1). Solo hubo 54 falsos negativos (casos predichos como suscripción cuando no suscribieron) y 28 falsos positivos (casos predichos como no suscripción cuando sí suscribieron).\n",
        "\n",
        "📈 **Métricas de Desempeño**: El modelo muestra un recall (sensibilidad) del 97.20%, lo que significa que identifica correctamente el 97.20% de los casos de no suscripción. La especificidad del 60% indica que identifica correctamente el 60% de los casos de suscripción. La precisión del 98.53% muestra que cuando el modelo predice que alguien no suscribirá, acierta el 98.53% de las veces.\n",
        "\n",
        "🌟 **Conclusión Final**: La curva ROC con un AUC de 0.96 confirma el excelente poder discriminativo del modelo. La arquitectura simple (2,2) con activación tanh y batch size de 128 resultó ser la configuración óptima, demostrando que para este problema, una red más simple es más efectiva que arquitecturas más complejas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slMw1Ix1unm9"
      },
      "outputs": [],
      "source": [
        "# Visualización de hiperparámetros y pesos optimizados\n",
        "print(\"\\n🏆 MEJORES HIPERPARÁMETROS ENCONTRADOS:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Arquitectura: {best_model.hidden_layer_sizes}\")\n",
        "print(f\"Función de activación: {best_model.activation}\")\n",
        "print(f\"Tasa de aprendizaje: {best_model.learning_rate_init}\")\n",
        "print(f\"Tamaño del batch: {best_model.batch_size}\")\n",
        "\n",
        "print(\"\\n🧮 PESOS OPTIMIZADOS POR CAPA:\")\n",
        "print(\"-\" * 50)\n",
        "for i, (pesos, sesgos) in enumerate(zip(best_model.coefs_, best_model.intercepts_)):\n",
        "    print(f\"\\nCapa {i+1}:\")\n",
        "    print(f\"Matriz de pesos:\\n{pesos}\")\n",
        "    print(f\"Vector de sesgos:\\n{sesgos}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "449CSXGrxGRl"
      },
      "source": [
        "## 📊 Análisis de la Red Neuronal Optimizada\n",
        "\n",
        "🧠 La red neuronal encontró su **configuración óptima** con una arquitectura simple pero efectiva de (2, 2) neuronas en sus capas ocultas, utilizando la función de activación **tanh** y una tasa de aprendizaje relativamente alta de 0.1.\n",
        "\n",
        "Los **pesos optimizados** muestran un patrón interesante: en la primera capa, los valores oscilan aproximadamente entre -7 y 0.8, sugiriendo que el modelo aprendió a dar diferentes niveles de importancia a las características de entrada. La segunda capa muestra pesos más uniformes alrededor de 0.9 a 3.7, indicando una consolidación de los patrones aprendidos, mientras que la capa final refina estos patrones para la predicción final.\n",
        "\n",
        "Esta estructura de pesos, junto con un tamaño de batch de 128, sugiere que el modelo encontró un buen **balance entre generalización y precisión.** 🎯"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNCU5WTbydsu"
      },
      "source": [
        "# 🧠 Deep Learning: Optimización y Técnicas de Regularización\n",
        "\n",
        "### 📝 ¿Qué vamos a hacer?\n",
        "Construiremos una red neuronal con múltiples capas para nuestro problema de marketing bancario, implementando técnicas modernas de optimización y regularización. El código está diseñado para permitir la experimentación con diferentes configuraciones de hiperparámetros, facilitando el aprendizaje sobre su impacto en el rendimiento del modelo.\n",
        "\n",
        "### ❌ Desafíos en el Entrenamiento\n",
        "\n",
        "1. **Vanishing Gradient (Gradiente Desvaneciente)**\n",
        "   - Los gradientes se vuelven muy pequeños al propagarse hacia atrás\n",
        "   - Las primeras capas aprenden muy lentamente\n",
        "   - Para combatirlo usamos:\n",
        "     * ReLU como función de activación\n",
        "     * Batch Normalization en cada capa\n",
        "     * Inicialización apropiada de pesos\n",
        "\n",
        "2. **Overfitting (Sobreajuste)**\n",
        "   - El modelo memoriza en lugar de generalizar\n",
        "   - Señales de overfitting:\n",
        "     * Buena precisión en entrenamiento pero mala en validación\n",
        "     * La pérdida de validación aumenta mientras la de entrenamiento baja\n",
        "   - Lo controlamos con:\n",
        "     * Dropout configurable\n",
        "     * Regularización L2 ajustable\n",
        "     * Early Stopping con paciencia personalizable\n",
        "\n",
        "3. **Selección de Hiperparámetros**\n",
        "   - Múltiples hiperparámetros para experimentar:\n",
        "     * Número de capas\n",
        "     * Neuronas por capa\n",
        "     * Tasa de aprendizaje\n",
        "     * Tamaño de batch\n",
        "     * Todos fácilmente ajustables en el código\n",
        "\n",
        "### ✅ Soluciones Implementadas\n",
        "\n",
        "1. **Batch Normalization**\n",
        "   - Aplicada después de cada capa densa\n",
        "   - Normaliza las activaciones\n",
        "   - Acelera el entrenamiento\n",
        "   - Reduce dependencia de la inicialización\n",
        "\n",
        "2. **Regularización L2**\n",
        "   - Implementada en cada capa densa\n",
        "   - Valores configurables para experimentación\n",
        "   - Penaliza pesos grandes\n",
        "   - Promueve soluciones más simples\n",
        "\n",
        "3. **Dropout**\n",
        "   - Aplicado después de cada capa\n",
        "   - Porcentaje de dropout ajustable\n",
        "   - Previene la co-adaptación\n",
        "   - Mejora la generalización\n",
        "\n",
        "4. **Optimización Automática**\n",
        "   - Búsqueda aleatoria de hiperparámetros\n",
        "   - Número de pruebas configurable\n",
        "   - Validación cruzada con split ajustable\n",
        "   - Early stopping personalizable\n",
        "\n",
        "5. **Arquitectura Flexible**\n",
        "   - ReLU como activación en capas ocultas\n",
        "   - Sigmoid en la capa de salida\n",
        "   - Batch Normalization + Dropout en cada capa\n",
        "   - Adam como optimizador\n",
        "   - Estructura personalizable para experimentación\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dn_LwcM15X7w"
      },
      "outputs": [],
      "source": [
        "# Se debe reiniciar el entorno de ejecución para correr este bloque de código.\n",
        "\n",
        "# Instalamos las dependencias\n",
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install keras-tuner\n",
        "!pip install datasets\n",
        "!pip install pandas numpy scikit-learn matplotlib seaborn\n",
        "\n",
        "# ======================\n",
        "# 1. IMPORTACIÓN DE LIBRERÍAS\n",
        "# ======================\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import keras_tuner as kt  # Para búsqueda de hiperparámetros\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Fijamos una semilla para reproducibilidad\n",
        "RANDOM_SEED = 42\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# ======================\n",
        "# 2. CONFIGURACIÓN DE HIPERPARÁMETROS\n",
        "# ======================\n",
        "# Definimos el rango de valores que puede tomar cada hiperparámetro\n",
        "HP_RANGES = {\n",
        "    # Número de capas ocultas (entre 3 y 4)\n",
        "    'n_layers': {\n",
        "        'min_value': 3,\n",
        "        'max_value': 4,\n",
        "        'step': 1\n",
        "    },\n",
        "\n",
        "    # Número de neuronas por capa (32 o 64)\n",
        "    'units': {\n",
        "        'min_value': 32,\n",
        "        'max_value': 64,\n",
        "        'step': 32\n",
        "    },\n",
        "\n",
        "    # Tasa de dropout para regularización (entre 0.2 y 0.4)\n",
        "    'dropout': {\n",
        "        'min_value': 0.2,\n",
        "        'max_value': 0.4,\n",
        "        'step': 0.1\n",
        "    },\n",
        "\n",
        "    # Tasa de aprendizaje del optimizador\n",
        "    'learning_rate': [0.001, 0.01],  # Probamos dos valores\n",
        "\n",
        "    # Factor de regularización L2\n",
        "    'l2_lambda': [0.0001, 0.001]  # Probamos dos valores\n",
        "}\n",
        "\n",
        "# Configuración del entrenamiento\n",
        "TRAINING_CONFIG = {\n",
        "    'epochs': 15,          # Número de pasadas por todo el dataset\n",
        "    'batch_size': 64,      # Tamaño de los mini-lotes\n",
        "    'max_trials': 5        # Número de combinaciones de hiperparámetros a probar\n",
        "}\n",
        "\n",
        "# ======================\n",
        "# 3. DEFINICIÓN DEL MODELO\n",
        "# ======================\n",
        "def create_model(hp):\n",
        "    \"\"\"\n",
        "    Función que crea el modelo con los hiperparámetros que se están probando.\n",
        "    hp: objeto que permite acceder a los hiperparámetros que se están evaluando\n",
        "    \"\"\"\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # Decidimos cuántas capas tendrá el modelo\n",
        "    n_layers = hp.Int('n_layers', **HP_RANGES['n_layers'])\n",
        "\n",
        "    # Primera capa (capa de entrada)\n",
        "    model.add(keras.layers.Dense(\n",
        "        units=hp.Int('units_0', **HP_RANGES['units']),  # Número de neuronas\n",
        "        input_shape=(3,),  # 3 características de entrada\n",
        "        kernel_regularizer=keras.regularizers.l2(  # Regularización L2\n",
        "            hp.Choice('l2_0', HP_RANGES['l2_lambda'])\n",
        "        )\n",
        "    ))\n",
        "    # Añadimos capas auxiliares para mejorar el entrenamiento\n",
        "    model.add(keras.layers.BatchNormalization())  # Normaliza las activaciones\n",
        "    model.add(keras.layers.ReLU())  # Función de activación\n",
        "    model.add(keras.layers.Dropout(  # Previene el sobreajuste\n",
        "        hp.Float('dropout_0', **HP_RANGES['dropout'])\n",
        "    ))\n",
        "\n",
        "    # Agregamos las capas ocultas adicionales\n",
        "    for i in range(1, n_layers):\n",
        "        model.add(keras.layers.Dense(\n",
        "            units=hp.Int(f'units_{i}', **HP_RANGES['units']),\n",
        "            kernel_regularizer=keras.regularizers.l2(\n",
        "                hp.Choice(f'l2_{i}', HP_RANGES['l2_lambda'])\n",
        "            )\n",
        "        ))\n",
        "        model.add(keras.layers.BatchNormalization())\n",
        "        model.add(keras.layers.ReLU())\n",
        "        model.add(keras.layers.Dropout(\n",
        "            hp.Float(f'dropout_{i}', **HP_RANGES['dropout'])\n",
        "        ))\n",
        "\n",
        "    # Capa de salida (una neurona con sigmoid para clasificación binaria)\n",
        "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Compilamos el modelo\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(\n",
        "            hp.Choice('learning_rate', HP_RANGES['learning_rate'])\n",
        "        ),\n",
        "        loss='binary_crossentropy',  # Función de pérdida para clasificación binaria\n",
        "        metrics=['accuracy']  # Métrica que queremos monitorear\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# ======================\n",
        "# 4. PREPARACIÓN DE DATOS\n",
        "# ======================\n",
        "# Cargamos y preparamos los datos\n",
        "dataset = load_dataset(\"cestwc/bank-marketing\", streaming=True)\n",
        "df = pd.DataFrame(list(dataset['train'].shuffle(seed=42).take(10000)))\n",
        "\n",
        "# Seleccionamos solo 3 características\n",
        "X = df[['age', 'balance', 'duration']]\n",
        "y = df['y'].astype(int)\n",
        "\n",
        "# Dividimos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Escalamos los datos\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ======================\n",
        "# 5. BÚSQUEDA DE MEJORES HIPERPARÁMETROS\n",
        "# ======================\n",
        "# Iniciamos la búsqueda aleatoria de hiperparámetros\n",
        "tuner = kt.RandomSearch(\n",
        "    create_model,\n",
        "    objective='val_accuracy',  # Optimizamos basándonos en la precisión\n",
        "    max_trials=TRAINING_CONFIG['max_trials'],\n",
        "    directory='keras_tuner',\n",
        "    project_name='bank_marketing',\n",
        "    overwrite=True\n",
        ")\n",
        "\n",
        "# Configuramos early stopping para evitar sobreajuste\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=2,  # Esperamos 2 épocas antes de parar\n",
        "    restore_best_weights=True  # Guardamos los mejores pesos\n",
        ")\n",
        "\n",
        "# Realizamos la búsqueda\n",
        "tuner.search(\n",
        "    X_train_scaled,\n",
        "    y_train,\n",
        "    epochs=TRAINING_CONFIG['epochs'],\n",
        "    batch_size=TRAINING_CONFIG['batch_size'],\n",
        "    validation_split=0.2,  # 20% de los datos para validación\n",
        "    callbacks=[stop_early],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ======================\n",
        "# 6. ENTRENAMIENTO DEL MEJOR MODELO\n",
        "# ======================\n",
        "# Obtenemos los mejores hiperparámetros encontrados\n",
        "best_hps = tuner.get_best_hyperparameters()[0]\n",
        "print(\"\\n🏆 Mejores hiperparámetros encontrados:\")\n",
        "print(\"Número de capas:\", best_hps.get('n_layers'))\n",
        "print(\"Tasa de aprendizaje:\", best_hps.get('learning_rate'))\n",
        "\n",
        "# Entrenamos el modelo final con los mejores hiperparámetros\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "history = best_model.fit(\n",
        "    X_train_scaled,\n",
        "    y_train,\n",
        "    epochs=TRAINING_CONFIG['epochs'],\n",
        "    batch_size=TRAINING_CONFIG['batch_size'],\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluamos el modelo final\n",
        "test_loss, test_acc = best_model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "print(f\"\\n📊 Precisión final en test: {test_acc:.2%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "demk2kkv80bH"
      },
      "outputs": [],
      "source": [
        "# ======================\n",
        "# 7. EVALUACIÓN COMPLETA (CON Y=0 COMO CLASE POSITIVA)\n",
        "# ======================\n",
        "from sklearn.metrics import classification_report, roc_curve, auc, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Predicciones\n",
        "y_probs = best_model.predict(X_test_scaled)\n",
        "y_pred = (y_probs > 0.5).astype(int)\n",
        "\n",
        "# Calculamos la matriz de confusión primero - esto es necesario para los cálculos siguientes\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# 1. Visualización de la función de pérdida\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Pérdida de entrenamiento')\n",
        "plt.plot(history.history['val_loss'], label='Pérdida de validación')\n",
        "plt.title('Función de Pérdida Durante el Entrenamiento')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Pérdida')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Precisión de entrenamiento')\n",
        "plt.plot(history.history['val_accuracy'], label='Precisión de validación')\n",
        "plt.title('Precisión Durante el Entrenamiento')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Precisión')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Matriz de Confusión\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['No (0)', 'Sí (1)'], yticklabels=['No (0)', 'Sí (1)'])\n",
        "plt.title('Matriz de Confusión')\n",
        "plt.xlabel('Predicho')\n",
        "plt.ylabel('Real')\n",
        "plt.show()\n",
        "\n",
        "# 3. Extraer valores de la matriz de confusión\n",
        "# Para Y=0 como clase positiva:\n",
        "# [[TP, FN],\n",
        "#  [FP, TN]]\n",
        "TP = cm[0,0]  # Verdaderos Positivos (predicho correctamente como 0)\n",
        "FN = cm[0,1]  # Falsos Negativos (predicho como 1 cuando era 0)\n",
        "FP = cm[1,0]  # Falsos Positivos (predicho como 0 cuando era 1)\n",
        "TN = cm[1,1]  # Verdaderos Negativos (predicho correctamente como 1)\n",
        "\n",
        "# 4. Calcular métricas manualmente\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "precision = TP / (TP + FP) if (TP + FP) > 0 else 0  # Evitar división por cero\n",
        "recall = TP / (TP + FN) if (TP + FN) > 0 else 0  # Evitar división por cero\n",
        "specificity = TN / (TN + FP) if (TN + FP) > 0 else 0  # Evitar división por cero\n",
        "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0  # Evitar división por cero\n",
        "\n",
        "# 5. Curva ROC y AUC\n",
        "# Invertir las probabilidades para Y=0 como clase positiva\n",
        "y_probs_inverted = 1 - y_probs\n",
        "\n",
        "# Calcular la curva ROC con Y=0 como clase positiva\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_probs_inverted, pos_label=0)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Graficar la curva ROC\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlabel('Tasa de Falsos Positivos (FPR)')\n",
        "plt.ylabel('Tasa de Verdaderos Positivos (TPR)')\n",
        "plt.title('Curva ROC (Y=0 como Clase Positiva)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "# 6. Reporte de Clasificación\n",
        "print(\"\\n📝 Reporte de Clasificación (Y=0 como Clase Positiva):\")\n",
        "print(classification_report(y_test, y_pred, target_names=['No (0)', 'Sí (1)'], zero_division=0))\n",
        "\n",
        "# 7. Resumen de Métricas\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Métrica': ['Pérdida final', 'Pérdida de validación final', 'Precisión', 'Recall', 'F1-Score',\n",
        "                'AUC', 'Exactitud (Accuracy)', 'Especificidad'],\n",
        "    'Valor': [\n",
        "        f\"{history.history['loss'][-1]:.3f}\",\n",
        "        f\"{history.history['val_loss'][-1]:.3f}\",\n",
        "        f\"{precision:.3f}\",\n",
        "        f\"{recall:.3f}\",\n",
        "        f\"{f1:.3f}\",\n",
        "        f\"{roc_auc:.3f}\",\n",
        "        f\"{accuracy:.3f}\",\n",
        "        f\"{specificity:.3f}\"\n",
        "    ],\n",
        "    'Fórmula': [\n",
        "        'Binary Crossentropy',\n",
        "        'Binary Crossentropy en validación',\n",
        "        f'TP/(TP+FP) = {TP}/{TP+FP}',\n",
        "        f'TP/(TP+FN) = {TP}/{TP+FN}',\n",
        "        'Media armónica de Precisión y Recall',\n",
        "        'Área bajo la curva ROC',\n",
        "        f'(TP+TN)/(TP+TN+FP+FN) = {TP+TN}/{TP+TN+FP+FN}',\n",
        "        f'TN/(TN+FP) = {TN}/{TN+FP}'\n",
        "    ],\n",
        "    'Descripción': [\n",
        "        'Pérdida en conjunto de entrenamiento',\n",
        "        'Pérdida en conjunto de validación',\n",
        "        'Porcentaje de predicciones positivas correctas',\n",
        "        'Porcentaje de casos positivos identificados',\n",
        "        'Balance entre precisión y recall',\n",
        "        'Capacidad discriminativa del modelo',\n",
        "        'Porcentaje total de predicciones correctas',\n",
        "        'Porcentaje de casos negativos identificados correctamente'\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\n📊 Resumen de Métricas (Y=0 como Clase Positiva):\")\n",
        "# Intentamos usar to_markdown si está disponible, si no usamos to_string\n",
        "try:\n",
        "    print(metrics_df.to_markdown(index=False))\n",
        "except AttributeError:\n",
        "    print(metrics_df.to_string(index=False))\n",
        "\n",
        "# 8. Imprimir valores de la matriz de confusión para verificación\n",
        "print(\"\\n🔍 Valores de la Matriz de Confusión:\")\n",
        "print(f\"Verdaderos Positivos (TP): {TP}\")\n",
        "print(f\"Falsos Negativos (FN): {FN}\")\n",
        "print(f\"Falsos Positivos (FP): {FP}\")\n",
        "print(f\"Verdaderos Negativos (TN): {TN}\")\n",
        "\n",
        "# 9. Imprimir valores finales de pérdida\n",
        "print(\"\\n📉 Valores Finales de Pérdida:\")\n",
        "print(f\"Pérdida de entrenamiento final: {history.history['loss'][-1]:.4f}\")\n",
        "print(f\"Pérdida de validación final: {history.history['val_loss'][-1]:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOGKEcWVtF4Duy46lUb/Vnh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}