{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNgk+lrLxRXtJQRjXCd1/H6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CamiloVga/Curso-IA-Aplicada/blob/main/Script_Clase_28_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üé® Inteligencia Artificial Aplicada\n",
        "## Universidad de los Andes\n",
        "\n",
        "### üë®‚Äçüè´ Profesores\n",
        "- **Profesor Magistral:** [Camilo Vega Barbosa](https://www.linkedin.com/in/camilovegabarbosa/)\n",
        "- **Asistente de Docencia:** [Sergio Julian Zona Moreno](https://www.linkedin.com/in/sergiozonamoreno/)\n",
        "\n",
        "### üìö Implementaci√≥n de RAG (Retrieval Augmented Generation) con Ollama\n",
        "\n",
        "Este script implementa un sistema de Generaci√≥n Aumentada por Recuperaci√≥n para consultas sobre documentos usando Ollama para inferencia r√°pida:\n",
        "\n",
        "1. **Arquitectura del Sistema RAG con Ollama üöÄ**\n",
        "   - Integraci√≥n de Ollama para inferencia r√°pida y eficiente en CPU\n",
        "   - Soporte para modelos como Llama-2-7b con menor huella de memoria\n",
        "   - Procesamiento de documentos en m√∫ltiples formatos (PDF, DOCX, CSV, TXT)\n",
        "   - Sistema de embeddings multilingual-e5-small para representaci√≥n vectorial √≥ptima\n",
        "   - Base de datos vectorial FAISS para b√∫squeda sem√°ntica de alta velocidad\n",
        "   - Dos m√©todos de inferencia: directo (v√≠a API HTTP) y est√°ndar (v√≠a LangChain)\n",
        "\n",
        "2. **Procesamiento de Documentos Optimizado üìÑ**\n",
        "   - Carga inteligente de m√∫ltiples formatos con metadata enriquecida\n",
        "   - Divisi√≥n recursiva de documentos en chunks sem√°nticos de tama√±o √≥ptimo\n",
        "   - Vectorizaci√≥n mediante embeddings eficientes para entornos con recursos limitados\n",
        "   - Manejo avanzado de errores con sistema de reintentos y verificaci√≥n de servicios\n",
        "   - Sistema de seguimiento para evitar reprocesamiento de archivos\n",
        "\n",
        "3. **Pipeline de Consulta y Respuesta Robusta üîç**\n",
        "   - Recuperaci√≥n contextual con k=6 fragmentos m√°s relevantes\n",
        "   - Sistema dual de generaci√≥n: m√©todo r√°pido (requests directos) y m√©todo robusto (LangChain)\n",
        "   - Fallback autom√°tico al m√©todo est√°ndar en caso de errores con llamadas directas\n",
        "   - Citaci√≥n de fuentes con metadatos completos para transparencia\n",
        "   - Interfaz intuitiva con opciones configurables seg√∫n necesidades de rendimiento\n",
        "   - Gesti√≥n de errores avanzada con mensajes informativos y soluciones alternativas"
      ],
      "metadata": {
        "id": "Nuh7hCL1RE5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================================\n",
        "# SECCI√ìN 1: INSTALACI√ìN DE DEPENDENCIAS\n",
        "# ============================================================================\n",
        "# Estas l√≠neas instalan las bibliotecas necesarias para el funcionamiento del sistema\n",
        "# Se ejecutan solamente en entornos como Google Colab o Jupyter Notebooks\n",
        "\n",
        "# Instalaci√≥n de bibliotecas esenciales para RAG\n",
        "!pip install -q langchain langchain_community sentence-transformers pypdf python-docx docx2txt unstructured faiss-cpu gradio\n",
        "!pip install -q chromadb requests\n",
        "\n",
        "# ============================================================================\n",
        "# SECCI√ìN 2: CONFIGURACI√ìN DE OLLAMA\n",
        "# ============================================================================\n",
        "# Ollama es una herramienta que permite ejecutar modelos de lenguaje localmente\n",
        "# con menor uso de recursos que otros sistemas\n",
        "\n",
        "# Instalaci√≥n de Ollama\n",
        "print(\"Instalando Ollama para inferencia r√°pida...\")\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# Iniciar el servicio de Ollama en segundo plano\n",
        "print(\"\\nIniciando servidor Ollama...\")\n",
        "!pkill ollama || true  # Detener cualquier instancia anterior\n",
        "!nohup /usr/local/bin/ollama serve > ollama_output.log 2>&1 &  # Iniciar en segundo plano\n",
        "\n",
        "# Dar tiempo al servidor para inicializar\n",
        "import time\n",
        "print(\"Esperando a que el servidor Ollama est√© listo...\")\n",
        "time.sleep(15)  # Esperar 15 segundos\n",
        "\n",
        "# Verificar que el servidor est√© activo\n",
        "print(\"\\nVerificando que el servidor Ollama est√© respondiendo...\")\n",
        "!curl -s http://localhost:11434/api/tags || echo \"El servidor Ollama no est√° respondiendo\"\n",
        "\n",
        "# Descargar el modelo LLM que usaremos (Llama2)\n",
        "print(\"\\nDescargando modelo llama2 desde Ollama...\")\n",
        "!ollama pull llama2\n",
        "\n",
        "# ============================================================================\n",
        "# SECCI√ìN 3: IMPORTACI√ìN DE BIBLIOTECAS\n",
        "# ============================================================================\n",
        "# Estas bibliotecas proporcionan las funcionalidades esenciales para el sistema RAG\n",
        "\n",
        "import os\n",
        "import logging\n",
        "import tempfile\n",
        "import subprocess\n",
        "import json\n",
        "import requests  # Para llamadas HTTP directas a Ollama\n",
        "from typing import List, Dict\n",
        "import torch\n",
        "import gradio as gr\n",
        "\n",
        "# Componentes de LangChain para RAG\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Para dividir documentos\n",
        "from langchain.embeddings import HuggingFaceEmbeddings  # Para crear embeddings\n",
        "from langchain.vectorstores import FAISS  # Base de datos vectorial\n",
        "from langchain.chains import RetrievalQA  # Framework para consultas RAG\n",
        "from langchain.prompts import PromptTemplate  # Para definir prompts\n",
        "from langchain_community.document_loaders import (  # Cargadores de documentos\n",
        "    PyPDFLoader,  # Para PDF\n",
        "    Docx2txtLoader,  # Para DOCX\n",
        "    CSVLoader,  # Para CSV\n",
        "    UnstructuredFileLoader  # Para texto plano y otros formatos\n",
        ")\n",
        "from langchain_community.llms import Ollama  # Integraci√≥n LangChain-Ollama\n",
        "\n",
        "# ============================================================================\n",
        "# SECCI√ìN 4: CONFIGURACI√ìN B√ÅSICA\n",
        "# ============================================================================\n",
        "\n",
        "# Configuraci√≥n de logs para monitoreo y depuraci√≥n\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Constantes para configuraci√≥n del sistema\n",
        "SUPPORTED_FORMATS = [\".pdf\", \".docx\", \".doc\", \".csv\", \".txt\"]  # Formatos soportados\n",
        "EMBEDDING_MODEL = \"intfloat/multilingual-e5-small\"  # Modelo para codificaci√≥n sem√°ntica\n",
        "OLLAMA_MODEL = \"llama2\"  # Modelo LLM local\n",
        "\n",
        "# ============================================================================\n",
        "# SECCI√ìN 5: CLASE PARA CARGA DE DOCUMENTOS\n",
        "# ============================================================================\n",
        "\n",
        "class DocumentLoader:\n",
        "    \"\"\"\n",
        "    Cargador unificado de documentos que soporta m√∫ltiples formatos.\n",
        "    Esta clase selecciona el cargador adecuado seg√∫n la extensi√≥n del archivo.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def load_file(file_path: str) -> List:\n",
        "        \"\"\"\n",
        "        Carga un archivo basado en su extensi√≥n y devuelve los documentos procesados.\n",
        "\n",
        "        Args:\n",
        "            file_path: Ruta al archivo a cargar\n",
        "\n",
        "        Returns:\n",
        "            Lista de documentos procesados con sus metadatos\n",
        "        \"\"\"\n",
        "        print(f\"Cargando archivo: {file_path}\")\n",
        "        ext = os.path.splitext(file_path)[1].lower()  # Obtener extensi√≥n del archivo\n",
        "\n",
        "        try:\n",
        "            # Seleccionar el cargador apropiado seg√∫n el tipo de archivo\n",
        "            if ext == '.pdf':\n",
        "                loader = PyPDFLoader(file_path)  # Para archivos PDF\n",
        "            elif ext in ['.docx', '.doc']:\n",
        "                loader = Docx2txtLoader(file_path)  # Para documentos Word\n",
        "            elif ext == '.csv':\n",
        "                loader = CSVLoader(file_path)  # Para archivos CSV\n",
        "            else:  # Para txt y otros formatos de texto\n",
        "                loader = UnstructuredFileLoader(file_path)\n",
        "\n",
        "            # Ejecutar la carga del documento\n",
        "            documents = loader.load()\n",
        "\n",
        "            # Enriquecer con metadatos para mejorar la recuperaci√≥n y visualizaci√≥n\n",
        "            for doc in documents:\n",
        "                doc.metadata.update({\n",
        "                    'title': os.path.basename(file_path),  # Nombre del archivo\n",
        "                    'type': 'document',  # Tipo de contenido\n",
        "                    'format': ext[1:],  # Formato sin el punto inicial\n",
        "                    'language': 'auto'  # Idioma (auto-detectado)\n",
        "                })\n",
        "\n",
        "            print(f\"‚úÖ Archivo cargado exitosamente: {file_path}\")\n",
        "            return documents\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error al cargar {file_path}: {str(e)}\")\n",
        "            raise  # Re-lanzar la excepci√≥n para manejo superior\n",
        "\n",
        "# ============================================================================\n",
        "# SECCI√ìN 6: CLASE PRINCIPAL DEL SISTEMA RAG\n",
        "# ============================================================================\n",
        "\n",
        "class RAGSystem:\n",
        "    \"\"\"\n",
        "    Sistema RAG completo con Ollama para consulta de documentos.\n",
        "\n",
        "    Esta clase implementa todo el flujo de trabajo RAG:\n",
        "    1. Carga y procesamiento de documentos\n",
        "    2. Generaci√≥n de embeddings y almacenamiento vectorial\n",
        "    3. Recuperaci√≥n de contexto relevante\n",
        "    4. Generaci√≥n de respuestas mediante LLM\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_model: str = EMBEDDING_MODEL, ollama_model: str = OLLAMA_MODEL):\n",
        "        \"\"\"\n",
        "        Inicializa el sistema RAG con los modelos especificados.\n",
        "\n",
        "        Args:\n",
        "            embedding_model: Modelo para generar embeddings (representaciones vectoriales)\n",
        "            ollama_model: Modelo de lenguaje a utilizar con Ollama\n",
        "        \"\"\"\n",
        "        self.embedding_model = embedding_model\n",
        "        self.ollama_model = ollama_model\n",
        "        self.embeddings = None  # Se inicializar√° posteriormente\n",
        "        self.vector_store = None  # Base de datos vectorial\n",
        "        self.qa_chain = None  # Cadena de pregunta-respuesta\n",
        "        self.is_initialized = False  # Flag de inicializaci√≥n\n",
        "        self.processed_files = set()  # Conjunto para evitar procesar archivos duplicados\n",
        "\n",
        "    def initialize_system(self):\n",
        "        \"\"\"\n",
        "        Inicializa los componentes del sistema RAG:\n",
        "        - Modelo de embeddings\n",
        "        - Conexi√≥n con Ollama\n",
        "        \"\"\"\n",
        "        try:\n",
        "            print(\"üöÄ Inicializando sistema RAG con Ollama...\")\n",
        "\n",
        "            # Inicializar el modelo de embeddings (usando CPU o GPU si est√° disponible)\n",
        "            print(\"üìä Cargando modelo de embeddings...\")\n",
        "            self.embeddings = HuggingFaceEmbeddings(\n",
        "                model_name=self.embedding_model,\n",
        "                model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},\n",
        "                encode_kwargs={'normalize_embeddings': True}  # Normalizaci√≥n para mejor b√∫squeda\n",
        "            )\n",
        "\n",
        "            # Verificaci√≥n de salud de Ollama - reintento si no responde\n",
        "            try:\n",
        "                response = requests.get(\"http://localhost:11434/api/tags\")\n",
        "                if response.status_code != 200:\n",
        "                    print(\"‚ö†Ô∏è Advertencia: Ollama no est√° respondiendo correctamente. Reintentando inicializaci√≥n...\")\n",
        "                    time.sleep(5)\n",
        "                    # Reinicio de emergencia del servicio Ollama\n",
        "                    subprocess.run(\"pkill ollama || true\", shell=True)\n",
        "                    subprocess.run(\"nohup /usr/local/bin/ollama serve > ollama_output.log 2>&1 &\", shell=True)\n",
        "                    time.sleep(15)  # Esperar a que reinicie\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Advertencia al verificar Ollama: {str(e)}\")\n",
        "\n",
        "            # Configurar Ollama como modelo de lenguaje mediante LangChain\n",
        "            print(\"üß† Configurando Ollama como LLM...\")\n",
        "            self.llm = Ollama(\n",
        "                model=self.ollama_model,\n",
        "                temperature=0.1,  # Temperatura baja para respuestas m√°s deterministas\n",
        "                num_predict=512  # M√°ximo de tokens a generar\n",
        "            )\n",
        "\n",
        "            self.is_initialized = True  # Marcar como inicializado\n",
        "            print(\"‚úÖ Sistema RAG inicializado correctamente\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error durante la inicializaci√≥n: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def process_documents(self, files: List[tempfile._TemporaryFileWrapper]) -> None:\n",
        "        \"\"\"\n",
        "        Procesa documentos cargados y actualiza la base de datos vectorial.\n",
        "\n",
        "        Args:\n",
        "            files: Lista de archivos temporales cargados por el usuario\n",
        "        \"\"\"\n",
        "        try:\n",
        "            documents = []  # Lista para almacenar todos los documentos\n",
        "            new_files = []  # Seguimiento de archivos nuevos procesados\n",
        "\n",
        "            print(f\"üìÑ Procesando {len(files)} documento(s)...\")\n",
        "\n",
        "            # Filtrar y procesar solo archivos que no se han procesado antes\n",
        "            for file in files:\n",
        "                if file.name not in self.processed_files:\n",
        "                    docs = DocumentLoader.load_file(file.name)  # Cargar el archivo\n",
        "                    documents.extend(docs)  # A√±adir documentos a la lista\n",
        "                    new_files.append(file.name)  # Registrar como nuevo\n",
        "                    self.processed_files.add(file.name)  # Marcar como procesado\n",
        "\n",
        "            # Si no hay archivos nuevos, terminar\n",
        "            if not new_files:\n",
        "                print(\"‚ÑπÔ∏è No hay documentos nuevos para procesar\")\n",
        "                return\n",
        "\n",
        "            # Verificar que se hayan cargado documentos\n",
        "            if not documents:\n",
        "                raise ValueError(\"No se pudieron cargar documentos.\")\n",
        "\n",
        "            # --------- DIVISI√ìN DE DOCUMENTOS ---------\n",
        "            # Dividir documentos en fragmentos m√°s peque√±os para procesamiento eficiente\n",
        "            print(\"‚úÇÔ∏è Dividiendo documentos en fragmentos...\")\n",
        "            text_splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=800,  # Tama√±o objetivo de cada fragmento (en caracteres)\n",
        "                chunk_overlap=200,  # Superposici√≥n entre fragmentos para mantener contexto\n",
        "                separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],  # Prioridad de separaci√≥n\n",
        "                length_function=len  # Funci√≥n para medir longitud\n",
        "            )\n",
        "\n",
        "            # Aplicar la divisi√≥n a todos los documentos\n",
        "            chunks = text_splitter.split_documents(documents)\n",
        "            print(f\"üß© Documentos divididos en {len(chunks)} fragmentos\")\n",
        "\n",
        "            # --------- VECTORIZACI√ìN Y ALMACENAMIENTO ---------\n",
        "            # Crear o actualizar la base de datos vectorial con los nuevos fragmentos\n",
        "            print(\"üîç Vectorizando fragmentos...\")\n",
        "            if self.vector_store is None:\n",
        "                # Primera carga: crear nueva base de datos vectorial\n",
        "                self.vector_store = FAISS.from_documents(chunks, self.embeddings)\n",
        "            else:\n",
        "                # Carga adicional: a√±adir a la base de datos existente\n",
        "                self.vector_store.add_documents(chunks)\n",
        "\n",
        "            # --------- CONFIGURACI√ìN DE PROMPT ---------\n",
        "            # Definir la plantilla de prompt para el LLM\n",
        "            prompt_template = \"\"\"\n",
        "            Contexto: {context}\n",
        "\n",
        "            Bas√°ndote √∫nicamente en el contexto proporcionado, responde a la siguiente pregunta de manera clara y concisa.\n",
        "            Si la informaci√≥n no est√° en el contexto, ind√≠calo expl√≠citamente.\n",
        "\n",
        "            Pregunta: {question}\n",
        "            \"\"\"\n",
        "\n",
        "            # Crear objeto de prompt con variables\n",
        "            PROMPT = PromptTemplate(\n",
        "                template=prompt_template,\n",
        "                input_variables=[\"context\", \"question\"]  # Variables a rellenar\n",
        "            )\n",
        "\n",
        "            # --------- CONFIGURACI√ìN DE CADENA QA ---------\n",
        "            # Inicializar la cadena de pregunta-respuesta con Ollama\n",
        "            print(\"‚öôÔ∏è Configurando cadena de pregunta-respuesta con Ollama...\")\n",
        "            self.qa_chain = RetrievalQA.from_chain_type(\n",
        "                llm=self.llm,  # Modelo de lenguaje\n",
        "                chain_type=\"stuff\",  # Tipo de cadena (insertar todo el contexto de una vez)\n",
        "                retriever=self.vector_store.as_retriever(\n",
        "                    search_kwargs={\"k\": 6}  # Recuperar los 6 fragmentos m√°s relevantes\n",
        "                ),\n",
        "                return_source_documents=True,  # Devolver documentos fuente para citas\n",
        "                chain_type_kwargs={\"prompt\": PROMPT}  # Usar nuestro prompt personalizado\n",
        "            )\n",
        "\n",
        "            print(f\"‚úÖ Procesamiento completado: {len(documents)} documentos a√±adidos a la base de conocimiento\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error procesando documentos: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    # ============================================================================\n",
        "    # M√âTODO 1: GENERACI√ìN MEDIANTE LANGCHAIN (m√°s robusto)\n",
        "    # ============================================================================\n",
        "\n",
        "    def generate_response(self, question: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Genera una respuesta utilizando el framework LangChain.\n",
        "        Este m√©todo es m√°s robusto y estructurado, con mejor manejo de errores.\n",
        "\n",
        "        Args:\n",
        "            question: Pregunta del usuario\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con la respuesta y fuentes utilizadas\n",
        "        \"\"\"\n",
        "        # Verificar que el sistema est√© inicializado\n",
        "        if not self.is_initialized or self.vector_store is None:\n",
        "            return {\n",
        "                'answer': \"Por favor, carga algunos documentos antes de hacer preguntas.\",\n",
        "                'sources': []\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            print(f\"‚ùì Procesando pregunta: {question}\")\n",
        "\n",
        "            # Ejecutar la cadena QA con LangChain y Ollama\n",
        "            result = self.qa_chain({\"query\": question})\n",
        "\n",
        "            # Preparar la respuesta estructurada\n",
        "            response = {\n",
        "                'answer': result['result'],  # Respuesta generada\n",
        "                'sources': []  # Lista para fuentes\n",
        "            }\n",
        "\n",
        "            # A√±adir informaci√≥n sobre las fuentes utilizadas\n",
        "            for doc in result['source_documents']:\n",
        "                source = {\n",
        "                    'title': doc.metadata.get('title', 'Desconocido'),\n",
        "                    'content': doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content,\n",
        "                    'metadata': doc.metadata\n",
        "                }\n",
        "                response['sources'].append(source)\n",
        "\n",
        "            print(\"‚úÖ Respuesta generada con √©xito usando el m√©todo LangChain\")\n",
        "            return response\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error generando respuesta con LangChain: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    # ============================================================================\n",
        "    # M√âTODO 2: GENERACI√ìN DIRECTA CON API DE OLLAMA (m√°s r√°pido)\n",
        "    # ============================================================================\n",
        "\n",
        "    def generate_with_raw_ollama(self, question: str, context: str) -> str:\n",
        "        \"\"\"\n",
        "        Genera una respuesta usando directamente la API HTTP de Ollama.\n",
        "        Este m√©todo es m√°s r√°pido pero menos robusto que el m√©todo LangChain.\n",
        "\n",
        "        Args:\n",
        "            question: Pregunta del usuario\n",
        "            context: Contexto recuperado de la base de conocimiento\n",
        "\n",
        "        Returns:\n",
        "            Texto de respuesta generado\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Formatear el prompt con contexto y pregunta\n",
        "            formatted_prompt = f\"\"\"Contexto:\n",
        "{context}\n",
        "\n",
        "Bas√°ndote √∫nicamente en el contexto proporcionado, responde a la siguiente pregunta de manera clara y concisa.\n",
        "Si la informaci√≥n no est√° en el contexto, ind√≠calo expl√≠citamente.\n",
        "\n",
        "Pregunta: {question}\n",
        "\"\"\"\n",
        "\n",
        "            # Configurar la llamada HTTP a Ollama\n",
        "            headers = {\"Content-Type\": \"application/json\"}\n",
        "            payload = {\n",
        "                \"model\": self.ollama_model,\n",
        "                \"prompt\": formatted_prompt,\n",
        "                \"stream\": False,  # No usar streaming para simplificar\n",
        "                \"temperature\": 0.1,  # Consistente con el otro m√©todo\n",
        "                \"num_predict\": 512  # N√∫mero m√°ximo de tokens\n",
        "            }\n",
        "\n",
        "            # Realizar la llamada API HTTP directa\n",
        "            print(\"Llamando a la API de Ollama con requests...\")\n",
        "            response = requests.post(\n",
        "                \"http://localhost:11434/api/generate\",\n",
        "                headers=headers,\n",
        "                json=payload\n",
        "            )\n",
        "\n",
        "            # Procesar la respuesta\n",
        "            if response.status_code == 200:\n",
        "                respuesta_json = response.json()\n",
        "                respuesta = respuesta_json.get('response', 'No se obtuvo respuesta')\n",
        "                return respuesta\n",
        "            else:\n",
        "                return f\"Error en la API de Ollama: C√≥digo {response.status_code}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error llamando directamente a Ollama: {str(e)}\")\n",
        "            # Si falla, devolver mensaje de error y sugerir usar el m√©todo est√°ndar\n",
        "            return \"Error al usar Ollama directamente. Intenta desactivar 'Usar Ollama directo'.\"\n",
        "\n",
        "# ============================================================================\n",
        "# SECCI√ìN 7: FUNCI√ìN DE PROCESAMIENTO DE RESPUESTAS\n",
        "# ============================================================================\n",
        "\n",
        "def process_response(user_input: str, chat_history, files, use_direct_ollama=True):\n",
        "    \"\"\"\n",
        "    Procesa la entrada del usuario y genera una respuesta utilizando el sistema RAG.\n",
        "    Esta funci√≥n coordina todo el proceso de consulta desde la entrada hasta la respuesta.\n",
        "\n",
        "    Args:\n",
        "        user_input: Pregunta o instrucci√≥n del usuario\n",
        "        chat_history: Historial de chat actual\n",
        "        files: Archivos cargados por el usuario\n",
        "        use_direct_ollama: Si es True, usa la API directa de Ollama; si es False, usa LangChain\n",
        "\n",
        "    Returns:\n",
        "        Historial de chat actualizado con la nueva pregunta y respuesta\n",
        "    \"\"\"\n",
        "    # Ignorar entradas vac√≠as\n",
        "    if not user_input.strip():\n",
        "        return chat_history\n",
        "\n",
        "    try:\n",
        "        # PASO 1: Inicializaci√≥n si es necesario\n",
        "        if not rag_system.is_initialized:\n",
        "            rag_system.initialize_system()\n",
        "\n",
        "        # PASO 2: Procesar documentos si hay archivos nuevos\n",
        "        if files:\n",
        "            rag_system.process_documents(files)\n",
        "\n",
        "        # Verificar que haya documentos procesados\n",
        "        if rag_system.vector_store is None:\n",
        "            answer = \"Por favor, carga algunos documentos antes de hacer preguntas.\"\n",
        "            chat_history.append((user_input, answer))\n",
        "            return chat_history\n",
        "\n",
        "        # PASO 3: Recuperar documentos relevantes para la consulta\n",
        "        print(\"üîç Buscando documentos relevantes...\")\n",
        "        documents = rag_system.vector_store.similarity_search(user_input, k=6)\n",
        "        # Unir el contenido de los documentos como contexto\n",
        "        context = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
        "\n",
        "        # PASO 4: Generar respuesta seg√∫n el m√©todo seleccionado\n",
        "        if use_direct_ollama:\n",
        "            # --------- M√âTODO DIRECTO (M√ÅS R√ÅPIDO) ---------\n",
        "            try:\n",
        "                print(\"üöÄ Usando m√©todo directo de Ollama...\")\n",
        "                answer = rag_system.generate_with_raw_ollama(user_input, context)\n",
        "\n",
        "                # Implementaci√≥n de fallback: si hay error, usar m√©todo est√°ndar\n",
        "                if answer.startswith(\"Error\"):\n",
        "                    print(\"‚ö†Ô∏è Retrocediendo al m√©todo est√°ndar...\")\n",
        "                    response = rag_system.generate_response(user_input)\n",
        "                    answer = response['answer']\n",
        "\n",
        "                    # A√±adir informaci√≥n de fuentes\n",
        "                    sources = set([doc.metadata.get('title', 'Desconocido') for doc in documents[:3]])\n",
        "                    if sources:\n",
        "                        answer += \"\\n\\nüìö Fuentes consultadas:\\n\" + \"\\n\".join([f\"‚Ä¢ {source}\" for source in sources])\n",
        "            except Exception as ollama_error:\n",
        "                # Manejo de error: si falla el m√©todo directo, usar el est√°ndar\n",
        "                print(f\"‚ùå Error en m√©todo directo: {str(ollama_error)}\")\n",
        "                print(\"‚ö†Ô∏è Retrocediendo al m√©todo est√°ndar...\")\n",
        "                response = rag_system.generate_response(user_input)\n",
        "                answer = response['answer']\n",
        "\n",
        "                # A√±adir informaci√≥n de fuentes\n",
        "                sources = set([doc.metadata.get('title', 'Desconocido') for doc in documents[:3]])\n",
        "                if sources:\n",
        "                    answer += \"\\n\\nüìö Fuentes consultadas:\\n\" + \"\\n\".join([f\"‚Ä¢ {source}\" for source in sources])\n",
        "        else:\n",
        "            # --------- M√âTODO EST√ÅNDAR (M√ÅS ROBUSTO) ---------\n",
        "            print(\"üîÑ Usando m√©todo est√°ndar de LangChain...\")\n",
        "            response = rag_system.generate_response(user_input)\n",
        "            answer = response['answer']\n",
        "\n",
        "            # A√±adir informaci√≥n de fuentes\n",
        "            sources = set([doc.metadata.get('title', 'Desconocido') for doc in documents[:3]])\n",
        "            if sources:\n",
        "                answer += \"\\n\\nüìö Fuentes consultadas:\\n\" + \"\\n\".join([f\"‚Ä¢ {source}\" for source in sources])\n",
        "\n",
        "        # PASO 5: Actualizar el historial de chat y retornar\n",
        "        chat_history.append((user_input, answer))\n",
        "        return chat_history\n",
        "\n",
        "    except Exception as e:\n",
        "        # Manejo de errores generales\n",
        "        error_message = f\"Lo siento, ocurri√≥ un error: {str(e)}\"\n",
        "        print(f\"‚ùå Error general en process_response: {str(e)}\")\n",
        "        chat_history.append((user_input, error_message))\n",
        "        return chat_history\n",
        "\n"
      ],
      "metadata": {
        "id": "WfO0j0klb57D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECCI√ìN 8: INICIALIZACI√ìN DEL SISTEMA\n",
        "# ============================================================================\n",
        "\n",
        "# Crear la instancia del sistema RAG\n",
        "print(\"üîß Inicializando sistema RAG con Ollama...\")\n",
        "rag_system = RAGSystem()\n",
        "print(\"‚úÖ Sistema RAG creado correctamente\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECCI√ìN 9: INTERFAZ GRADIO\n",
        "# ============================================================================\n",
        "\n",
        "# Crear la interfaz web con Gradio\n",
        "print(\"üåê Creando interfaz Gradio...\")\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    # Encabezado\n",
        "    gr.HTML(\"\"\"\n",
        "        <div style=\"text-align: center; max-width: 800px; margin: 0 auto; padding: 20px;\">\n",
        "            <h1 style=\"color: #2d333a;\">üìö RAG Assistant con Ollama</h1>\n",
        "            <p style=\"color: #4a5568;\">\n",
        "                Asistente IA para an√°lisis y consulta de documentos usando Ollama\n",
        "            </p>\n",
        "        </div>\n",
        "    \"\"\")\n",
        "\n",
        "    # Secci√≥n de carga de archivos y configuraci√≥n\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            # Selector de archivos\n",
        "            files = gr.Files(\n",
        "                label=\"Carga tus documentos\",\n",
        "                file_types=SUPPORTED_FORMATS,\n",
        "                file_count=\"multiple\"\n",
        "            )\n",
        "\n",
        "            # Opci√≥n para seleccionar m√©todo de generaci√≥n\n",
        "            use_direct_ollama = gr.Checkbox(\n",
        "                label=\"Usar Ollama directo (m√°s r√°pido)\",\n",
        "                value=False,  # Falso por defecto para mayor estabilidad\n",
        "                info=\"Hace llamadas directas a la API de Ollama para respuestas m√°s r√°pidas.\"\n",
        "            )\n",
        "\n",
        "            # Informaci√≥n sobre formatos soportados\n",
        "            gr.HTML(\"\"\"\n",
        "                <div style=\"font-size: 0.9em; color: #666; margin-top: 0.5em;\">\n",
        "                    Formatos soportados: PDF, DOCX, CSV, TXT\n",
        "                </div>\n",
        "            \"\"\")\n",
        "\n",
        "    # Interfaz de chat\n",
        "    chatbot = gr.Chatbot(\n",
        "        show_label=False,\n",
        "        container=True,\n",
        "        height=500,\n",
        "        bubble_full_width=False,\n",
        "        show_copy_button=True,\n",
        "        scale=2\n",
        "    )\n",
        "\n",
        "    # √Årea de entrada de texto y bot√≥n de limpieza\n",
        "    with gr.Row():\n",
        "        message = gr.Textbox(\n",
        "            placeholder=\"üí≠ Pregunta cualquier cosa sobre tus documentos...\",\n",
        "            show_label=False,\n",
        "            container=False,\n",
        "            scale=8,\n",
        "            autofocus=True\n",
        "        )\n",
        "        clear = gr.Button(\"üóëÔ∏è Limpiar\", size=\"sm\", scale=1)\n",
        "\n",
        "    # Secci√≥n de instrucciones\n",
        "    gr.HTML(\"\"\"\n",
        "        <div style=\"background-color: #f8f9fa; padding: 15px; border-radius: 10px; margin: 20px 0;\">\n",
        "            <h3 style=\"color: #2d333a; margin-bottom: 10px;\">üîç C√≥mo usar:</h3>\n",
        "            <ol style=\"color: #666; margin-left: 20px;\">\n",
        "                <li>Carga uno o m√°s documentos (PDF, DOCX, CSV, o TXT)</li>\n",
        "                <li>Espera a que los documentos sean procesados</li>\n",
        "                <li>Haz preguntas sobre el contenido de tus documentos</li>\n",
        "                <li>Activa \"Usar Ollama directo\" para respuestas m√°s r√°pidas (desact√≠valo si hay errores)</li>\n",
        "            </ol>\n",
        "            <p style=\"color: #666; font-style: italic; margin-top: 10px;\">\n",
        "                Nota: La primera respuesta puede tardar un poco. Desde la segunda respuesta es m√°s r√°pido.\n",
        "            </p>\n",
        "        </div>\n",
        "    \"\"\")\n",
        "\n",
        "    # Pie de p√°gina con informaci√≥n t√©cnica y cr√©ditos\n",
        "    gr.HTML(\"\"\"\n",
        "        <div style=\"text-align: center; max-width: 800px; margin: 20px auto; padding: 20px;\n",
        "                    background-color: #f8f9fa; border-radius: 10px;\">\n",
        "            <div style=\"margin-bottom: 15px;\">\n",
        "                <h3 style=\"color: #2d333a;\">‚ö° Sobre este asistente</h3>\n",
        "                <p style=\"color: #666; font-size: 14px;\">\n",
        "                    Esta aplicaci√≥n utiliza tecnolog√≠a RAG (Retrieval Augmented Generation) combinando:\n",
        "                </p>\n",
        "                <ul style=\"list-style: none; color: #666; font-size: 14px;\">\n",
        "                    <li>üîπ Motor LLM: Ollama con Llama2</li>\n",
        "                    <li>üîπ Embeddings: multilingual-e5-small</li>\n",
        "                    <li>üîπ Base de datos vectorial: FAISS</li>\n",
        "                </ul>\n",
        "            </div>\n",
        "            <div style=\"border-top: 1px solid #ddd; padding-top: 15px;\">\n",
        "                <p style=\"color: #666; font-size: 14px;\">\n",
        "                    Creado para el curso de Inteligencia Artificial Aplicada - Universidad de los Andes<br>\n",
        "                    Por <a href=\"https://www.linkedin.com/in/camilovegabarbosa/\"\n",
        "                    target=\"_blank\" style=\"color: #2196F3; text-decoration: none;\">Camilo Vega</a>,\n",
        "                    Profesor de IA ü§ñ\n",
        "                </p>\n",
        "            </div>\n",
        "        </div>\n",
        "    \"\"\")\n",
        "\n",
        "    # --------- FUNCIONES DE CONTROL DE LA INTERFAZ ---------\n",
        "    # Funci√≥n para limpiar el contexto y reiniciar\n",
        "    def clear_context():\n",
        "        # Eliminar la base de conocimiento y reiniciar el registro de archivos\n",
        "        rag_system.vector_store = None\n",
        "        rag_system.processed_files.clear()\n",
        "        return None\n",
        "\n",
        "    # Conectar eventos de la interfaz con funciones\n",
        "    message.submit(process_response, [message, chatbot, files, use_direct_ollama], [chatbot])\n",
        "    clear.click(clear_context, None, chatbot)\n",
        "\n",
        "# Lanzar la interfaz web\n",
        "print(\"üöÄ Lanzando interfaz Gradio...\")\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "mMf6tK8fegNL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}