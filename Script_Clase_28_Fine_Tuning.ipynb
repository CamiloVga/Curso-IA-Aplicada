{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPiDYKNTFkQEwI2nluIKlGW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CamiloVga/Curso-IA-Aplicada/blob/main/Script_Clase_28_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üé® Inteligencia Artificial Aplicada\n",
        "## Universidad de los Andes\n",
        "\n",
        "### üë®‚Äçüè´ Profesores\n",
        "- **Profesor Magistral:** [Camilo Vega Barbosa](https://www.linkedin.com/in/camilovegabarbosa/)\n",
        "- **Asistente de Docencia:** [Sergio Julian Zona Moreno](https://www.linkedin.com/in/sergiozonamoreno/)\n",
        "\n",
        "### üìö Fine-Tuning con PEFT-LoRA para Modelos de Lenguaje\n",
        "Este script implementa un proceso de fine-tuning eficiente para modelos de lenguaje:\n",
        "\n",
        "1. **Configuraci√≥n del Modelo y Datos üöÄ**\n",
        "   - Uso de Microsoft Phi-2, un modelo de 2.7B de par√°metros con alta capacidad\n",
        "   - Carga de datos especializados para aprender el \"idioma 4\" (reemplazar \"a\" por \"4\")\n",
        "   - Preparaci√≥n eficiente de tokens y formato de instrucci√≥n\n",
        "   - Integraci√≥n con GitHub para obtener el dataset directamente desde repositorio\n",
        "\n",
        "2. **Implementaci√≥n de PEFT-LoRA üß†**\n",
        "   - Parameter-Efficient Fine-Tuning (PEFT) con Low-Rank Adaptation (LoRA)\n",
        "   - Modificaci√≥n selectiva de solo ~1% de los par√°metros del modelo\n",
        "   - Enfoque en capas estrat√©gicas: q_proj, k_proj, v_proj, o_proj, fc1, fc2\n",
        "   - Cuantizaci√≥n de 8-bits para reducci√≥n dr√°stica de memoria requerida\n",
        "   - Preservaci√≥n de los pesos originales del modelo para mantener capacidades base\n",
        "\n",
        "3. **Proceso de Entrenamiento Optimizado üìà**\n",
        "   - Configuraci√≥n adaptable: modo r√°pido para pruebas y modo profundo para producci√≥n\n",
        "   - Visualizaci√≥n en tiempo real de la curva de p√©rdida para monitoreo del aprendizaje\n",
        "   - Sistema de callbacks personalizados para seguimiento del progreso\n",
        "   - Guardado eficiente del modelo adaptado con solo los par√°metros LoRA (~MB vs ~GB)\n",
        "\n",
        "4. **Inferencia y Aplicaci√≥n Interactiva üîç**\n",
        "   - Interfaz Gradio para pruebas del modelo con cualquier texto de entrada\n",
        "   - Generaci√≥n controlada con par√°metros ajustables (temperatura, top_p)\n",
        "   - Ejemplos predefinidos para demostraci√≥n inmediata\n",
        "   - Despliegue web temporal para compartir el modelo con otros usuarios\n",
        "\n"
      ],
      "metadata": {
        "id": "GWvTTRsiLuVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalamos las bibliotecas necesarias\n",
        "!pip install -q transformers datasets peft bitsandbytes accelerate gradio\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# 1. CARGA DE DATOS\n",
        "# Descargamos los datos directamente desde el repositorio de GitHub\n",
        "# Definimos la ruta al archivo CSV en tu repositorio\n",
        "github_repo = \"CamiloVga/Curso-IA-Aplicada\"\n",
        "branch = \"main\"\n",
        "file_path = \"Semana 14_Fine-Tuning y RAG/Base Fine-Tuning Idioma 4.csv\"\n",
        "\n",
        "# Cargamos el dataset desde GitHub\n",
        "print(\"Cargando dataset desde GitHub...\")\n",
        "dataset = load_dataset(\"csv\",\n",
        "                      data_files=f\"https://raw.githubusercontent.com/{github_repo}/{branch}/{file_path}\")\n",
        "\n",
        "# Exploramos el dataset\n",
        "print(\"Estructura del dataset:\")\n",
        "print(dataset)\n",
        "print(\"\\nEjemplo de entrada:\")\n",
        "print(dataset[\"train\"][0])\n",
        "print(f\"N√∫mero de ejemplos: {len(dataset['train'])}\")\n",
        "\n",
        "# 2. PREPROCESAMIENTO DE DATOS\n",
        "# Phi-2 es un modelo m√°s peque√±o (2.7B) pero de alto rendimiento y acceso abierto\n",
        "model_name = \"microsoft/phi-2\"\n",
        "\n",
        "print(f\"Cargando tokenizador para {model_name}...\")\n",
        "# Cargamos el tokenizador\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Aseguramos que el tokenizador tenga tokens especiales necesarios\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Definimos una funci√≥n para preprocesar los datos\n",
        "def preprocess_function(examples):\n",
        "    # Formateamos los textos como instrucciones siguiendo un formato tipo chat\n",
        "    prompts = []\n",
        "    for i in range(len(examples[\"input\"])):\n",
        "        # Formato de instrucci√≥n para Phi-2\n",
        "        prompt = f\"<s>Traduce este texto al idioma 4: {examples['input'][i]}\\n\\n{examples['output'][i]}</s>\"\n",
        "        prompts.append(prompt)\n",
        "\n",
        "    # Tokenizamos los textos\n",
        "    tokenized_inputs = tokenizer(\n",
        "        prompts,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Configuramos las labels igual que los input_ids para entrenamiento de LM causal\n",
        "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].clone()\n",
        "\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Aplicamos la funci√≥n de preprocesamiento al dataset\n",
        "print(\"Procesando y tokenizando el dataset...\")\n",
        "tokenized_dataset = dataset[\"train\"].map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"train\"].column_names\n",
        ")\n",
        "\n",
        "print(f\"Dataset tokenizado: {tokenized_dataset}\")\n",
        "\n",
        "# 3. CONFIGURACI√ìN DEL MODELO\n",
        "# Configuramos BitsAndBytes para cuantizaci√≥n de 8 bits (ahorra memoria)\n",
        "print(\"Configurando cuantizaci√≥n para el modelo...\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,  # Phi-2 funciona bien con cuantizaci√≥n de 8 bits\n",
        "    bnb_8bit_use_double_quant=True,\n",
        "    bnb_8bit_quant_type=\"nf4\",\n",
        "    bnb_8bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Cargamos el modelo base con cuantizaci√≥n\n",
        "print(f\"Cargando modelo {model_name} con cuantizaci√≥n...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Preparamos el modelo para entrenamiento con LoRA\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# 4. CONFIGURACI√ìN DE LORA\n",
        "# Para Phi-2, ajustamos los target_modules a su arquitectura espec√≠fica\n",
        "print(\"Configurando adaptadores LoRA...\")\n",
        "peft_config = LoraConfig(\n",
        "    r=16,  # Dimensi√≥n del adaptador LoRA - Determina el tama√±o de las matrices de bajo rango\n",
        "           # Valores m√°s altos aumentan la capacidad pero tambi√©n el n√∫mero de par√°metros\n",
        "           # T√≠picamente entre 8-64 dependiendo de la complejidad de la tarea\n",
        "\n",
        "    lora_alpha=32,  # Par√°metro de escala - Factor que controla la magnitud de la actualizaci√≥n LoRA\n",
        "                    # Generalmente se establece como 2*r para un buen equilibrio\n",
        "                    # Valores m√°s altos = mayor impacto de las actualizaciones LoRA\n",
        "\n",
        "    lora_dropout=0.05,  # Regularizaci√≥n - Aplica dropout a las capas LoRA para prevenir sobreajuste\n",
        "                        # Valores t√≠picos entre 0.01-0.1\n",
        "\n",
        "    bias=\"none\",  # Configuraci√≥n para bias - \"none\" significa que no se entrenan los sesgos\n",
        "                  # Otras opciones: \"all\" (todos los sesgos) o \"lora_only\" (solo sesgos en capas LoRA)\n",
        "\n",
        "    task_type=\"CAUSAL_LM\",  # Tipo de tarea - Modelado de lenguaje causal (generaci√≥n de texto)\n",
        "                           # Otras opciones: \"SEQ_CLS\" (clasificaci√≥n), \"SEQ_2_SEQ_LM\" (seq2seq), etc.\n",
        "\n",
        "    # Ajustamos los m√≥dulos objetivo seg√∫n la arquitectura de Phi-2\n",
        "    # Estas son las √∫nicas capas del modelo que se modificar√°n durante el entrenamiento\n",
        "    target_modules=[\n",
        "        \"q_proj\",  # Proyecci√≥n de queries en los bloques de atenci√≥n\n",
        "                  # Transforma los vectores de entrada en queries para la auto-atenci√≥n\n",
        "\n",
        "        \"k_proj\",  # Proyecci√≥n de keys en los bloques de atenci√≥n\n",
        "                  # Transforma los vectores de entrada en keys para la auto-atenci√≥n\n",
        "\n",
        "        \"v_proj\",  # Proyecci√≥n de values en los bloques de atenci√≥n\n",
        "                  # Transforma los vectores de entrada en values para la auto-atenci√≥n\n",
        "\n",
        "        \"o_proj\",  # Proyecci√≥n de output en los bloques de atenci√≥n\n",
        "                  # Combina los resultados de la atenci√≥n para la salida\n",
        "\n",
        "        \"fc1\",    # Primera capa feed-forward en los bloques del transformador\n",
        "                 # Expande la dimensionalidad (MLP)\n",
        "\n",
        "        \"fc2\"     # Segunda capa feed-forward en los bloques del transformador\n",
        "                 # Reduce la dimensionalidad de vuelta (MLP)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Convertimos el modelo a un modelo PEFT usando LoRA\n",
        "# Esto a√±ade adaptadores de bajo rango a las capas especificadas\n",
        "# sin modificar los pesos originales del modelo\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "# Imprimir par√°metros entrenables vs totales\n",
        "# Esto mostrar√° la eficiencia de LoRA - t√≠picamente solo entrena <1% de par√°metros\n",
        "print(\"Resumen de par√°metros del modelo:\")\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AwjVMXkDF4gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "info=pd.DataFrame(dataset)\n",
        "info"
      ],
      "metadata": {
        "id": "AAtWnU55kRdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. ENTRENAMIENTO\n",
        "# Configuramos los argumentos de entrenamiento para un entrenamiento r√°pido inicial\n",
        "print(\"Configurando par√°metros de entrenamiento...\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_idioma4_phi2\",\n",
        "    # Configuraci√≥n para entrenamiento r√°pido\n",
        "    per_device_train_batch_size=8,         # Tama√±o de batch m√°s grande para velocidad\n",
        "                                          # Para entrenamiento profundo: reducir a 2-4\n",
        "\n",
        "    gradient_accumulation_steps=2,        # Menos pasos de acumulaci√≥n para velocidad\n",
        "                                          # Para entrenamiento profundo: aumentar a 8-16\n",
        "\n",
        "    warmup_steps=5,                       # Menos pasos de calentamiento\n",
        "                                          # Para entrenamiento profundo: aumentar a 50-100\n",
        "\n",
        "    max_steps=50,                         # Menos pasos totales para prueba r√°pida\n",
        "                                          # Para entrenamiento profundo: aumentar a 500-1000\n",
        "\n",
        "    learning_rate=3e-4,                   # Learning rate m√°s alto para convergencia r√°pida\n",
        "                                          # Para entrenamiento profundo: reducir a 1e-4 o 5e-5\n",
        "\n",
        "    fp16=True,                            # Usar precisi√≥n mixta para acelerar el entrenamiento\n",
        "\n",
        "    logging_steps=5,                      # Registro frecuente para ver progreso\n",
        "\n",
        "    save_steps=5,                         # Guardar checkpoints\n",
        "                                          # Para entrenamiento profundo: cada 100-200 pasos\n",
        "\n",
        "    save_total_limit=2,                   # Mantener menos checkpoints para ahorrar espacio\n",
        "\n",
        "    report_to=\"none\",                     # No enviar m√©tricas a servicios externos\n",
        "\n",
        "    # Par√°metros adicionales para entrenamiento m√°s estable\n",
        "    # Descomentar para entrenamiento profundo:\n",
        "    # weight_decay=0.01,                  # Regularizaci√≥n L2 para evitar sobreajuste\n",
        "    # lr_scheduler_type=\"cosine\",         # Programaci√≥n de tasa de aprendizaje tipo coseno\n",
        "    # max_grad_norm=1.0,                  # Recorte de gradiente para estabilidad\n",
        ")\n",
        "\n",
        "# Creamos un callback personalizado para registrar las p√©rdidas durante el entrenamiento\n",
        "from transformers import TrainerCallback\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class LossCallback(TrainerCallback):\n",
        "    def __init__(self):\n",
        "        self.losses = []\n",
        "        self.steps = []\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs is not None and \"loss\" in logs:\n",
        "            self.losses.append(logs[\"loss\"])\n",
        "            self.steps.append(state.global_step)\n",
        "\n",
        "    def plot_loss(self):\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(self.steps, self.losses, marker='o', linestyle='-', color='blue')\n",
        "        plt.title('P√©rdida durante el entrenamiento', fontsize=16)\n",
        "        plt.xlabel('Pasos de entrenamiento', fontsize=14)\n",
        "        plt.ylabel('P√©rdida', fontsize=14)\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('loss_plot.png')  # Guardar la gr√°fica como imagen\n",
        "        plt.show()\n",
        "\n",
        "# Instanciamos el callback\n",
        "loss_callback = LossCallback()\n",
        "\n",
        "# Collator de datos para el entrenamiento\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # No es masked language modeling, sino causal\n",
        ")\n",
        "\n",
        "# Inicializamos el Trainer con nuestro callback\n",
        "print(\"Inicializando Trainer...\")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator,\n",
        "    callbacks=[loss_callback]  # A√±adimos nuestro callback para registrar las p√©rdidas\n",
        ")\n",
        "\n",
        "# Entrenamos el modelo\n",
        "print(\"Comenzando entrenamiento...\")\n",
        "trainer.train()\n",
        "print(\"Entrenamiento completado.\")\n",
        "\n",
        "# Graficamos la p√©rdida\n",
        "print(\"Generando gr√°fica de p√©rdida...\")\n",
        "loss_callback.plot_loss()\n",
        "\n",
        "# 6. GUARDADO DEL MODELO\n",
        "# Guardamos el modelo adaptado con LoRA\n",
        "print(\"Guardando modelo y tokenizador...\")\n",
        "model.save_pretrained(\"./idioma4_phi2_lora\")\n",
        "tokenizer.save_pretrained(\"./idioma4_phi2_lora\")\n",
        "print(\"Modelo guardado en './idioma4_phi2_lora'\")\n",
        "\n"
      ],
      "metadata": {
        "id": "IF7_46vCJaGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. INFERENCIA CON GRADIO\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "def generate_idioma4(input_text, max_length=200):\n",
        "    \"\"\"Traduce texto al idioma 4 usando el modelo fine-tuneado.\"\"\"\n",
        "    # Manejamos el caso de texto vac√≠o\n",
        "    if not input_text.strip():\n",
        "        return \"Por favor ingresa alg√∫n texto para traducir.\"\n",
        "\n",
        "    # Preparamos el prompt seg√∫n el formato usado en entrenamiento\n",
        "    prompt = f\"<s>Traduce este texto al idioma 4: {input_text}\\n\\n\"\n",
        "\n",
        "    # Tokenizamos\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generamos la respuesta\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs.input_ids,\n",
        "            attention_mask=inputs.attention_mask,\n",
        "            max_length=max_length,\n",
        "            temperature=0.7,        # Controla la aleatoriedad (m√°s bajo = m√°s determinista)\n",
        "            top_p=0.9,              # Muestreo nucleus - considera tokens con probabilidad acumulada de 0.9\n",
        "            do_sample=True,         # Muestreo probabil√≠stico en lugar de greedy decoding\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    # Decodificamos la respuesta\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extraemos solo la parte de respuesta (despu√©s del doble salto de l√≠nea)\n",
        "    if \"\\n\\n\" in generated_text:\n",
        "        response = generated_text.split(\"\\n\\n\")[1].strip()\n",
        "    else:\n",
        "        response = generated_text.replace(prompt, \"\").strip()\n",
        "\n",
        "    return response\n",
        "\n",
        "# Creamos la interfaz con Gradio\n",
        "def create_gradio_interface():\n",
        "    \"\"\"Crea y lanza una interfaz Gradio para la traducci√≥n al idioma 4.\"\"\"\n",
        "\n",
        "    # Definimos la interfaz\n",
        "    demo = gr.Interface(\n",
        "        fn=generate_idioma4,              # Funci√≥n que procesa la entrada\n",
        "        inputs=[\n",
        "            gr.Textbox(\n",
        "                placeholder=\"Escribe texto para traducir al idioma 4...\",\n",
        "                label=\"Texto Original\",\n",
        "                lines=5\n",
        "            )\n",
        "        ],\n",
        "        outputs=[\n",
        "            gr.Textbox(label=\"Traducci√≥n al Idioma 4\", lines=5)\n",
        "        ],\n",
        "        title=\"Traductor al Idioma 4\",\n",
        "        description=\"\"\"\n",
        "        <h3>¬°Bienvenido al Traductor de Idioma 4!</h3>\n",
        "        <p>Este es un modelo de lenguaje fine-tuneado con PEFT-LoRA para traducir texto al \"idioma 4\",\n",
        "        donde cada letra 'a' se reemplaza por el n√∫mero '4'.</p>\n",
        "        <p><b>Ejemplo:</b> \"La casa amarilla\" ‚Üí \"L4 c4s4 4m4rill4\"</p>\n",
        "        \"\"\",\n",
        "        examples=[\n",
        "            [\"Hola a todos, me llamo Camilo\"],\n",
        "            [\"La casa amarilla est√° en la playa\"],\n",
        "            [\"Vamos a aprender inteligencia artificial\"],\n",
        "            [\"El agua clara cae desde la alta cascada\"]\n",
        "        ],\n",
        "        theme=gr.themes.Soft()  # Tema visual m√°s atractivo\n",
        "    )\n",
        "\n",
        "    # Lanzamos la interfaz\n",
        "    demo.launch(share=True)  # share=True crea un enlace compartible\n",
        "    return demo\n",
        "\n",
        "# Lanzamos la interfaz Gradio\n",
        "print(\"Iniciando interfaz Gradio para el traductor de Idioma 4...\")\n",
        "interface = create_gradio_interface()"
      ],
      "metadata": {
        "id": "gaZCB29eJcPB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}