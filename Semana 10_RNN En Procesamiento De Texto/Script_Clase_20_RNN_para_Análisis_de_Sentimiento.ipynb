{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOa3jiT9R2/PV8ZYrj1JtoV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CamiloVga/Curso-IA-Aplicada/blob/main/Semana%2010_RNN%20En%20Procesamiento%20De%20Texto/Script_Clase_20_RNN_para_An%C3%A1lisis_de_Sentimiento.ipynb)"
  ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ñ Inteligencia Artificial Aplicada para la Econom√≠a\n",
        "## Universidad de los Andes\n",
        "\n",
        "### üë®‚Äçüè´ Profesores\n",
        "- **Profesor Magistral:** [Camilo Vega Barbosa](https://www.linkedin.com/in/camilovegabarbosa/)\n",
        "- **Asistente de Docencia:** [Sergio Julian Zona Moreno](https://www.linkedin.com/in/sergiozonamoreno/)\n",
        "\n",
        "### üìö An√°lisis de Sentimiento con RNN para Tweets Financieros\n",
        "Este notebook implementa Redes Neuronales Recurrentes (RNN) para an√°lisis de sentimiento en tweets financieros, utilizando el dataset **TimKoornstra/financial-tweets-sentiment**:\n",
        "\n",
        "1. **Preprocesamiento de Texto para Tweets Financieros üßπ**\n",
        "   - Limpieza y normalizaci√≥n de tweets del sector financiero\n",
        "   - Tokenizaci√≥n adaptada para texto de redes sociales\n",
        "   - Manejo de vocabulario especializado en finanzas y mercados\n",
        "\n",
        "2. **Arquitecturas RNN Avanzadas üß†**\n",
        "   - Simple RNN: Fundamentos de procesamiento secuencial\n",
        "   - LSTM: Manejo de dependencias a largo plazo\n",
        "   - GRU: Optimizaci√≥n computacional con rendimiento comparable\n",
        "   - Redes Bidireccionales: Captura de contexto completo\n",
        "\n",
        "3. **Embeddings Sem√°nticos üî§**\n",
        "   - Embeddings tradicionales vs. BERT\n",
        "   - Transferencia de conocimiento desde modelos preentrenados\n",
        "   - Representaci√≥n contextual de t√©rminos financieros y burs√°tiles\n",
        "\n",
        "4. **Optimizaci√≥n de Hiperpar√°metros üìä**\n",
        "   - Grid Search exhaustivo\n",
        "   - Evaluaci√≥n comparativa de arquitecturas\n",
        "   - M√©tricas especializadas para clasificaci√≥n de sentimiento\n",
        "   - Visualizaci√≥n de resultados experimentales"
      ],
      "metadata": {
        "id": "DD59pSh5hsqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################################################\n",
        "# SECCI√ìN 1: INSTALACI√ìN DE LIBRER√çAS Y CONFIGURACI√ìN INICIAL\n",
        "#####################################################################\n",
        "\n",
        "# Instalaci√≥n de paquetes necesarios\n",
        "!pip install transformers datasets tensorflow nltk scikit-learn matplotlib pandas -q\n",
        "\n",
        "# Importaciones principales organizadas por categor√≠a\n",
        "# --- An√°lisis de datos y visualizaci√≥n ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- Procesamiento de lenguaje natural ---\n",
        "import re\n",
        "import nltk\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "\n",
        "# --- Modelos de Deep Learning ---\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, GRU, SimpleRNN, Bidirectional\n",
        "from tensorflow.keras.layers import Embedding, GlobalMaxPooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# --- Evaluaci√≥n de modelos ---\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "# --- Otros utilitarios ---\n",
        "from datasets import load_dataset\n",
        "import time\n",
        "import itertools\n",
        "\n",
        "# Descarga recursos de NLTK necesarios\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "# Configurar semilla aleatoria para reproducibilidad\n",
        "# Esto asegura que los resultados sean consistentes en m√∫ltiples ejecuciones\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n"
      ],
      "metadata": {
        "id": "mlE3vBhIe-_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base de Datos y preprocesamiento\n",
        "\n",
        "\n",
        "**Este bloque de c√≥digo** se encarga del preprocesamiento de la **base de datos TimKoornstra/financial-tweets-sentiment**, que contiene tweets clasificados seg√∫n su sentimiento sobre temas financieros: **positivo (1), negativo (2) o neutral (0)**.\n",
        "\n",
        "La secci√≥n comienza cargando el dataset \"**TimKoornstra/financial-tweets-sentiment**\", que es una colecci√≥n integral de tweets enfocados en temas financieros, meticulosamente curada para an√°lisis de sentimiento en el dominio de finanzas y mercados burs√°tiles.\n",
        "\n",
        "El c√≥digo realiza una **exploraci√≥n inicial de los datos**, verificando la distribuci√≥n de las clases en el dataset original: 17,368 sentimientos alcistas (bullish), 8,542 bajistas (bearish) y 12,181 neutrales.\n",
        "\n",
        "A continuaci√≥n, aplica **t√©cnicas especializadas de preprocesamiento para tweets**: conversi√≥n a min√∫sculas, eliminaci√≥n de menciones (@usuario), enlaces, hashtags, caracteres especiales, y normalizaci√≥n de espacios. Tambi√©n realiza tokenizaci√≥n adaptada para el lenguaje t√≠pico de Twitter.\n",
        "\n",
        "Los datos se dividen en **conjuntos de entrenamiento (80%) y prueba (20%)**, manteniendo la distribuci√≥n de las clases mediante estratificaci√≥n para asegurar representatividad en ambos conjuntos.\n",
        "\n",
        "Finalmente, realiza la **tokenizaci√≥n de los textos** y carga un **modelo BERT preentrenado** para generar embeddings avanzados. Estos embeddings proporcionan **representaciones vectoriales ricas en contexto** para t√©rminos financieros espec√≠ficos, permitiendo que las redes neuronales recurrentes capturen mejor los matices del sentimiento expresado en los tweets sobre mercados financieros.\n"
      ],
      "metadata": {
        "id": "GJsZ-GVrfE8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################################################\n",
        "# SECCI√ìN 2: PREPROCESAMIENTO DE LA BASE DE DATOS\n",
        "#####################################################################\n",
        "\n",
        "print(\"Cargando dataset de tweets financieros...\")\n",
        "\n",
        "try:\n",
        "    # Cargamos el dataset TimKoornstra/financial-tweets-sentiment\n",
        "    dataset = load_dataset(\"TimKoornstra/financial-tweets-sentiment\",\n",
        "                         split=\"train\",\n",
        "                         trust_remote_code=True)\n",
        "\n",
        "    # Convertir a DataFrame\n",
        "    df = pd.DataFrame(dataset)\n",
        "\n",
        "    # Verificamos las columnas del dataset\n",
        "    print(f\"Columnas en el dataset: {df.columns.tolist()}\")\n",
        "\n",
        "    # Renombramos columnas para coincidir con el dataset real (nombres en min√∫sculas)\n",
        "    if 'tweet' in df.columns:\n",
        "        df.rename(columns={'tweet': 'text'}, inplace=True)\n",
        "    if 'sentiment' in df.columns:\n",
        "        df.rename(columns={'sentiment': 'label'}, inplace=True)\n",
        "\n",
        "    # Limitamos a una muestra estratificada\n",
        "    # Verificar distribuci√≥n original\n",
        "    class_distribution = df['label'].value_counts()\n",
        "    print(\"\\nDistribuci√≥n original de clases:\")\n",
        "    print(class_distribution)\n",
        "\n",
        "    # Muestreo estratificado asegurando m√≠nimo 2 muestras por clase\n",
        "    sample_size = min(3333, class_distribution.min())  # Ajustar seg√∫n disponibilidad\n",
        "    df = df.groupby('label', group_keys=False).apply(\n",
        "        lambda x: x.sample(n=max(2, min(len(x), sample_size)), random_state=42)  # M√≠nimo 2 muestras por clase\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    # Verificar nueva distribuci√≥n\n",
        "    new_class_distribution = df['label'].value_counts()\n",
        "    print(\"\\nDistribuci√≥n despu√©s del muestreo:\")\n",
        "    print(new_class_distribution)\n",
        "\n",
        "    # Validaci√≥n cr√≠tica: cada clase debe tener al menos 2 muestras\n",
        "    if (new_class_distribution < 2).any():\n",
        "        raise ValueError(f\"Clase con menos de 2 muestras: {new_class_distribution}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar el dataset: {e}\")\n",
        "    print(\"Creando datos de ejemplo como √∫ltimo recurso...\")\n",
        "    # Datos m√≠nimos de ejemplo mejorados\n",
        "    texts = [\n",
        "        \"$AAPL looking bullish today, great earnings report!\",\n",
        "        \"$TSLA down 5% after disappointing delivery numbers\",\n",
        "        \"Markets steady as Fed maintains current policy stance\",\n",
        "        \"Positive outlook for $GOOGL after AI conference\",\n",
        "        \"$AMZN faces regulatory challenges in EU markets\",\n",
        "        \"Oil prices volatile amid Middle East tensions\"\n",
        "    ]\n",
        "    labels = [1, 2, 0, 1, 2, 0]  # Duplicamos muestras por clase\n",
        "    df = pd.DataFrame({\"text\": texts, \"label\": labels})\n",
        "\n",
        "# Exploraci√≥n b√°sica del dataset\n",
        "print(\"\\nInformaci√≥n del dataset:\")\n",
        "print(f\"N√∫mero de ejemplos: {len(df)}\")\n",
        "print(f\"Distribuci√≥n de clases: {df['label'].value_counts().to_dict()}\")\n",
        "\n",
        "# Validaci√≥n final para train_test_split\n",
        "class_dist = df['label'].value_counts()\n",
        "if (class_dist < 2).any():\n",
        "    raise ValueError(f\"No hay suficientes muestras para divisi√≥n estratificada. Distribuci√≥n: {class_dist}\")\n",
        "\n",
        "print(\"\\nPrimeras filas del dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Preprocesamiento de texto especializado para tweets financieros\n",
        "print(\"\\nPreparando los datos...\")\n",
        "\n",
        "def preprocess_tweet(text):\n",
        "    \"\"\"\n",
        "    Funci√≥n especializada para preprocesar tweets financieros\n",
        "    - Elimina menciones, URLs, hashtags\n",
        "    - Preserva s√≠mbolos de empresas ($AAPL, $MSFT, etc.)\n",
        "    - Normaliza el texto\n",
        "    \"\"\"\n",
        "    # Verificar tipo de dato\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "\n",
        "    # Convertir a min√∫sculas\n",
        "    text = text.lower()\n",
        "\n",
        "    # Eliminar URLs\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "    # Eliminar menciones (@usuario)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "    # Preservar s√≠mbolos de acciones ($AAPL)\n",
        "    stock_symbols = re.findall(r'\\$[A-Za-z]+', text)\n",
        "\n",
        "    # Eliminar caracteres especiales pero mantener letras, n√∫meros y espacios\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s\\$]', ' ', text)  # Mantener s√≠mbolos de d√≥lar\n",
        "\n",
        "    # Reintroducir los s√≠mbolos de acciones\n",
        "    for symbol in stock_symbols:\n",
        "        text = text + ' ' + symbol.lower()\n",
        "\n",
        "    # Eliminar espacios m√∫ltiples\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Aplicar preprocesamiento\n",
        "df['processed_text'] = df['text'].apply(preprocess_tweet)\n",
        "\n",
        "# Separar datos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['processed_text'],\n",
        "    df['label'],\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df['label']\n",
        ")\n",
        "\n",
        "print(f\"\\nTama√±o conjunto entrenamiento: {len(X_train)}\")\n",
        "print(f\"Tama√±o conjunto prueba: {len(X_test)}\")\n",
        "\n",
        "# Tokenizaci√≥n usando TensorFlow\n",
        "print(\"\\nTokenizando textos...\")\n",
        "\n",
        "# Par√°metros de tokenizaci√≥n\n",
        "max_words = 15000  # Aumentamos el vocabulario para capturar t√©rminos financieros\n",
        "max_len = 50       # Longitud m√°xima ajustada para tweets (t√≠picamente cortos)\n",
        "\n",
        "# El tokenizador convierte palabras en n√∫meros (√≠ndices)\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')  # OOV = Out Of Vocabulary\n",
        "tokenizer.fit_on_texts(X_train)  # Aprende el vocabulario del conjunto de entrenamiento\n",
        "\n",
        "# Convertir textos a secuencias de √≠ndices\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Padding: hacer que todas las secuencias tengan la misma longitud\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')\n",
        "\n",
        "# Tama√±o real del vocabulario (limitado por max_words)\n",
        "vocab_size = min(max_words, len(tokenizer.word_index)) + 1\n",
        "print(f\"Tama√±o del vocabulario: {vocab_size} palabras √∫nicas\")\n",
        "\n",
        "# Cargar BERT para embeddings\n",
        "print(\"\\nCargando modelo BERT para embeddings...\")\n",
        "\n",
        "# Usamos el modelo BERT base, que tiene buen rendimiento para ingl√©s general\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def get_bert_embeddings(tokenizer, bert_tokenizer, bert_model, vocab_size=15000, embedding_dim=768):\n",
        "    print(\"Generando embeddings BERT para el vocabulario financiero...\")\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    # Limitamos a las 300 palabras m√°s frecuentes para ahorrar tiempo\n",
        "    # En un escenario real, se procesar√≠a todo el vocabulario\n",
        "    words = []\n",
        "    for word, idx in tokenizer.word_index.items():\n",
        "        if idx < 300:  # Aumentamos el n√∫mero de palabras para capturar m√°s t√©rminos financieros\n",
        "            words.append(word)\n",
        "\n",
        "    # Calcular embeddings para cada palabra\n",
        "    for word in words:\n",
        "        idx = tokenizer.word_index[word]\n",
        "        if idx >= vocab_size:\n",
        "            continue\n",
        "\n",
        "        # Tokenizar la palabra con BERT\n",
        "        bert_tokens = bert_tokenizer(word, return_tensors='tf')\n",
        "\n",
        "        # Obtener salida del modelo BERT\n",
        "        outputs = bert_model(bert_tokens)\n",
        "\n",
        "        # Usar la representaci√≥n del token\n",
        "        word_embedding = outputs.last_hidden_state.numpy()[:, 1, :]\n",
        "        embedding_matrix[idx] = word_embedding\n",
        "\n",
        "    print(f\"Embeddings BERT generados para {len(words)} palabras\")\n",
        "    return embedding_matrix\n",
        "\n",
        "# Obtener matriz de embeddings BERT\n",
        "bert_embedding_matrix = get_bert_embeddings(\n",
        "    tokenizer,\n",
        "    bert_tokenizer,\n",
        "    bert_model,\n",
        "    vocab_size=vocab_size\n",
        ")"
      ],
      "metadata": {
        "id": "BUFVM8VUbD1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################################################\n",
        "# BLOQUE ADICIONAL: VISUALIZACI√ìN CORREGIDA DE EMBEDDINGS\n",
        "#####################################################################\n",
        "\n",
        "# 1. Visualizaci√≥n de muestra de datos limpios\n",
        "print(\"\\n=== MUESTRA DE DATOS LIMPIOS ===\")\n",
        "print(\"Texto original vs. Texto procesado:\")\n",
        "sample_df = df.sample(5, random_state=42)[['text', 'processed_text', 'label']]\n",
        "display(sample_df)\n",
        "\n",
        "# 2. Visualizaci√≥n de distribuci√≥n de clases\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(x='label', data=df, palette='viridis')\n",
        "plt.title('Distribuci√≥n de Sentimientos en Tweets Financieros')\n",
        "plt.xlabel('Sentimiento (0=Neutral, 1=Positivo, 2=Negativo)')\n",
        "plt.ylabel('Cantidad de Tweets')\n",
        "plt.show()\n",
        "\n",
        "# 3. Visualizaci√≥n mejorada de embeddings con manejo de errores\n",
        "print(\"\\n=== VISUALIZACI√ìN DE EMBEDDINGS ===\")\n",
        "\n",
        "def safe_tsne(embeddings, n_components=2, perplexity=5, random_state=42):\n",
        "    \"\"\"Funci√≥n segura para aplicar t-SNE con par√°metros autom√°ticos\"\"\"\n",
        "    from sklearn.manifold import TSNE\n",
        "    n_samples = embeddings.shape[0]\n",
        "\n",
        "    # Ajustar perplexity autom√°ticamente si es necesario\n",
        "    safe_perplexity = min(perplexity, n_samples - 1) if n_samples > 1 else 1\n",
        "\n",
        "    try:\n",
        "        tsne = TSNE(n_components=n_components,\n",
        "                   perplexity=safe_perplexity,\n",
        "                   random_state=random_state)\n",
        "        return tsne.fit_transform(embeddings)\n",
        "    except Exception as e:\n",
        "        print(f\"Error en t-SNE: {e}. Usando PCA como alternativa...\")\n",
        "        from sklearn.decomposition import PCA\n",
        "        pca = PCA(n_components=n_components)\n",
        "        return pca.fit_transform(embeddings)\n",
        "\n",
        "# Configuraci√≥n\n",
        "num_words_to_plot = min(50, len(tokenizer.word_index))  # M√°ximo 50 palabras\n",
        "\n",
        "# Obtener palabras m√°s frecuentes\n",
        "top_words = [word for word, _ in sorted(tokenizer.word_index.items(), key=lambda x: x[1])][:num_words_to_plot]\n",
        "\n",
        "# Verificar embeddings disponibles\n",
        "use_bert = 'bert_embedding_matrix' in locals()\n",
        "embedding_source = \"BERT\" if use_bert else \"tradicionales\"\n",
        "print(f\"Usando embeddings {embedding_source} para visualizaci√≥n...\")\n",
        "\n",
        "# Obtener embeddings\n",
        "word_embeddings = {}\n",
        "for word in top_words:\n",
        "    try:\n",
        "        if use_bert:\n",
        "            if word in tokenizer.word_index and tokenizer.word_index[word] < len(bert_embedding_matrix):\n",
        "                word_embeddings[word] = bert_embedding_matrix[tokenizer.word_index[word]]\n",
        "        else:\n",
        "            # Crear embedding dummy basado en frecuencia si no hay BERT\n",
        "            word_embeddings[word] = np.array([tokenizer.word_index[word]])\n",
        "    except Exception as e:\n",
        "        print(f\"Error obteniendo embedding para '{word}': {e}\")\n",
        "\n",
        "if not word_embeddings:\n",
        "    print(\"No se encontraron embeddings v√°lidos. Creando datos de ejemplo...\")\n",
        "    word_embeddings = {f\"word_{i}\": np.random.rand(10) for i in range(num_words_to_plot)}\n",
        "\n",
        "# Convertir a matriz\n",
        "words = list(word_embeddings.keys())\n",
        "embeddings = np.array(list(word_embeddings.values()))\n",
        "\n",
        "# Asegurar dimensionalidad m√≠nima\n",
        "if embeddings.ndim == 1:\n",
        "    embeddings = embeddings.reshape(-1, 1)\n",
        "if embeddings.shape[1] < 2:\n",
        "    embeddings = np.hstack([embeddings, np.zeros((embeddings.shape[0], 2 - embeddings.shape[1]))])\n",
        "\n",
        "# Reducci√≥n dimensional segura\n",
        "print(f\"Aplicando reducci√≥n dimensional a {len(embeddings)} embeddings...\")\n",
        "embeddings_2d = safe_tsne(embeddings) if len(embeddings) > 1 else embeddings[:, :2]\n",
        "\n",
        "# Visualizaci√≥n\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.6, s=100)\n",
        "\n",
        "# A√±adir etiquetas\n",
        "for i, word in enumerate(words):\n",
        "    plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
        "                fontsize=9,\n",
        "                bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
        "\n",
        "plt.title(f'Visualizaci√≥n de Embeddings {embedding_source}', pad=20)\n",
        "plt.xlabel('Dimensi√≥n 1')\n",
        "plt.ylabel('Dimensi√≥n 2')\n",
        "plt.grid(alpha=0.2)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4. Visualizaci√≥n especial para t√©rminos financieros (solo si hay suficientes datos)\n",
        "if use_bert and len(word_embeddings) > 3:\n",
        "    print(\"\\n=== VISUALIZACI√ìN DE T√âRMINOS FINANCIEROS ===\")\n",
        "\n",
        "    finance_terms = ['bull', 'bear', 'market', 'price', 'stock', 'earnings',\n",
        "                    'dollar', 'investment', 'profit', 'loss', 'apple', 'tesla',\n",
        "                    'growth', 'decline', 'rally', 'crash', 'dividend', 'volatility',\n",
        "                    'bullish', 'bearish', 'neutral', 'forecast', 'economy', 'trade',\n",
        "                    'bank', 'fed', 'rate', 'currency', 'crypto', 'bitcoin']\n",
        "\n",
        "    # Filtrar t√©rminos existentes\n",
        "    valid_terms = [term for term in finance_terms\n",
        "                  if term in word_embeddings or\n",
        "                  (term in tokenizer.word_index and tokenizer.word_index[term] < len(bert_embedding_matrix))]\n",
        "\n",
        "    if len(valid_terms) > 3:  # Necesitamos al menos 4 t√©rminos para t-SNE\n",
        "        print(f\"Visualizando {len(valid_terms)} t√©rminos financieros...\")\n",
        "\n",
        "        # Obtener embeddings\n",
        "        term_embeddings = []\n",
        "        valid_words = []\n",
        "        for term in valid_terms:\n",
        "            try:\n",
        "                if term in word_embeddings:\n",
        "                    term_embeddings.append(word_embeddings[term])\n",
        "                    valid_words.append(term)\n",
        "                elif term in tokenizer.word_index and tokenizer.word_index[term] < len(bert_embedding_matrix):\n",
        "                    term_embeddings.append(bert_embedding_matrix[tokenizer.word_index[term]])\n",
        "                    valid_words.append(term)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        term_embeddings = np.array(term_embeddings)\n",
        "\n",
        "        # Reducci√≥n dimensional segura\n",
        "        term_embeddings_2d = safe_tsne(term_embeddings) if len(term_embeddings) > 3 else term_embeddings[:, :2]\n",
        "\n",
        "        # Visualizaci√≥n\n",
        "        plt.figure(figsize=(14, 10))\n",
        "        plt.scatter(term_embeddings_2d[:, 0], term_embeddings_2d[:, 1],\n",
        "                   c='green', alpha=0.7, s=150, edgecolor='w')\n",
        "\n",
        "        for i, term in enumerate(valid_words):\n",
        "            plt.annotate(term, (term_embeddings_2d[i, 0], term_embeddings_2d[i, 1]),\n",
        "                        fontsize=10,\n",
        "                        bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
        "\n",
        "        plt.title('Relaci√≥n Sem√°ntica entre T√©rminos Financieros', pad=20)\n",
        "        plt.xlabel('Dimensi√≥n Sem√°ntica 1')\n",
        "        plt.ylabel('Dimensi√≥n Sem√°ntica 2')\n",
        "        plt.grid(alpha=0.2)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"Solo {len(valid_terms)} t√©rminos financieros encontrados (se necesitan al menos 4 para visualizaci√≥n)\")\n",
        "else:\n",
        "    print(\"No hay suficientes datos para visualizaci√≥n especializada de t√©rminos financieros\")"
      ],
      "metadata": {
        "id": "nzF9Vdg6sPhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento de diferentes RNN para an√°lisis de sentimiento\n",
        "\n",
        "\n",
        "**Esta secci√≥n del c√≥digo** implementa y eval√∫a diferentes arquitecturas de **Redes Neuronales Recurrentes (RNN)** para la clasificaci√≥n de sentimiento en tweets financieros.\n",
        "\n",
        "El proceso comienza definiendo una **funci√≥n flexible para crear modelos RNN** que permite experimentar con distintas configuraciones: **SimpleRNN b√°sicas, LSTM con memoria de largo plazo, o GRU m√°s eficientes computacionalmente**. La funci√≥n tambi√©n incorpora opciones para utilizar **capas bidireccionales** (que procesan el texto en ambas direcciones) y elegir entre **embeddings tradicionales o pre-entrenados de BERT**.\n",
        "\n",
        "El n√∫cleo de esta secci√≥n es la implementaci√≥n de un exhaustivo **Grid Search**, una t√©cnica de optimizaci√≥n que prueba sistem√°ticamente **24 combinaciones diferentes de hiperpar√°metros** (3 tipos de RNN √ó 2 opciones de bidireccionalidad √ó 2 tama√±os de unidades √ó 2 tasas de aprendizaje), tanto con embeddings tradicionales como con BERT, resultando en **48 modelos evaluados en total**.\n",
        "\n",
        "Para cada modelo, se registran m√©tricas clave como **accuracy, F1-score y tiempo de ejecuci√≥n**. El c√≥digo implementa **early stopping** para evitar sobreajuste, deteniendo el entrenamiento cuando el rendimiento deja de mejorar en el conjunto de validaci√≥n.\n",
        "\n",
        "Finalmente, todos los resultados se combinan, ordenan por precisi√≥n, y se presentan los **5 mejores modelos** con sus configuraciones detalladas, proporcionando una visi√≥n clara de qu√© arquitecturas y par√°metros ofrecen mejor rendimiento para la clasificaci√≥n de sentimiento en tweets del √°mbito financiero.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y0GCIe7OftOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#####################################################################\n",
        "# SECCI√ìN 3: IMPLEMENTACI√ìN Y EVALUACI√ìN DE MODELOS RNN\n",
        "#####################################################################\n",
        "\n",
        "print(\"\\nPreparando Grid Search para RNN...\")\n",
        "\n",
        "# Determinar n√∫mero de clases para la capa de salida\n",
        "num_classes = len(df['label'].unique())\n",
        "print(f\"N√∫mero de clases a predecir: {num_classes}\")\n",
        "\n",
        "# Funci√≥n para crear diferentes modelos RNN\n",
        "def create_rnn_model(model_type='lstm', bidirectional=False, units=64,\n",
        "                   learning_rate=0.001, dropout_rate=0.3, use_bert=False):\n",
        "    \"\"\"\n",
        "    Crea un modelo RNN con diferentes configuraciones\n",
        "\n",
        "    Args:\n",
        "        model_type: Tipo de celda recurrente ('simple_rnn', 'lstm', 'gru')\n",
        "        bidirectional: Si usar capa bidireccional\n",
        "        units: N√∫mero de unidades en la capa recurrente\n",
        "        learning_rate: Tasa de aprendizaje\n",
        "        dropout_rate: Tasa de dropout\n",
        "        use_bert: Si usar embeddings BERT pre-entrenados\n",
        "    \"\"\"\n",
        "    # Input layer - recibe secuencias de √≠ndices\n",
        "    inp = Input(shape=(max_len,))\n",
        "\n",
        "    # Embedding layer - convierte √≠ndices en vectores densos\n",
        "    if use_bert:\n",
        "        # Usar embeddings BERT (enfoque de transfer learning)\n",
        "        x = Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            output_dim=bert_embedding_matrix.shape[1],\n",
        "            weights=[bert_embedding_matrix],\n",
        "            input_length=max_len,\n",
        "            trainable=False  # No se actualizan durante el entrenamiento\n",
        "        )(inp)\n",
        "    else:\n",
        "        # Embedding tradicional (se aprende durante el entrenamiento)\n",
        "        x = Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            output_dim=128,  # Dimensionalidad del embedding (hiperpar√°metro)\n",
        "            input_length=max_len\n",
        "        )(inp)\n",
        "\n",
        "    # Seleccionar tipo de capa recurrente\n",
        "    # NOTA PARA ESTUDIANTES: Cada tipo tiene caracter√≠sticas diferentes:\n",
        "    # - SimpleRNN: La m√°s b√°sica, problemas con dependencias largas\n",
        "    # - LSTM: Mejor memoria de largo plazo, m√°s par√°metros\n",
        "    # - GRU: Similar a LSTM pero m√°s eficiente computacionalmente\n",
        "    if model_type == 'simple_rnn':\n",
        "        rnn_layer = SimpleRNN(units, return_sequences=True)\n",
        "    elif model_type == 'lstm':\n",
        "        rnn_layer = LSTM(units, return_sequences=True)\n",
        "    else:  # gru\n",
        "        rnn_layer = GRU(units, return_sequences=True)\n",
        "\n",
        "    # Aplicar capa recurrente\n",
        "    if bidirectional:\n",
        "        x = Bidirectional(rnn_layer)(x)\n",
        "    else:\n",
        "        x = rnn_layer(x)\n",
        "\n",
        "    # Global Pooling - colapsa la secuencia a un vector fijo\n",
        "    x = GlobalMaxPooling1D()(x)\n",
        "\n",
        "    # Dropout - t√©cnica de regularizaci√≥n para evitar sobreajuste\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    # Capa de salida - softmax para clasificaci√≥n multiclase\n",
        "    out = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Compilar modelo\n",
        "    model = Model(inputs=inp, outputs=out)\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        loss='sparse_categorical_crossentropy',  # Para etiquetas enteras (no one-hot)\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Definir par√°metros para el grid search\n",
        "param_grid = {\n",
        "    'model_type': ['lstm', 'gru'],           # Tipo de RNN\n",
        "    'bidirectional': [False, True],                         # Usar o no capa bidireccional\n",
        "    'units': [32, 64],                                      # N√∫mero de unidades (neuronas)\n",
        "    'learning_rate': [0.001, 0.0001],                       # Tasa de aprendizaje\n",
        "    'batch_size': [32],                                     # Tama√±o del lote\n",
        "    'epochs': [10]                                           # √âpocas de entrenamiento\n",
        "}\n",
        "\n",
        "# Funci√≥n para realizar grid search\n",
        "def run_grid_search(param_grid, X_train, y_train, X_val, y_val, use_bert=False):\n",
        "    \"\"\"\n",
        "    Realiza grid search sobre los hiperpar√°metros\n",
        "    \"\"\"\n",
        "    # Generar todas las combinaciones posibles de hiperpar√°metros\n",
        "    keys = param_grid.keys()\n",
        "    combinations = list(itertools.product(*param_grid.values()))\n",
        "    param_combinations = [dict(zip(keys, combo)) for combo in combinations]\n",
        "\n",
        "    # Lista para almacenar resultados\n",
        "    results = []\n",
        "\n",
        "    print(f\"Probando {len(param_combinations)} combinaciones de modelos...\")\n",
        "\n",
        "    # Probar cada combinaci√≥n\n",
        "    for i, params in enumerate(param_combinations):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Extraer par√°metros\n",
        "        model_type = params['model_type']\n",
        "        bidirectional = params['bidirectional']\n",
        "        units = params['units']\n",
        "        learning_rate = params['learning_rate']\n",
        "        batch_size = params['batch_size']\n",
        "        epochs = params['epochs']\n",
        "\n",
        "        # Crear nombre descriptivo para el modelo\n",
        "        model_name = f\"{model_type.upper()}\"\n",
        "        if bidirectional:\n",
        "            model_name = f\"Bi{model_name}\"\n",
        "        model_name += f\"_u{units}_lr{learning_rate}\"\n",
        "        if use_bert:\n",
        "            model_name += \"_BERT\"\n",
        "\n",
        "        print(f\"\\n[{i+1}/{len(param_combinations)}] Entrenando {model_name}...\")\n",
        "\n",
        "        # Crear modelo con los par√°metros actuales\n",
        "        model = create_rnn_model(\n",
        "            model_type=model_type,\n",
        "            bidirectional=bidirectional,\n",
        "            units=units,\n",
        "            learning_rate=learning_rate,\n",
        "            dropout_rate=0.3,  # Fijo para simplificar\n",
        "            use_bert=use_bert\n",
        "        )\n",
        "\n",
        "        # Early stopping - detiene el entrenamiento cuando deja de mejorar\n",
        "        early_stopping = EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            patience=2,  # N√∫mero de √©pocas sin mejora antes de detenerse\n",
        "            restore_best_weights=True  # Restaura los mejores pesos encontrados\n",
        "        )\n",
        "\n",
        "        # Entrenar modelo\n",
        "        history = model.fit(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            callbacks=[early_stopping],\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Evaluar modelo en conjunto de validaci√≥n\n",
        "        y_pred_probs = model.predict(X_val)\n",
        "        y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "        # Calcular m√©tricas de rendimiento\n",
        "        accuracy = accuracy_score(y_val, y_pred)\n",
        "        f1 = f1_score(y_val, y_pred, average='weighted')\n",
        "\n",
        "        # Tiempo de ejecuci√≥n\n",
        "        execution_time = time.time() - start_time\n",
        "\n",
        "        # Guardar resultados\n",
        "        result = {\n",
        "            'model_name': model_name,\n",
        "            'model_type': model_type,\n",
        "            'bidirectional': bidirectional,\n",
        "            'units': units,\n",
        "            'learning_rate': learning_rate,\n",
        "            'batch_size': batch_size,\n",
        "            'accuracy': accuracy,\n",
        "            'f1_score': f1,\n",
        "            'execution_time': execution_time,\n",
        "            'use_bert': use_bert\n",
        "        }\n",
        "\n",
        "        results.append(result)\n",
        "\n",
        "        print(f\"Resultado: Acc={accuracy:.4f}, F1={f1:.4f}, Tiempo={execution_time:.1f}s\")\n",
        "\n",
        "    # Ordenar resultados por precisi√≥n (de mayor a menor)\n",
        "    results_sorted = sorted(results, key=lambda x: x['accuracy'], reverse=True)\n",
        "\n",
        "    return results_sorted\n",
        "\n",
        "# Ejecutar grid search con embeddings tradicionales\n",
        "print(\"\\nEjecutando grid search con embeddings tradicionales...\")\n",
        "results_traditional = run_grid_search(\n",
        "    param_grid,\n",
        "    X_train_pad,\n",
        "    y_train,\n",
        "    X_test_pad,\n",
        "    y_test,\n",
        "    use_bert=False\n",
        ")\n",
        "\n",
        "# Ejecutar grid search con BERT embeddings\n",
        "print(\"\\nEjecutando grid search con embeddings BERT...\")\n",
        "results_bert = run_grid_search(\n",
        "    param_grid,\n",
        "    X_train_pad,\n",
        "    y_train,\n",
        "    X_test_pad,\n",
        "    y_test,\n",
        "    use_bert=True\n",
        ")\n",
        "\n",
        "# Combinar y ordenar todos los resultados\n",
        "all_results = results_traditional + results_bert\n",
        "all_results_sorted = sorted(all_results, key=lambda x: x['accuracy'], reverse=True)\n",
        "\n",
        "# Mostrar los mejores resultados\n",
        "print(\"\\n=== RESULTADOS DEL GRID SEARCH ===\")\n",
        "print(\"\\nTop modelos por precisi√≥n:\")\n",
        "for i, result in enumerate(all_results_sorted[:5]):\n",
        "    print(f\"\\n{i+1}. {result['model_name']}:\")\n",
        "    print(f\"   Accuracy: {result['accuracy']:.4f}\")\n",
        "    print(f\"   F1-Score: {result['f1_score']:.4f}\")\n",
        "    print(f\"   Tiempo: {result['execution_time']:.1f}s\")\n",
        "    print(f\"   Usa BERT: {result['use_bert']}\")\n",
        "    print(f\"   Tipo: {result['model_type'].upper()}, Bidireccional: {result['bidirectional']}\")\n",
        "\n",
        "# Crear DataFrame para visualizaci√≥n\n",
        "results_df = pd.DataFrame(all_results_sorted)"
      ],
      "metadata": {
        "id": "cJ1ZH8U1bJFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VISUALIZACIONES: Comparar rendimiento entre diferentes modelos\n",
        "\n",
        "\n",
        "# 1. Gr√°fico de barras para los mejores modelos\n",
        "plt.figure(figsize=(12, 6))\n",
        "top_n = min(10, len(all_results_sorted))\n",
        "top_models = results_df.head(top_n)['model_name'].tolist()\n",
        "top_accs = results_df.head(top_n)['accuracy'].tolist()\n",
        "\n",
        "# Colores basados en tipo de modelo\n",
        "colors = []\n",
        "for model_type in results_df.head(top_n)['model_type']:\n",
        "    if model_type == 'simple_rnn':\n",
        "        colors.append('blue')\n",
        "    elif model_type == 'lstm':\n",
        "        colors.append('green')\n",
        "    else:  # gru\n",
        "        colors.append('red')\n",
        "\n",
        "# Gr√°fico\n",
        "bars = plt.bar(top_models, top_accs, color=colors)\n",
        "plt.xlabel('Modelo')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Top 10 Modelos por Accuracy')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "# A√±adir valores sobre las barras\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "            f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 2. Comparaci√≥n por tipo de modelo y bidireccionalidad\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Subgr√°fico 1: Comparaci√≥n por tipo de modelo\n",
        "plt.subplot(1, 2, 1)\n",
        "model_type_perf = results_df.groupby('model_type')['accuracy'].mean()\n",
        "bars1 = plt.bar(model_type_perf.index, model_type_perf.values, color=['blue', 'red', 'green'])\n",
        "plt.title('Rendimiento por tipo de modelo')\n",
        "plt.ylabel('Accuracy promedio')\n",
        "\n",
        "# A√±adir valores sobre las barras\n",
        "for bar in bars1:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "            f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Subgr√°fico 2: Comparaci√≥n bidireccional vs unidireccional\n",
        "plt.subplot(1, 2, 2)\n",
        "bidir_perf = results_df.groupby('bidirectional')['accuracy'].mean()\n",
        "bars2 = plt.bar(['Unidireccional', 'Bidireccional'], bidir_perf.values, color=['lightgray', 'darkgray'])\n",
        "plt.title('Unidireccional vs Bidireccional')\n",
        "plt.ylabel('Accuracy promedio')\n",
        "\n",
        "# A√±adir valores sobre las barras\n",
        "for bar in bars2:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "            f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Entrenar el mejor modelo con configuraci√≥n √≥ptima\n",
        "best_result = all_results_sorted[0]\n",
        "print(f\"\\nMejor modelo: {best_result['model_name']}\")\n",
        "print(f\"Accuracy: {best_result['accuracy']:.4f}\")\n",
        "print(f\"F1-Score: {best_result['f1_score']:.4f}\")\n",
        "\n",
        "# Crear y entrenar el mejor modelo\n",
        "print(\"\\nEntrenando el mejor modelo con la configuraci√≥n √≥ptima...\")\n",
        "best_model = create_rnn_model(\n",
        "    model_type=best_result['model_type'],\n",
        "    bidirectional=best_result['bidirectional'],\n",
        "    units=best_result['units'],\n",
        "    learning_rate=best_result['learning_rate'],\n",
        "    dropout_rate=0.3,\n",
        "    use_bert=best_result['use_bert']\n",
        ")\n",
        "\n",
        "# Early stopping con un poco m√°s de paciencia para el modelo final\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Entrenar modelo final\n",
        "history = best_model.fit(\n",
        "    X_train_pad,\n",
        "    y_train,\n",
        "    validation_data=(X_test_pad, y_test),\n",
        "    epochs=10,  # M√°s √©pocas para el modelo final\n",
        "    batch_size=best_result['batch_size'],\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluar modelo final\n",
        "y_pred_probs = best_model.predict(X_test_pad)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# Matriz de confusi√≥n\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=sorted(df['label'].unique()),\n",
        "            yticklabels=sorted(df['label'].unique()))\n",
        "plt.xlabel('Predicci√≥n')\n",
        "plt.ylabel('Valor real')\n",
        "plt.title('Matriz de Confusi√≥n del Mejor Modelo')\n",
        "plt.show()\n",
        "\n",
        "# Reporte de clasificaci√≥n\n",
        "print(\"\\nReporte de clasificaci√≥n del mejor modelo:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Guardar el mejor modelo\n",
        "model_filename = f'best_financial_sentiment_model_{best_result[\"model_type\"]}.keras'\n",
        "best_model.save(model_filename)\n",
        "print(f\"\\nMejor modelo guardado como '{model_filename}'\")\n",
        "\n",
        "# Graficar el historial de entrenamiento\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.title('Precisi√≥n del modelo')\n",
        "plt.ylabel('Precisi√≥n')\n",
        "plt.xlabel('√âpoca')\n",
        "plt.legend(['Train'], loc='lower right')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('P√©rdida del modelo')\n",
        "plt.ylabel('P√©rdida')\n",
        "plt.xlabel('√âpoca')\n",
        "plt.legend(['Train'], loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nProceso de grid search y entrenamiento completo.\")"
      ],
      "metadata": {
        "id": "qMEZJzX5bTkI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
